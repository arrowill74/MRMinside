{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_NAME = 'MRM_ALL_Embedding200_L2_retrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def newPath(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def writeProgress(msg, count, total):\n",
    "    sys.stdout.write(msg + \"{:.2%}\\r\".format(count/total))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 4876)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_4876.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n",
      "150 32\n",
      "128 4876 200\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "\n",
    "print(usr_nb, movie_nb)\n",
    "\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "\n",
    "print(usr_test_amount, movie_test_amount)\n",
    "\n",
    "latent_dim = 128 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "embedding_dims = 200\n",
    "\n",
    "print(latent_dim, ft_dim, embedding_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)\n",
    "# print('Before:', usr_genre)\n",
    "# print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "Avg of followers: 142.0969696969697\n",
      "The num of followers over 5: 163\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "print('Avg of followers:', np.mean(moive_followers))\n",
    "\n",
    "asc = np.sort(moive_followers)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)\n",
    "\n",
    "over5 = 0\n",
    "for num in moive_followers:\n",
    "    if num >= 5:\n",
    "        over5 += 1\n",
    "print('The num of followers over 5:', over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 50: 125\n",
      "Over 100: 89\n",
      "Over 150: 58\n",
      "Over 200: 42\n",
      "Over 250: 31\n",
      "Over 300: 21\n"
     ]
    }
   ],
   "source": [
    "print('Over 50:', np.sum(moive_followers >= 50))\n",
    "print('Over 100:', np.sum(moive_followers >= 100))\n",
    "print('Over 150:', np.sum(moive_followers >= 150))\n",
    "print('Over 200:', np.sum(moive_followers >= 200))\n",
    "print('Over 250:', np.sum(moive_followers >= 250))\n",
    "print('Over 300:', np.sum(moive_followers >= 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,) [  0   2   3   4   9  12  24  28  30  34  40  44  49  55  57  58  60  66\n",
      "  68  78  80  81  84  86  87  99 101 102 112 119 122 123 125 126 127 128\n",
      " 129 134 144 156 161 164]\n",
      "32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]\n"
     ]
    }
   ],
   "source": [
    "over200_idx = np.nonzero(moive_followers >= 200)[0]\n",
    "print(over200_idx.shape, over200_idx)\n",
    "\n",
    "random.seed(42)\n",
    "movie_test_idx = sorted(random.sample(list(over200_idx), movie_test_amount))\n",
    "print(len(movie_test_idx), movie_test_idx) # 32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "Avg of followers: 14.820480404551201\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "print('Avg of followers:', np.mean(each_user))\n",
    "\n",
    "asc = np.sort(each_user)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 10: 1582\n",
      "Over 12: 937\n",
      "Over 14: 613\n",
      "Over 16: 440\n",
      "Over 18: 315\n",
      "Over 20: 229\n"
     ]
    }
   ],
   "source": [
    "print('Over 10:', np.sum(each_user >= 10))\n",
    "print('Over 12:', np.sum(each_user >= 12))\n",
    "print('Over 14:', np.sum(each_user >= 14))\n",
    "print('Over 16:', np.sum(each_user >= 16))\n",
    "print('Over 18:', np.sum(each_user >= 18))\n",
    "print('Over 20:', np.sum(each_user >= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10]) # 150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init\n",
    "train_t = []\n",
    "train_f = []\n",
    "test_t = []\n",
    "test_f = []\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    # init\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    t_for_test = []\n",
    "    f_for_test = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "#         print(len(t_for_train) + len(f_for_train))\n",
    "        \n",
    "    else: #if in test id, choose half of true and other \n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #the others for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    if not (len(t_for_train) + len(f_for_train) + len(t_for_test) + len(f_for_test)) == movie_nb:\n",
    "        print('Error!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.lib.npyio.NpzFile object at 0x7f10159fdc88>\n",
      "User latent shape:  (1582, 128)\n",
      "photo latent shape:  (165, 128)\n",
      "Auxilary latent shape:  (165, 128)\n",
      "Embedding shape: (200, 4876)\n",
      "Wu weight shape: (1582, 165, 128)\n",
      "Wy weight shape: (1582, 165, 128)\n",
      "Wa weight shape: (1582, 165, 128)\n",
      "Wv weight shape: (1582, 165, 200)\n",
      "Beta shape: (1582, 200)\n"
     ]
    }
   ],
   "source": [
    "# reload params if crash\n",
    "params = np.load('./weight/' + SAVE_NAME + '.npz')\n",
    "print(params)\n",
    "U = params['U']\n",
    "Y = params['Y']\n",
    "A = params['A']\n",
    "E = params['E']\n",
    "Au = params['Wu']\n",
    "Ay = params['Wy']\n",
    "Aa = params['Wa']\n",
    "Av = params['Wv']\n",
    "B = params['B']\n",
    "\n",
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Wu weight shape:', Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "alpha:         [0.02774524 0.02953485 0.08315664 0.02260261 0.03408557 0.07617806\n",
      " 0.00969344]\n",
      "softmax alpha: [0.14100926 0.14126184 0.14904332 0.14028597 0.14190615 0.14800683\n",
      " 0.13848663]\n",
      "==================================================\n",
      "1 51\n",
      "alpha:         [0.19391839 0.07012048 0.13753379 0.03810934 0.02937639 0.09551304]\n",
      "softmax alpha: [0.18385665 0.16244808 0.17377681 0.15733028 0.15596231 0.16662587]\n",
      "==================================================\n",
      "2 54\n",
      "alpha:         [0.08465813 0.06927584 0.07905364 0.05089013 0.07589684 0.11613613]\n",
      "softmax alpha: [0.16752669 0.16496947 0.16659042 0.1619641  0.16606535 0.17288397]\n",
      "==================================================\n",
      "3 61\n",
      "alpha:         [0.28550511 0.23000379 0.16383496 0.25330815 0.22539796]\n",
      "softmax alpha: [0.21090681 0.19952011 0.18674541 0.2042244  0.19860327]\n",
      "==================================================\n",
      "4 65\n",
      "alpha:         [0.13224631 0.09930894 0.14849889 0.08164523 0.06131635 0.07902678]\n",
      "softmax alpha: [0.17198853 0.16641595 0.17480662 0.16350224 0.16021198 0.16307468]\n",
      "==================================================\n",
      "5 88\n",
      "alpha:         [0.02002347 0.02278649 0.00864983 0.02789286 0.00745877 0.03862086\n",
      " 0.01520124 0.00779309 0.03073421]\n",
      "softmax alpha: [0.11111796 0.11142541 0.10986131 0.11199584 0.10973053 0.1132038\n",
      " 0.11058341 0.10976722 0.11231451]\n",
      "==================================================\n",
      "6 93\n",
      "alpha:         [0.01509565 0.03411477 0.05065883 0.05313986 0.06749632 0.01659217\n",
      " 0.04362265]\n",
      "softmax alpha: [0.13930652 0.14198137 0.14434985 0.14470843 0.14680092 0.13951515\n",
      " 0.14333775]\n",
      "==================================================\n",
      "7 96\n",
      "alpha:         [0.39054895 0.13888501 0.21241662 0.34673703 0.1811472 ]\n",
      "softmax alpha: [0.22817813 0.17740986 0.19094669 0.21839704 0.18506828]\n",
      "==================================================\n",
      "8 114\n",
      "alpha:         [0.09188664 0.06459599 0.08291426 0.12594356 0.18045796]\n",
      "softmax alpha: [0.19640956 0.1911219  0.19465518 0.20321388 0.21459948]\n",
      "==================================================\n",
      "9 130\n",
      "alpha:         [0.39736522 0.17385757 0.52557195 0.12013723 0.49667804]\n",
      "softmax alpha: [0.20838586 0.166648   0.23689054 0.15793182 0.23014378]\n",
      "==================================================\n",
      "10 135\n",
      "alpha:         [0.09028401 0.0857652  0.22669154 0.05639296 0.30687072]\n",
      "softmax alpha: [0.18691072 0.18606801 0.21422752 0.18068226 0.23211148]\n",
      "==================================================\n",
      "11 142\n",
      "alpha:         [0.0213338  0.02537442 0.01461178 0.17049194 0.07181244 0.14300473]\n",
      "softmax alpha: [0.15774456 0.15838324 0.15668776 0.18311881 0.16591169 0.17815393]\n",
      "==================================================\n",
      "12 146\n",
      "alpha:         [0.08433935 0.11748742 0.07408125 0.18030119 0.16469601]\n",
      "softmax alpha: [0.1920164  0.19848804 0.19005674 0.21135573 0.20808308]\n",
      "==================================================\n",
      "13 161\n",
      "alpha:         [0.06284894 0.19733683 0.33874745 0.0816437  0.62841826]\n",
      "softmax alpha: [0.16022849 0.18329351 0.21113535 0.16326843 0.28207421]\n",
      "==================================================\n",
      "14 163\n",
      "alpha:         [0.20967321 0.12629621 0.09287905 0.12498303 0.32196699 0.26950761]\n",
      "softmax alpha: [0.16923132 0.15569353 0.15057667 0.15548921 0.18934302 0.17966624]\n",
      "==================================================\n",
      "15 178\n",
      "alpha:         [0.40939011 0.38469242 0.73362638 0.46863841 0.26922195]\n",
      "softmax alpha: [0.18907775 0.18446516 0.26148962 0.2006188  0.16434866]\n",
      "==================================================\n",
      "16 186\n",
      "alpha:         [0.15390399 0.0924722  0.11385532 0.04257608 0.08958218 0.23829016]\n",
      "softmax alpha: [0.17177534 0.16154047 0.1650319  0.15367801 0.16107429 0.18689999]\n",
      "==================================================\n",
      "17 189\n",
      "alpha:         [0.22676852 0.05192306 0.44854466 0.27892943 0.42034912]\n",
      "softmax alpha: [0.18674098 0.15678525 0.23310725 0.19674007 0.22662646]\n",
      "==================================================\n",
      "18 191\n",
      "alpha:         [0.23370878 1.04370447 0.60693311 0.27618638 0.11572453]\n",
      "softmax alpha: [0.15077473 0.33892626 0.21898674 0.15731725 0.13399502]\n",
      "==================================================\n",
      "19 198\n",
      "alpha:         [0.48937986 0.25726186 0.22107088 0.70351211 0.44409427]\n",
      "softmax alpha: [0.21043616 0.16684485 0.16091453 0.26068547 0.20111899]\n",
      "==================================================\n",
      "20 206\n",
      "alpha:         [0.12702075 0.25660821 0.05982671 0.3230233  0.13933224]\n",
      "softmax alpha: [0.18859776 0.21469189 0.1763415  0.22943483 0.19093403]\n",
      "==================================================\n",
      "21 209\n",
      "alpha:         [0.08465619 0.06259863 0.08597869 0.13706145 0.04465707 0.16921618]\n",
      "softmax alpha: [0.16441045 0.16082366 0.16462803 0.17325618 0.15796396 0.17891772]\n",
      "==================================================\n",
      "22 224\n",
      "alpha:         [0.15031041 0.12864546 0.10542832 0.05205138 0.11139252 0.09994527]\n",
      "softmax alpha: [0.17379806 0.17007323 0.1661701  0.15753301 0.16716413 0.16526147]\n",
      "==================================================\n",
      "23 228\n",
      "alpha:         [0.18221059 0.17861712 0.22960221 0.17138454 0.06531981 0.06494564]\n",
      "softmax alpha: [0.17202201 0.17140496 0.18037067 0.17016973 0.15304494 0.15298769]\n",
      "==================================================\n",
      "24 255\n",
      "alpha:         [0.02477568 0.04645727 0.09032605 0.01698904 0.05156212 0.12749581\n",
      " 0.12932925 0.04905406]\n",
      "softmax alpha: [0.11973139 0.12235571 0.12784278 0.11880271 0.12298191 0.13268408\n",
      " 0.13292757 0.12267385]\n",
      "==================================================\n",
      "25 283\n",
      "alpha:         [0.02100151 0.04433788 0.21766554 0.1220326  0.10987106 0.062636\n",
      " 0.07985815]\n",
      "softmax alpha: [0.13256742 0.13569744 0.16137896 0.1466608  0.14488798 0.13820331\n",
      " 0.14060408]\n",
      "==================================================\n",
      "26 285\n",
      "alpha:         [0.01235167 0.01925    0.12537354 0.00495469 0.01116783 0.07677883\n",
      " 0.05642331 0.02083385 0.02869637]\n",
      "softmax alpha: [0.10805404 0.10880201 0.1209834  0.10725772 0.1079262  0.11524481\n",
      " 0.11292266 0.10897448 0.10983467]\n",
      "==================================================\n",
      "27 292\n",
      "alpha:         [0.06555451 0.06184641 0.16558309 0.21124794 0.13570883 0.16556502]\n",
      "softmax alpha: [0.15537121 0.15479614 0.17171665 0.17973986 0.16666261 0.17171354]\n",
      "==================================================\n",
      "28 313\n",
      "alpha:         [0.23827108 0.23861912 0.41888761 0.13000688 0.35958652]\n",
      "softmax alpha: [0.19139945 0.19146608 0.22928807 0.17176005 0.21608635]\n",
      "==================================================\n",
      "29 318\n",
      "alpha:         [0.00694426 0.00439211 0.01037175 0.00165919 0.0057276  0.0064191\n",
      " 0.00288195 0.00347204 0.00530011 0.0042713  0.00267757 0.00330044]\n",
      "softmax alpha: [0.08351327 0.0833004  0.0838     0.08307306 0.08341172 0.08346942\n",
      " 0.0831747  0.08322379 0.08337607 0.08329034 0.0831577  0.08320951]\n",
      "==================================================\n",
      "30 326\n",
      "alpha:         [0.0981264  0.01056414 0.08996848 0.06488117 0.05552816 0.02788808\n",
      " 0.01122169]\n",
      "softmax alpha: [0.14964301 0.13709721 0.1484272  0.14474988 0.14340234 0.13949297\n",
      " 0.13718739]\n",
      "==================================================\n",
      "31 327\n",
      "alpha:         [0.01725268 0.01596465 0.0169625  0.00560216 0.03233337 0.01430095\n",
      " 0.01123877 0.02395986 0.00556371 0.01010354]\n",
      "softmax alpha: [0.10018959 0.10006063 0.10016052 0.0990291  0.10171197 0.0998943\n",
      " 0.09958887 0.10086384 0.0990253  0.09947588]\n",
      "==================================================\n",
      "32 333\n",
      "alpha:         [0.06471722 0.20094369 0.55858582 0.40934099 0.16711285]\n",
      "softmax alpha: [0.15863284 0.18178393 0.25994263 0.22390371 0.17573689]\n",
      "==================================================\n",
      "33 334\n",
      "alpha:         [0.25789986 0.10499559 0.09745986 0.10491837 0.22645516 0.34611037]\n",
      "softmax alpha: [0.17763917 0.15245205 0.15130753 0.15244028 0.17214027 0.19402071]\n",
      "==================================================\n",
      "34 350\n",
      "alpha:         [0.03765897 0.14810821 0.17276339 0.15570178 0.59513926 0.04020574]\n",
      "softmax alpha: [0.14014346 0.15650937 0.1604161  0.15770236 0.24472788 0.14050083]\n",
      "==================================================\n",
      "35 393\n",
      "alpha:         [0.00068996 0.00162502 0.00084039 0.00180219 0.00393457 0.00203737\n",
      " 0.00215045 0.01637487 0.0039205  0.00106284 0.00141508 0.00378089\n",
      " 0.00062247 0.02185983 0.00163531]\n",
      "softmax alpha: [0.06642854 0.06649068 0.06643853 0.06650246 0.06664442 0.06651811\n",
      " 0.06652563 0.06747868 0.06664349 0.06645331 0.06647673 0.06663418\n",
      " 0.06642406 0.06784981 0.06649137]\n",
      "==================================================\n",
      "36 407\n",
      "alpha:         [0.15257457 0.36443936 0.37356388 0.35107939 0.49616694]\n",
      "softmax alpha: [0.16358921 0.20219312 0.20404648 0.19950979 0.2306614 ]\n",
      "==================================================\n",
      "37 429\n",
      "alpha:         [0.34131835 0.11352135 0.45973975 0.05260011 0.24744949]\n",
      "softmax alpha: [0.21826295 0.1737997  0.24570261 0.16352768 0.19870705]\n",
      "==================================================\n",
      "38 432\n",
      "alpha:         [0.05049023 0.0529366  0.2072934  0.1061273  0.19522307 0.01240259]\n",
      "softmax alpha: [0.15753519 0.15792105 0.18427921 0.1665484  0.18206827 0.15164788]\n",
      "==================================================\n",
      "39 435\n",
      "alpha:         [0.24801363 0.468511   0.17975626 0.21621227 0.24368401]\n",
      "softmax alpha: [0.19435985 0.24230778 0.181536   0.18827619 0.19352017]\n",
      "==================================================\n",
      "40 440\n",
      "alpha:         [0.00161264 0.0075426  0.00308294 0.00101951 0.00578207 0.00283688\n",
      " 0.00444093 0.00859916 0.00509828 0.00247064 0.00249766 0.01061937\n",
      " 0.00433538]\n",
      "softmax alpha: [0.07669252 0.07714865 0.07680536 0.07664704 0.07701295 0.07678646\n",
      " 0.07690973 0.07723021 0.07696031 0.07675835 0.07676042 0.07738639\n",
      " 0.07690162]\n",
      "==================================================\n",
      "41 447\n",
      "alpha:         [0.06067916 0.09937824 0.15720278 0.06641486 0.05268688 0.06853653]\n",
      "softmax alpha: [0.16269492 0.16911448 0.17918171 0.16363077 0.1613998  0.16397831]\n",
      "==================================================\n",
      "42 449\n",
      "alpha:         [0.00128712 0.00020267 0.0003242  0.00098046 0.00051321 0.00058281\n",
      " 0.00035292 0.00089156 0.0005798  0.00044289 0.0117164  0.00146019\n",
      " 0.00145214 0.00220796 0.00015075 0.00065047 0.00034334]\n",
      "softmax alpha: [0.05881551 0.05875177 0.05875891 0.05879748 0.05877001 0.0587741\n",
      " 0.05876059 0.05879225 0.05877393 0.05876588 0.05943213 0.05882569\n",
      " 0.05882522 0.0588697  0.05874872 0.05877808 0.05876003]\n",
      "==================================================\n",
      "43 451\n",
      "alpha:         [9.50326802e-05 4.71340239e-04 2.29103909e-04 9.62529662e-04\n",
      " 2.07029129e-04 4.65250038e-04 8.36604688e-05 1.82129687e-03\n",
      " 1.40163945e-05 1.64690519e-03 1.47643479e-03 8.17208809e-04\n",
      " 9.42114996e-03 5.48961173e-05 2.01600224e-03 9.30360194e-03\n",
      " 3.01250966e-03 6.88968820e-04 2.09311300e-05 1.29335096e-03]\n",
      "softmax alpha: [0.04991938 0.04993817 0.04992608 0.04996271 0.04992498 0.04993787\n",
      " 0.04991882 0.05000563 0.04991534 0.04999691 0.04998839 0.04995545\n",
      " 0.05038712 0.04991738 0.05001537 0.05038119 0.05006524 0.04994904\n",
      " 0.04991569 0.04997924]\n",
      "==================================================\n",
      "44 457\n",
      "alpha:         [0.53167166 0.18335757 0.08990974 0.21228695 0.20186904]\n",
      "softmax alpha: [0.2635434  0.18602926 0.16943276 0.19148957 0.18950501]\n",
      "==================================================\n",
      "45 466\n",
      "alpha:         [0.2829868  0.441353   0.17401269 0.39351855 0.22952346 0.13930288]\n",
      "softmax alpha: [0.16668548 0.1952879  0.14947581 0.18616631 0.15800795 0.14437654]\n",
      "==================================================\n",
      "46 469\n",
      "alpha:         [0.06022656 0.14718361 0.15493135 0.01550365 0.1536498  0.09529135\n",
      " 0.13094021 0.0379032 ]\n",
      "softmax alpha: [0.1200299  0.1309346  0.13195298 0.11478008 0.13178399 0.12431338\n",
      " 0.12882495 0.11738012]\n",
      "==================================================\n",
      "47 476\n",
      "alpha:         [0.04206454 0.47857838 0.30628571 0.20154617 0.30391477]\n",
      "softmax alpha: [0.15817893 0.24475084 0.2060148  0.1855285  0.20552693]\n",
      "==================================================\n",
      "48 501\n",
      "alpha:         [0.07049084 0.1888169  0.09752912 0.10438967 0.0780592  0.14270123]\n",
      "softmax alpha: [0.15949023 0.17952396 0.1638614  0.16498944 0.16070188 0.17143309]\n",
      "==================================================\n",
      "49 505\n",
      "alpha:         [0.06973525 0.27928483 0.0336754  0.16676495 0.04854993 0.17270809]\n",
      "softmax alpha: [0.15656565 0.19306463 0.1510205  0.17251861 0.15328365 0.17354697]\n",
      "==================================================\n",
      "50 514\n",
      "alpha:         [0.33164815 0.1290266  0.16078115 0.24283898 1.02439824]\n",
      "softmax alpha: [0.17941252 0.14650597 0.15123285 0.16416607 0.35868258]\n",
      "==================================================\n",
      "51 538\n",
      "alpha:         [1.03259861 0.22706955 0.18948192 0.22547441 0.0837593 ]\n",
      "softmax alpha: [0.36892909 0.1648565  0.15877495 0.16459374 0.14284572]\n",
      "==================================================\n",
      "52 541\n",
      "alpha:         [0.00071544 0.00122039 0.00112326 0.00019731 0.0005315  0.00108049\n",
      " 0.02205629 0.0014642  0.00064851 0.00185357 0.00160267 0.00078738\n",
      " 0.00100887 0.0003102  0.00103921 0.00101972]\n",
      "softmax alpha: [0.06240077 0.06243229 0.06242622 0.06236845 0.06238929 0.06242355\n",
      " 0.06374677 0.06244751 0.06239659 0.06247183 0.06245616 0.06240526\n",
      " 0.06241908 0.06237549 0.06242098 0.06241976]\n",
      "==================================================\n",
      "53 542\n",
      "alpha:         [0.10879724 0.2274546  0.10979278 0.20488033 0.30865533]\n",
      "softmax alpha: [0.18351789 0.20663822 0.18370068 0.20202577 0.22411745]\n",
      "==================================================\n",
      "54 546\n",
      "alpha:         [0.02161204 0.11456136 0.02207947 0.06963704 0.23097887 0.11069089\n",
      " 0.06159858]\n",
      "softmax alpha: [0.13308612 0.14604953 0.13314835 0.13963355 0.16408151 0.14548534\n",
      " 0.13851561]\n",
      "==================================================\n",
      "55 548\n",
      "alpha:         [0.04942998 0.12210764 0.177463   0.11340034 0.07794839 0.06091901]\n",
      "softmax alpha: [0.15826493 0.17019554 0.17988242 0.16872003 0.16284336 0.16009372]\n",
      "==================================================\n",
      "56 552\n",
      "alpha:         [0.19073551 0.24233206 0.58571438 0.61573016 0.22476371]\n",
      "softmax alpha: [0.16389413 0.17257246 0.24327672 0.25068956 0.16956713]\n",
      "==================================================\n",
      "57 563\n",
      "alpha:         [0.24305986 0.05748626 0.17537205 0.06649257 0.07769567 0.011057  ]\n",
      "softmax alpha: [0.19069954 0.15840032 0.17821867 0.15983337 0.16163407 0.15121403]\n",
      "==================================================\n",
      "58 569\n",
      "alpha:         [0.03053733 0.06034458 0.03967055 0.15137951 0.06566068 0.10264946]\n",
      "softmax alpha: [0.15927622 0.16409527 0.16073759 0.17973474 0.16496994 0.17118624]\n",
      "==================================================\n",
      "59 592\n",
      "alpha:         [0.07932427 0.08507164 0.01864646 0.04541193 0.09335367 0.02955144\n",
      " 0.16038548 0.02617279]\n",
      "softmax alpha: [0.12639358 0.12712211 0.11895234 0.12217915 0.12817931 0.12025661\n",
      " 0.13706591 0.11985099]\n",
      "==================================================\n",
      "60 600\n",
      "alpha:         [0.22634141 0.1862047  0.05660274 0.15052476 0.33775796]\n",
      "softmax alpha: [0.2062115  0.19809875 0.17401886 0.19115521 0.23051568]\n",
      "==================================================\n",
      "61 644\n",
      "alpha:         [0.14494892 0.68384357 0.44708806 0.28160766 0.09965151]\n",
      "softmax alpha: [0.16210055 0.27785835 0.21928142 0.18583804 0.15492163]\n",
      "==================================================\n",
      "62 646\n",
      "alpha:         [0.11098222 0.38041512 0.19065893 0.06545675 0.34896186]\n",
      "softmax alpha: [0.17805198 0.23310953 0.19281906 0.17012783 0.22589159]\n",
      "==================================================\n",
      "63 664\n",
      "alpha:         [0.00257406 0.00227312 0.00350008 0.00320109 0.0035022  0.00893518\n",
      " 0.00358621 0.00193027 0.00360781 0.00609118 0.00515536 0.00194048\n",
      " 0.00537354]\n",
      "softmax alpha: [0.07681528 0.07679216 0.07688644 0.07686346 0.0768866  0.07730546\n",
      " 0.07689306 0.07676584 0.07689472 0.07708592 0.07701381 0.07676662\n",
      " 0.07703062]\n",
      "==================================================\n",
      "64 689\n",
      "alpha:         [0.00892896 0.01436807 0.02636853 0.01940073 0.13533485 0.04942485\n",
      " 0.04602138 0.04100388 0.04716257]\n",
      "softmax alpha: [0.10730764 0.10789289 0.10919546 0.10843725 0.12176656 0.11174235\n",
      " 0.11136268 0.11080532 0.11148984]\n",
      "==================================================\n",
      "65 696\n",
      "alpha:         [0.01521873 0.01557    0.06857333 0.05209369 0.02545282 0.07028113\n",
      " 0.03500997 0.01242977 0.02012047]\n",
      "softmax alpha: [0.10891195 0.10895022 0.11488072 0.11300305 0.11003229 0.11507708\n",
      " 0.11108893 0.10860863 0.10944712]\n",
      "==================================================\n",
      "66 704\n",
      "alpha:         [0.01308258 0.01258829 0.0150944  0.01981271 0.01278081 0.02132339\n",
      " 0.04269367 0.02309238 0.01384895 0.03668322]\n",
      "softmax alpha: [0.09919643 0.09914741 0.0993962  0.09986629 0.0991665  0.10001727\n",
      " 0.10217767 0.10019436 0.09927248 0.10156538]\n",
      "==================================================\n",
      "67 727\n",
      "alpha:         [0.34817136 0.37230465 0.37232251 0.3479468  0.22179171]\n",
      "softmax alpha: [0.20284273 0.20779754 0.20780125 0.20279719 0.17876128]\n",
      "==================================================\n",
      "68 735\n",
      "alpha:         [0.31766471 0.12712122 0.16652293 0.20500777 0.25465968]\n",
      "softmax alpha: [0.22130594 0.18291153 0.19026243 0.19772737 0.20779273]\n",
      "==================================================\n",
      "69 740\n",
      "alpha:         [0.09428274 0.25617305 0.26771103 0.22542301 0.11797605]\n",
      "softmax alpha: [0.18085764 0.21263999 0.21510763 0.20620081 0.18519393]\n",
      "==================================================\n",
      "70 741\n",
      "alpha:         [0.03711096 0.0327238  0.05210782 0.06505936 0.05875984 0.10355436\n",
      " 0.03395895]\n",
      "softmax alpha: [0.14032107 0.13970681 0.14244131 0.14429814 0.14339199 0.1499612\n",
      " 0.13987948]\n",
      "==================================================\n",
      "71 747\n",
      "alpha:         [0.0261068  0.00431503 0.035928   0.05469574 0.05568985 0.01001791\n",
      " 0.00514656 0.00938732 0.0439141 ]\n",
      "softmax alpha: [0.11096286 0.10857094 0.11205802 0.11418096 0.11429452 0.10919188\n",
      " 0.10866126 0.10912304 0.11295651]\n",
      "==================================================\n",
      "72 758\n",
      "alpha:         [0.00697186 0.00580907 0.00458701 0.00940909 0.0126273  0.00781355\n",
      " 0.03257882 0.00379338 0.00215498 0.00546194]\n",
      "softmax alpha: [0.09978188 0.09966592 0.0995442  0.10002537 0.10034779 0.0998659\n",
      " 0.10236999 0.09946523 0.0993024  0.09963133]\n",
      "==================================================\n",
      "73 775\n",
      "alpha:         [0.08306377 1.14315435 0.27477706 0.08154709 0.61580521]\n",
      "softmax alpha: [0.12820453 0.37007936 0.15529712 0.12801023 0.21840875]\n",
      "==================================================\n",
      "74 777\n",
      "alpha:         [0.05792196 0.03163348 0.05174911 0.04881779 0.01987191 0.07984305\n",
      " 0.04896704 0.07791902]\n",
      "softmax alpha: [0.12570791 0.1224463  0.12493433 0.12456864 0.12101458 0.12849399\n",
      " 0.12458724 0.12824701]\n",
      "==================================================\n",
      "75 778\n",
      "alpha:         [0.1779141  0.11750513 0.07480253 0.37702435 0.09799803]\n",
      "softmax alpha: [0.20052473 0.18876986 0.18087859 0.24470363 0.18512319]\n",
      "==================================================\n",
      "76 781\n",
      "alpha:         [0.35393225 0.06210285 0.50967172 0.10514731 0.17373978]\n",
      "softmax alpha: [0.22073716 0.16486769 0.25793617 0.17211928 0.18433969]\n",
      "==================================================\n",
      "77 788\n",
      "alpha:         [0.04672058 0.13429823 0.06877618 0.10966885 0.22341889 0.09656932]\n",
      "softmax alpha: [0.15568606 0.16993554 0.15915796 0.16580126 0.18577567 0.1636435 ]\n",
      "==================================================\n",
      "78 810\n",
      "alpha:         [0.51579794 0.435368   0.09723683 0.26502143 0.28364111]\n",
      "softmax alpha: [0.24086342 0.22224938 0.15848642 0.18743901 0.19096176]\n",
      "==================================================\n",
      "79 817\n",
      "alpha:         [0.00146028 0.00176724 0.00441879 0.00177818 0.00215028 0.00491538\n",
      " 0.00286315 0.00205547 0.0014651  0.0135871  0.00312831 0.00236507\n",
      " 0.00250371]\n",
      "softmax alpha: [0.07677212 0.07679569 0.07699958 0.07679653 0.07682511 0.07703783\n",
      " 0.07687989 0.07681782 0.07677249 0.07770879 0.07690028 0.07684161\n",
      " 0.07685226]\n",
      "==================================================\n",
      "80 821\n",
      "alpha:         [0.17603342 0.27527921 0.1861867  0.06331315 0.04975994]\n",
      "softmax alpha: [0.20452754 0.22586748 0.20661475 0.18272502 0.18026521]\n",
      "==================================================\n",
      "81 859\n",
      "alpha:         [0.17301627 0.23679296 0.18624529 0.32594271 0.23826926]\n",
      "softmax alpha: [0.1882587  0.20065635 0.19076572 0.21936643 0.2009528 ]\n",
      "==================================================\n",
      "82 864\n",
      "alpha:         [0.09265777 0.10334129 0.12831489 0.14587963 0.04660294 0.05604871]\n",
      "softmax alpha: [0.16609234 0.1678763  0.17212156 0.17517154 0.15861645 0.16012181]\n",
      "==================================================\n",
      "83 865\n",
      "alpha:         [0.04221707 0.2275909  0.1158143  0.02988626 0.1471654  0.22032938]\n",
      "softmax alpha: [0.15212829 0.18311201 0.16374682 0.15026395 0.16896178 0.18178715]\n",
      "==================================================\n",
      "84 877\n",
      "alpha:         [0.05243625 0.66516391 0.49143193 0.06300525 0.06136163]\n",
      "softmax alpha: [0.15585556 0.28762493 0.24175513 0.15751153 0.15725285]\n",
      "==================================================\n",
      "85 919\n",
      "alpha:         [0.03596272 0.03470309 0.04237588 0.0245198  0.01527731 0.04933697\n",
      " 0.03481221 0.01028098 0.02399617]\n",
      "softmax alpha: [0.11175197 0.11161129 0.11247095 0.11048048 0.10946407 0.1132566\n",
      " 0.11162347 0.10891852 0.11042265]\n",
      "==================================================\n",
      "86 928\n",
      "alpha:         [0.04305365 0.25016233 0.09991587 0.07683802 0.21290989]\n",
      "softmax alpha: [0.18155007 0.2233277  0.19217256 0.18778842 0.21516125]\n",
      "==================================================\n",
      "87 939\n",
      "alpha:         [0.11993523 0.15585853 0.04807105 0.0769532  0.30944522]\n",
      "softmax alpha: [0.19478675 0.20191134 0.18127971 0.1865918  0.23543041]\n",
      "==================================================\n",
      "88 940\n",
      "alpha:         [0.06825232 1.06871464 0.28461927 0.47629495 0.46211774]\n",
      "softmax alpha: [0.12582306 0.34218069 0.15621661 0.18922168 0.18655797]\n",
      "==================================================\n",
      "89 946\n",
      "alpha:         [0.0111425  0.01036818 0.01766856 0.02036195 0.01597742 0.00761503\n",
      " 0.0319072  0.02347491 0.01470406]\n",
      "softmax alpha: [0.11045673 0.11037123 0.11117993 0.11147978 0.11099207 0.11006778\n",
      " 0.1127743  0.11182736 0.11085083]\n",
      "==================================================\n",
      "90 958\n",
      "alpha:         [0.15892922 0.09731593 0.04129024 0.0661915  0.042837   0.08530122\n",
      " 0.21985288]\n",
      "softmax alpha: [0.15099071 0.14196848 0.1342333  0.13761784 0.13444109 0.14027297\n",
      " 0.16047561]\n",
      "==================================================\n",
      "91 1010\n",
      "alpha:         [0.59894116 0.05824682 0.21252985 0.0729959  0.38867906 0.09863047]\n",
      "softmax alpha: [0.23421698 0.1363948  0.15914833 0.13842141 0.18980284 0.14201565]\n",
      "==================================================\n",
      "92 1022\n",
      "alpha:         [0.01120172 0.0492591  0.04806972 0.09207606 0.14683233 0.05337491\n",
      " 0.06803558 0.1315133 ]\n",
      "softmax alpha: [0.11716127 0.12170606 0.12156139 0.12703031 0.13417997 0.12220801\n",
      " 0.12401286 0.13214013]\n",
      "==================================================\n",
      "93 1034\n",
      "alpha:         [0.075637   0.09210084 0.02355103 0.1498053  0.10332333 0.09924965\n",
      " 0.03985922]\n",
      "softmax alpha: [0.14164994 0.14400135 0.13446082 0.1525553  0.1456265  0.14503448\n",
      " 0.13667161]\n",
      "==================================================\n",
      "94 1043\n",
      "alpha:         [0.04283157 0.01807074 0.03669904 0.05129123 0.07776119 0.12578349\n",
      " 0.04297337]\n",
      "softmax alpha: [0.14084302 0.13739845 0.13998194 0.14203956 0.14584955 0.15302448\n",
      " 0.140863  ]\n",
      "==================================================\n",
      "95 1083\n",
      "alpha:         [0.0453225  0.06713519 0.03132288 0.05610107 0.0484451  0.01961167\n",
      " 0.02786987 0.0161257 ]\n",
      "softmax alpha: [0.12577579 0.12854944 0.12402725 0.12713881 0.12616915 0.12258321\n",
      " 0.12359972 0.12215663]\n",
      "==================================================\n",
      "96 1093\n",
      "alpha:         [0.08183513 0.21846774 0.27886221 0.04534937 0.20784304 0.31981792]\n",
      "softmax alpha: [0.14855896 0.17030901 0.18091169 0.14323636 0.16850911 0.18847487]\n",
      "==================================================\n",
      "97 1098\n",
      "alpha:         [0.04911064 0.02113081 0.11182321 0.04922904 0.16470249 0.09914611\n",
      " 0.07472046]\n",
      "softmax alpha: [0.13817816 0.13436555 0.14712115 0.13819452 0.15511018 0.14526786\n",
      " 0.14176258]\n",
      "==================================================\n",
      "98 1103\n",
      "alpha:         [0.15699226 0.06233953 0.11810227 0.09003056 0.12760826 0.18749156]\n",
      "softmax alpha: [0.17215292 0.15660558 0.16558641 0.16100275 0.16716798 0.17748436]\n",
      "==================================================\n",
      "99 1116\n",
      "alpha:         [0.02975287 0.04498195 0.03575395 0.02354099 0.03762716 0.0403446\n",
      " 0.05500282 0.04434313 0.14183456]\n",
      "softmax alpha: [0.10878317 0.11045252 0.10943795 0.10810952 0.10964314 0.1099415\n",
      " 0.11156491 0.11038198 0.12168532]\n",
      "==================================================\n",
      "100 1130\n",
      "alpha:         [0.33003133 0.3638864  0.33830031 0.3627433  0.23910149]\n",
      "softmax alpha: [0.20043805 0.20734007 0.20210234 0.2071032  0.18301633]\n",
      "==================================================\n",
      "101 1133\n",
      "alpha:         [0.46517616 0.14265915 0.80025018 0.08920486 0.11216744]\n",
      "softmax alpha: [0.22165279 0.16054835 0.30988033 0.15219169 0.15572684]\n",
      "==================================================\n",
      "102 1140\n",
      "alpha:         [0.14280478 0.20785762 0.12011744 0.06606607 0.14926997 0.06602122]\n",
      "softmax alpha: [0.16939128 0.18077698 0.16559151 0.15687865 0.17048997 0.15687161]\n",
      "==================================================\n",
      "103 1149\n",
      "alpha:         [0.09443766 0.52894608 0.39769909 0.39710599 0.37014222]\n",
      "softmax alpha: [0.15222099 0.23506023 0.20614808 0.20602585 0.20054485]\n",
      "==================================================\n",
      "104 1161\n",
      "alpha:         [0.04730888 0.07602071 0.03831517 0.05535894 0.02468241 0.07866949\n",
      " 0.03581862]\n",
      "softmax alpha: [0.14232206 0.14646761 0.14104779 0.14347238 0.13913797 0.14685609\n",
      " 0.1406961 ]\n",
      "==================================================\n",
      "105 1182\n",
      "alpha:         [0.03303787 0.04794909 0.03150877 0.04302266 0.01008854 0.02142285\n",
      " 0.02032333 0.10397476]\n",
      "softmax alpha: [0.12422093 0.12608709 0.12403112 0.12546746 0.1214026  0.12278645\n",
      " 0.12265151 0.13335284]\n",
      "==================================================\n",
      "106 1195\n",
      "alpha:         [0.29653992 0.09953543 0.5424246  0.73821536 0.10639944]\n",
      "softmax alpha: [0.18241209 0.14979442 0.23325984 0.2837075  0.15082615]\n",
      "==================================================\n",
      "107 1197\n",
      "alpha:         [0.14271469 0.22293115 0.33130735 0.0875092  0.12927364]\n",
      "softmax alpha: [0.19142383 0.20741185 0.23115364 0.18114258 0.1888681 ]\n",
      "==================================================\n",
      "108 1206\n",
      "alpha:         [0.00994221 0.0111601  0.02478074 0.01285787 0.01794855 0.03063047\n",
      " 0.02619434 0.02363651 0.03184695]\n",
      "softmax alpha: [0.10988589 0.1100198  0.11152859 0.11020674 0.1107692  0.11218291\n",
      " 0.11168636 0.11140105 0.11231946]\n",
      "==================================================\n",
      "109 1209\n",
      "alpha:         [0.16960347 0.10583307 0.07531145 0.26443286 0.13260332]\n",
      "softmax alpha: [0.20360857 0.19102971 0.18528726 0.22386177 0.19621269]\n",
      "==================================================\n",
      "110 1220\n",
      "alpha:         [0.00359022 0.00653321 0.00163499 0.00390726 0.00422772 0.00235946\n",
      " 0.00663636 0.00433118 0.00266462 0.00316396 0.00176345 0.0029353\n",
      " 0.00090834]\n",
      "softmax alpha: [0.0769349  0.07716166 0.07678463 0.0769593  0.07698397 0.07684027\n",
      " 0.07716962 0.07699193 0.07686373 0.07690212 0.07679449 0.07688454\n",
      " 0.07672885]\n",
      "==================================================\n",
      "111 1221\n",
      "alpha:         [0.11055252 0.12934936 0.08487476 0.07685391 0.12094955 0.15181578]\n",
      "softmax alpha: [0.16630456 0.16946012 0.16208859 0.1607937  0.16804265 0.17331037]\n",
      "==================================================\n",
      "112 1232\n",
      "alpha:         [0.11969391 0.35810117 0.02883047 0.13199514 0.27664646]\n",
      "softmax alpha: [0.18640421 0.23658907 0.17021359 0.18871137 0.21808176]\n",
      "==================================================\n",
      "113 1236\n",
      "alpha:         [0.1140417  0.12470224 0.15835189 0.08519894 0.19884111]\n",
      "softmax alpha: [0.19546169 0.19755656 0.20431738 0.18990456 0.2127598 ]\n",
      "==================================================\n",
      "114 1247\n",
      "alpha:         [0.16653979 0.0933916  0.14509978 0.09493813 0.22379589 0.54768375]\n",
      "softmax alpha: [0.15718233 0.14609517 0.15384821 0.14632129 0.16644461 0.23010839]\n",
      "==================================================\n",
      "115 1266\n",
      "alpha:         [0.11080659 0.10076131 0.08066032 0.14936777 0.15191777 0.08632663]\n",
      "softmax alpha: [0.16618451 0.16452349 0.16124942 0.17271794 0.17315893 0.16216571]\n",
      "==================================================\n",
      "116 1285\n",
      "alpha:         [0.03140353 0.61179583 0.29903887 0.13671808 0.18163679]\n",
      "softmax alpha: [0.15706539 0.28063489 0.20526412 0.17450908 0.18252652]\n",
      "==================================================\n",
      "117 1287\n",
      "alpha:         [0.19253637 0.36300877 0.23515322 0.20067223 0.28998193]\n",
      "softmax alpha: [0.1872686  0.22207527 0.1954219  0.18879841 0.20643581]\n",
      "==================================================\n",
      "118 1300\n",
      "alpha:         [0.10738159 0.21509332 0.28701281 0.17975721 0.1698568  0.03796898]\n",
      "softmax alpha: [0.15666685 0.17448404 0.18749511 0.16842612 0.16676686 0.14616103]\n",
      "==================================================\n",
      "119 1301\n",
      "alpha:         [0.0052426  0.00582607 0.0044941  0.00488464 0.00355764 0.0044528\n",
      " 0.00501276 0.00605219 0.00208682 0.00788187 0.00442147 0.00473154\n",
      " 0.00672104]\n",
      "softmax alpha: [0.0769395  0.07698441 0.07688194 0.07691197 0.07680997 0.07687876\n",
      " 0.07692182 0.07700182 0.07669708 0.07714284 0.07687635 0.07690019\n",
      " 0.07705334]\n",
      "==================================================\n",
      "120 1309\n",
      "alpha:         [0.18913266 0.21479901 0.2569341  0.23170463 0.25931405 0.19280804]\n",
      "softmax alpha: [0.16087445 0.16505696 0.17216025 0.16787107 0.17257047 0.16146681]\n",
      "==================================================\n",
      "121 1310\n",
      "alpha:         [0.22446086 0.12679248 0.02926209 0.15015228 0.09973348]\n",
      "softmax alpha: [0.22022826 0.19973593 0.18117543 0.20445665 0.19440374]\n",
      "==================================================\n",
      "122 1316\n",
      "alpha:         [0.3188543  0.06626997 0.20677301 0.25593007 0.09779937 0.16876258]\n",
      "softmax alpha: [0.18967851 0.1473405  0.1695672  0.17811089 0.15206007 0.16324283]\n",
      "==================================================\n",
      "123 1327\n",
      "alpha:         [0.0806596  0.054636   0.04546292 0.15922591 0.05561138 0.01440829\n",
      " 0.03395669]\n",
      "softmax alpha: [0.14520127 0.14147135 0.14017956 0.15706931 0.14160941 0.13589323\n",
      " 0.13857587]\n",
      "==================================================\n",
      "124 1330\n",
      "alpha:         [0.46432644 0.16346987 0.33093886 0.15021233 0.15675611]\n",
      "softmax alpha: [0.24503939 0.18137421 0.2144403  0.17898551 0.18016059]\n",
      "==================================================\n",
      "125 1342\n",
      "alpha:         [0.0202877  0.12645377 0.00922588 0.04655812 0.01762766 0.05797408\n",
      " 0.13361999 0.07068905]\n",
      "softmax alpha: [0.11997499 0.13341297 0.11865516 0.12316855 0.11965628 0.12458269\n",
      " 0.13437248 0.12617687]\n",
      "==================================================\n",
      "126 1354\n",
      "alpha:         [0.00116025 0.00042299 0.00126181 0.00093403 0.00083717 0.00095013\n",
      " 0.00345963 0.00122254 0.00069872 0.00162853 0.0016508  0.00115336\n",
      " 0.00100403 0.00124706 0.00068147]\n",
      "softmax alpha: [0.06666261 0.06661348 0.06666938 0.06664753 0.06664108 0.06664861\n",
      " 0.06681607 0.06666676 0.06663185 0.06669384 0.06669532 0.06666215\n",
      " 0.0666522  0.0666684  0.0666307 ]\n",
      "==================================================\n",
      "127 1372\n",
      "alpha:         [0.06679073 0.14912558 0.13545299 0.01978369 0.0925767  0.01558341]\n",
      "softmax alpha: [0.16427975 0.17837813 0.17595584 0.15673614 0.16857095 0.15607918]\n",
      "==================================================\n",
      "128 1385\n",
      "alpha:         [0.35025058 0.27084861 0.35322337 0.41031739 0.35886345]\n",
      "softmax alpha: [0.20011193 0.1848371  0.20070771 0.21250036 0.20184291]\n",
      "==================================================\n",
      "129 1393\n",
      "alpha:         [0.1036759  0.30360599 0.09567073 0.17973474 0.13077707]\n",
      "softmax alpha: [0.18797551 0.22957775 0.18647674 0.20283047 0.19313952]\n",
      "==================================================\n",
      "130 1399\n",
      "alpha:         [0.2293121  0.17279618 0.34180497 0.58077826 0.06384745]\n",
      "softmax alpha: [0.18751998 0.17721602 0.20984691 0.26649395 0.15892315]\n",
      "==================================================\n",
      "131 1402\n",
      "alpha:         [0.13844588 0.08227799 0.22431185 0.11955139 0.06604237 0.03901353]\n",
      "softmax alpha: [0.17088671 0.16155294 0.18620845 0.1676882  0.15895121 0.15471248]\n",
      "==================================================\n",
      "132 1409\n",
      "alpha:         [0.20409155 0.08786356 0.26003961 0.08933316 0.12173128 0.03780094]\n",
      "softmax alpha: [0.17834414 0.15877484 0.18860656 0.15900835 0.16424428 0.15102184]\n",
      "==================================================\n",
      "133 1429\n",
      "alpha:         [0.14596771 0.02823534 0.10780599 0.19699231 0.17069853 0.06658556]\n",
      "softmax alpha: [0.17086522 0.15188789 0.16446756 0.17980981 0.17514355 0.15782596]\n",
      "==================================================\n",
      "134 1436\n",
      "alpha:         [0.03835031 0.14913767 0.0482099  0.0317079  0.049192   0.06520763\n",
      " 0.14112481]\n",
      "softmax alpha: [0.13761199 0.15373424 0.1389755  0.13670094 0.13911205 0.14135796\n",
      " 0.15250731]\n",
      "==================================================\n",
      "135 1437\n",
      "alpha:         [0.14662847 0.18122372 0.42551777 0.42970631 0.72944769]\n",
      "softmax alpha: [0.15443656 0.15987282 0.20411278 0.2049695  0.27660835]\n",
      "==================================================\n",
      "136 1442\n",
      "alpha:         [0.76485273 0.74485997 0.56453516 0.07917868 0.17491711]\n",
      "softmax alpha: [0.25928297 0.25415066 0.21221554 0.13061394 0.14373688]\n",
      "==================================================\n",
      "137 1466\n",
      "alpha:         [0.21782425 0.19062063 0.32238532 0.30205143 0.08395451]\n",
      "softmax alpha: [0.1981781  0.19285961 0.22002192 0.2155932  0.17334717]\n",
      "==================================================\n",
      "138 1470\n",
      "alpha:         [0.21049232 0.17734443 0.13276444 0.53738308 0.13842458]\n",
      "softmax alpha: [0.19194855 0.18569015 0.1775939  0.26616545 0.17860195]\n",
      "==================================================\n",
      "139 1493\n",
      "alpha:         [0.27814353 0.26318278 0.7263827  0.32050758 0.16705871]\n",
      "softmax alpha: [0.18218783 0.17948245 0.28522472 0.19007187 0.16303312]\n",
      "==================================================\n",
      "140 1494\n",
      "alpha:         [0.0374503  0.09202748 0.05338833 0.40027453 0.19088945]\n",
      "softmax alpha: [0.17619966 0.1860834  0.17903044 0.25326643 0.20542006]\n",
      "==================================================\n",
      "141 1508\n",
      "alpha:         [0.02371529 0.0029267  0.00924627 0.01524495 0.01754432 0.00776126\n",
      " 0.00955789 0.01693647 0.01121333 0.00204259]\n",
      "softmax alpha: [0.10121488 0.09913249 0.09976095 0.10036118 0.10059221 0.09961291\n",
      " 0.09979204 0.10053109 0.09995738 0.09904488]\n",
      "==================================================\n",
      "142 1516\n",
      "alpha:         [0.02835825 0.01798964 0.02429937 0.01992891 0.04476242 0.03977337\n",
      " 0.02404345 0.03761969]\n",
      "softmax alpha: [0.1248399  0.12355217 0.12433422 0.12379201 0.12690469 0.12627313\n",
      " 0.1243024  0.12600147]\n",
      "==================================================\n",
      "143 1518\n",
      "alpha:         [0.20012862 0.08218958 0.176529   0.14823808 0.14063035 0.16293056]\n",
      "softmax alpha: [0.17480738 0.1553601  0.17073029 0.16596786 0.16471001 0.16842434]\n",
      "==================================================\n",
      "144 1525\n",
      "alpha:         [0.04425929 0.02290679 0.00833525 0.03079346 0.03382913 0.02444028\n",
      " 0.00614318 0.01813175 0.02201775]\n",
      "softmax alpha: [0.11344263 0.11104603 0.10943965 0.11192528 0.11226556 0.11121645\n",
      " 0.10920001 0.11051704 0.11094735]\n",
      "==================================================\n",
      "145 1529\n",
      "alpha:         [0.44392539 0.14447513 0.13566591 0.17943858 0.25289097]\n",
      "softmax alpha: [0.24572112 0.18213479 0.18053737 0.18861548 0.20299124]\n",
      "==================================================\n",
      "146 1547\n",
      "alpha:         [0.0070303  0.0064017  0.00882018 0.01227417 0.00356998 0.00457137\n",
      " 0.01446425 0.00830294 0.0111053  0.00249227 0.00940347]\n",
      "softmax alpha: [0.09081681 0.09075974 0.09097951 0.09129429 0.0905031  0.09059377\n",
      " 0.09149446 0.09093246 0.09118765 0.09040562 0.09103259]\n",
      "==================================================\n",
      "147 1554\n",
      "alpha:         [0.39503765 0.27041847 0.0992024  0.32201059 0.36968892]\n",
      "softmax alpha: [0.22068778 0.19483046 0.16417184 0.20514599 0.21516393]\n",
      "==================================================\n",
      "148 1563\n",
      "alpha:         [0.11082482 0.06597995 0.05021195 0.02068553 0.05050689 0.07221712\n",
      " 0.04570272 0.01686476 0.02217408]\n",
      "softmax alpha: [0.11796393 0.11279071 0.11102618 0.1077959  0.11105893 0.11349641\n",
      " 0.11052666 0.10738482 0.10795647]\n",
      "==================================================\n",
      "149 1573\n",
      "alpha:         [0.22706191 0.1067891  0.14101055 0.12272671 0.22931845 0.19280787]\n",
      "softmax alpha: [0.17625077 0.15627777 0.16171839 0.15878842 0.17664894 0.1703157 ]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "\n",
    "#with Embedding\n",
    "result = np.zeros((usr_test_amount, movie_nb))\n",
    "RS = np.zeros((usr_test_amount, movie_nb))\n",
    "\n",
    "#test_idx --> Test  index length = 150\n",
    "test_yes_id = []\n",
    "\n",
    "for s in range(usr_test_amount):\n",
    "    print(s, test_idx[s])\n",
    "\n",
    "    yes = []\n",
    "    sample = random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]]))\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha = np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a category vec *user_category vec\n",
    "        \n",
    "# #         ''' Observe each part in attention\n",
    "#         WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "#         WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "#         WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "#         WvVy = np.sum(np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))\n",
    "#         print('The sum of each par -->',\n",
    "#               '\\nw1:',testW1,\n",
    "#               '\\nWuU:',WuUu,\n",
    "#               '\\nwyY:',WyYy,\n",
    "#               '\\nWaA:',WaAa,\n",
    "#               '\\nWvV:',WvVy)\n",
    "# #         '''\n",
    "        \n",
    "        alpha[a] = np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T) +\n",
    "                                np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T) +\n",
    "                                np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T) +\n",
    "                                np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))))*r\n",
    "        \n",
    "    mul = np.zeros((1,latent_dim))\n",
    "    \n",
    "    print(\"{:<15}{}\".format('alpha:', alpha))\n",
    "    print(\"{:<15}{}\".format('softmax alpha:', softmax(alpha)))\n",
    "    print('==================================================')\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        mul += alpha[i] * A[sample[i]] #attention alpha * Ai part \n",
    "    new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "    for k in range(movie_nb):\n",
    "        result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 165)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(RS.shape)\n",
    "\n",
    "testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "target = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "        \n",
    "for z in range(usr_test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    # positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    # not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "#     print(user_id)\n",
    "#     print(youtube_t)\n",
    "#     print(youtube_f)\n",
    "    \n",
    "    #targetRS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "        \n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "    \n",
    "#     print(testRS[z])\n",
    "#     print(target[z])\n",
    "#     print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n",
      "num of positive data in testing: 1078.0\n"
     ]
    }
   ],
   "source": [
    "print(target.shape, testRS.shape)\n",
    "sumtarget = np.sum(target)\n",
    "print('num of positive data in testing:', sumtarget) # whole matrix: 4800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "def topN(RSls, n):\n",
    "    maxn = np.argsort(RSls)[::-1][:n]\n",
    "    return maxn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32)\n"
     ]
    }
   ],
   "source": [
    "all_sort = []\n",
    "\n",
    "for i in range(usr_test_amount):\n",
    "    all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "    \n",
    "all_sort = np.asarray(all_sort)\n",
    "print(all_sort.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def DCG(prec_list): #n[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    return dcg\n",
    "\n",
    "def NDCG(target, testRS, num_ndcg): #target\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(usr_test_amount): # the number of testing users\n",
    "#         print(target[m][:num_ndcg])\n",
    "        idcg = DCG(target[m][:num_ndcg])\n",
    "        \n",
    "        pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #prec_list  score\n",
    "#         print(pre_list)\n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "#         print(ndcg)\n",
    "        total_ndcg += ndcg\n",
    "        \n",
    "    avg_ndcg = total_ndcg/usr_test_amount\n",
    "    return avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def MAP(target,testRS):\n",
    "    total_prec = 0\n",
    "    for u in range(usr_test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec += average_precision_score(y_true, y_scores)\n",
    "        \n",
    "    Map_value = total_prec/usr_test_amount\n",
    "    \n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1\n",
      "Num of TP: 69\n",
      "prec: 0.46\n",
      "recall: 0.0640074211502783\n",
      "F1_score: 0.11237785016286646\n",
      "*****\n",
      "Top 3\n",
      "Num of TP: 255\n",
      "prec: 0.5666666666666667\n",
      "recall: 0.23654916512059368\n",
      "F1_score: 0.3337696335078534\n",
      "*****\n",
      "Top 5\n",
      "Num of TP: 526\n",
      "prec: 0.7013333333333334\n",
      "recall: 0.48794063079777367\n",
      "F1_score: 0.575492341356674\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "NDCG@ 10\n",
      "NDCG score: 0.4160738701382943\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "MAP: 0.4069408103797797\n"
     ]
    }
   ],
   "source": [
    "# Top N\n",
    "N = [1, 3, 5]\n",
    "correct = 0\n",
    "\n",
    "for n in N:\n",
    "    print('Top', n)\n",
    "    \n",
    "    for i in range(len(testRS)):\n",
    "        topn = topN(testRS[i], n)\n",
    "        sum_target = int(np.sum(target[i]))\n",
    "        \n",
    "        TP = 0\n",
    "        for i in topn:\n",
    "            if i < sum_target:\n",
    "                TP += 1\n",
    "                \n",
    "        correct += TP\n",
    "\n",
    "    print('Num of TP:', correct)\n",
    "\n",
    "    prec = correct/(len(testRS)*n)\n",
    "    recall = correct/sumtarget\n",
    "    \n",
    "    print('prec:', prec)\n",
    "    print('recall:', recall)\n",
    "    print('F1_score:', F1_score(prec, recall))\n",
    "    \n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# NDCG\n",
    "num_ndcgs = [10]\n",
    "for num_ndcg in num_ndcgs:\n",
    "    print('NDCG@', num_ndcg)\n",
    "    print('NDCG score:', NDCG(target, testRS, num_ndcg))\n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# MAP\n",
    "print('MAP:', MAP(target,testRS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
