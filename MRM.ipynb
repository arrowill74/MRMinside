{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_NAME = 'MRM_E200_5epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def newPath(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def writeProgress(msg, count, total):\n",
    "    sys.stdout.write(msg + \"{:.2%}\\r\".format(count/total))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 4876)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_4876.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n",
      "150 32\n",
      "128 4876 200\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "print(usr_nb, movie_nb)\n",
    "\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "print(usr_test_amount, movie_test_amount)\n",
    "\n",
    "latent_dim = 128 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "embedding_dims = 200\n",
    "print(latent_dim, ft_dim, embedding_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)\n",
    "# print('Before:', usr_genre)\n",
    "# print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "Avg of followers: 142.0969696969697\n",
      "The num of followers over 5: 163\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "print('Avg of followers:', np.mean(moive_followers))\n",
    "\n",
    "asc = np.sort(moive_followers)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)\n",
    "\n",
    "over5 = 0\n",
    "for num in moive_followers:\n",
    "    if num >= 5:\n",
    "        over5 += 1\n",
    "print('The num of followers over 5:', over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 50: 125\n",
      "Over 100: 89\n",
      "Over 150: 58\n",
      "Over 200: 42\n",
      "Over 250: 31\n",
      "Over 300: 21\n"
     ]
    }
   ],
   "source": [
    "print('Over 50:', np.sum(moive_followers >= 50))\n",
    "print('Over 100:', np.sum(moive_followers >= 100))\n",
    "print('Over 150:', np.sum(moive_followers >= 150))\n",
    "print('Over 200:', np.sum(moive_followers >= 200))\n",
    "print('Over 250:', np.sum(moive_followers >= 250))\n",
    "print('Over 300:', np.sum(moive_followers >= 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,) [  0   2   3   4   9  12  24  28  30  34  40  44  49  55  57  58  60  66\n",
      "  68  78  80  81  84  86  87  99 101 102 112 119 122 123 125 126 127 128\n",
      " 129 134 144 156 161 164]\n",
      "32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]\n"
     ]
    }
   ],
   "source": [
    "over200_idx = np.nonzero(moive_followers >= 200)[0]\n",
    "print(over200_idx.shape, over200_idx)\n",
    "\n",
    "random.seed(42)\n",
    "movie_test_idx = sorted(random.sample(list(over200_idx), movie_test_amount))\n",
    "print(len(movie_test_idx), movie_test_idx) # 32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "Avg of followers: 14.820480404551201\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "print('Avg of followers:', np.mean(each_user))\n",
    "\n",
    "asc = np.sort(each_user)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 10: 1582\n",
      "Over 12: 937\n",
      "Over 14: 613\n",
      "Over 16: 440\n",
      "Over 18: 315\n",
      "Over 20: 229\n"
     ]
    }
   ],
   "source": [
    "print('Over 10:', np.sum(each_user >= 10))\n",
    "print('Over 12:', np.sum(each_user >= 12))\n",
    "print('Over 14:', np.sum(each_user >= 14))\n",
    "print('Over 16:', np.sum(each_user >= 16))\n",
    "print('Over 18:', np.sum(each_user >= 18))\n",
    "print('Over 20:', np.sum(each_user >= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10]) # 150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init\n",
    "train_t = []\n",
    "train_f = []\n",
    "test_t = []\n",
    "test_f = []\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    # init\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    t_for_test = []\n",
    "    f_for_test = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "#         print(len(t_for_train) + len(f_for_train))\n",
    "        \n",
    "    else: #if in test id, choose half of true and other \n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #the others for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    if not (len(t_for_train) + len(f_for_train) + len(t_for_test) + len(f_for_test)) == movie_nb:\n",
    "        print('Error!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None, ft_dim))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "image_j = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [usr_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [movie_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [movie_nb, latent_dim],\n",
    "                               initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    \n",
    "#     W1 = tf.get_variable(\"W1\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [usr_nb, movie_nb, embedding_dims], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     Wve = tf.get_variable(\"Wve\", [embedding_dims, ft_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1, latent_dim], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "with tf.variable_scope('feature_level'):\n",
    "    embedding = tf.get_variable(\"embedding\", [embedding_dims,ft_dim],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Beta = tf.get_variable(\"beta\", [usr_nb, embedding_dims],\n",
    "                           initializer=tf.random_normal_initializer(0.01, 0.001, seed=10))\n",
    "    \n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user)\n",
    "vi = tf.nn.embedding_lookup(item_latent, i)\n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "# w1 = tf.nn.embedding_lookup(W1, user)\n",
    "wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user))\n",
    "wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user))\n",
    "wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user))\n",
    "wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user))\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-19-45fa636fc37a>:95: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "a_list = tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    \n",
    "    a_list = tf.concat([a_list,[(tf.nn.relu(tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                            tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(wvui, tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "    q += 1\n",
    "    return q, a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "norm_par = [wu,wy,wa,wv]\n",
    "# norm_par = [tf.reduce_sum(tf.multiply(u, u)),\n",
    "#             tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "#             tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "#             tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "#             tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "#             tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "#             tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "#             tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "#             tf.reduce_sum(tf.multiply(embedding,embedding))]\n",
    "\n",
    "wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "\n",
    "last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(latent_dim),0)\n",
    "q = tf.constant(0)\n",
    "\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_, aux_np = tf.while_loop(sum_att_cond, sum_att_body, [q,aux_np])\n",
    "\n",
    "aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "aux_np += u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new = tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "# ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "l2_norm = tf.add_n([\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.001 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(embedding,embedding))\n",
    "          ])\n",
    "\n",
    "loss = l2_norm - tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Thu Mar 12 19:07:36 2020\n",
      "Epoch: 0\n",
      "total_loss          [[0.61387319]]\n",
      "train_auc:          0.7869858726752503\n",
      "\tCurrent time: Fri Mar 13 00:23:46 2020  sec\n",
      "==================================================\n",
      "Epoch: 1\n",
      "total_loss          [[0.54947819]]\n",
      "train_auc:          0.8300384477825465\n",
      "\tCurrent time: Fri Mar 13 05:42:06 2020  sec\n",
      "==================================================\n",
      "Epoch: 2\n",
      "total_loss          [[0.5259891]]\n",
      "train_auc:          0.8486230329041488\n",
      "\tCurrent time: Fri Mar 13 10:59:16 2020  sec\n",
      "==================================================\n",
      "Epoch: 3\n",
      "total_loss          [[0.51012813]]\n",
      "train_auc:          0.8599383047210301\n",
      "\tCurrent time: Fri Mar 13 16:27:58 2020  sec\n",
      "==================================================\n",
      "Epoch: 4\n",
      "total_loss          [[0.50315245]]\n",
      "train_auc:          0.8665325464949929\n",
      "\tCurrent time: Fri Mar 13 21:56:30 2020  sec\n",
      "==================================================\n",
      "Total cost time: 96531.46195411682  sec\n",
      "End time: Fri Mar 13 21:56:30 2020\n"
     ]
    }
   ],
   "source": [
    "print('Start time:', time.ctime())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_yes_id=[]\n",
    "\n",
    "for q in range(5):\n",
    "    print('Epoch:',q)\n",
    "    train_auc = 0\n",
    "    total_loss = 0\n",
    "    xuij_auc = 0\n",
    "    length = 0\n",
    "    \n",
    "    for z in range(usr_nb):\n",
    "        writeProgress('Progress:', z, usr_nb)\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes = []\n",
    "        yesr = []\n",
    "        \n",
    "#         #選全部的Positive\n",
    "#         sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #選全部的電影\n",
    "        sample = all_auxilary\n",
    "        \n",
    "        #change\n",
    "        r_3 = np.zeros(len(sample))\n",
    "         \n",
    "        for b in range(len(sample)):\n",
    "            yes.append(all_npy[sample[b]])\n",
    "            yesr.append(movie_genre[sample[b]] * usr_genre_norm[z])\n",
    "        \n",
    "        for b in range(len(yesr)):\n",
    "            r_3[b]=max(yesr[b])\n",
    "        #print('r_3:',r_3)\n",
    "        \n",
    "        yes = np.array(yes)\n",
    "        \n",
    "        # positive sample\n",
    "        train_t_sample = train_t[z]\n",
    "        for ta in train_t_sample:\n",
    "            #print(ta,'--> positive feedback')\n",
    "            \n",
    "            pos = sample.index(ta)\n",
    "            \n",
    "            image_1=np.expand_dims(all_npy[ta],0)\n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            \n",
    "            for b in train_f_sample:\n",
    "                image_2 = np.expand_dims(all_npy[b],0)\n",
    "                \n",
    "                _last_be_relu, _norm_par, _a_list, r3, _auc, _loss, _ = sess.run(\n",
    "                    [last_be_relu, norm_par, a_list_smooth, a_list_soft, auc, loss, train_op], \n",
    "                    feed_dict={user: [z], i: [ta], j: [b], xf: yes, \n",
    "                               l_id:sample, l_id_len:[len(sample)],\n",
    "                               positive_id: train_t[z], positive_len:[len(train_t[z])],\n",
    "                               r: r_3, image_i: image_1, image_j: image_2})\n",
    "                \n",
    "                '''Observe all params\n",
    "                print('u,vi,vj',_norm_par[:3])\n",
    "                print('w1,wu,wy,wa,wv',_norm_par[3:7])\n",
    "                print('beta',_norm_par[7])\n",
    "                print('Embedding',_norm_par[8])\n",
    "                print('after softmax:', r3)\n",
    "                print('before softmax:', _a_list)\n",
    "                print('---------------------------------------------------')\n",
    "                '''\n",
    "                train_auc += _auc\n",
    "                total_loss += _loss\n",
    "                length += 1\n",
    "    \n",
    "    print(\"{:<20}{}\".format('total_loss', total_loss/length))\n",
    "    print(\"{:<20}{}\".format('train_auc:', train_auc/length))\n",
    "    \n",
    "    loss_acc_list.append([total_loss/length, train_auc/length])\n",
    "    \n",
    "    print('\\tCurrent time:', time.ctime(), ' sec')\n",
    "    print('==================================================')\n",
    "    \n",
    "print('Total cost time:',time.time()-t0, ' sec')\n",
    "\n",
    "print('End time:', time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "loss= [[0.61387319]]\n",
      "acc= 0.7869858726752503\n",
      "==================================================\n",
      "Iteration: 1\n",
      "loss= [[0.54947819]]\n",
      "acc= 0.8300384477825465\n",
      "==================================================\n",
      "Iteration: 2\n",
      "loss= [[0.5259891]]\n",
      "acc= 0.8486230329041488\n",
      "==================================================\n",
      "Iteration: 3\n",
      "loss= [[0.51012813]]\n",
      "acc= 0.8599383047210301\n",
      "==================================================\n",
      "Iteration: 4\n",
      "loss= [[0.50315245]]\n",
      "acc= 0.8665325464949929\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(loss_acc_list)):\n",
    "    print('Iteration:',i)\n",
    "    print('loss=',loss_acc_list[i][0])\n",
    "    print('acc=',loss_acc_list[i][1])\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 6)\n",
      "[0.6138731949660229, 0.5494781887070815, 0.5259891013389664, 0.5101281336619278, 0.5031524527226395]\n",
      "[0.7869858726752503, 0.8300384477825465, 0.8486230329041488, 0.8599383047210301, 0.8665325464949929]\n"
     ]
    }
   ],
   "source": [
    "# training history\n",
    "epochs = range(1, len(loss_acc_list) + 1)\n",
    "print(epochs)\n",
    "loss = [ls[0].tolist()[0][0] for ls in loss_acc_list]\n",
    "print(loss)\n",
    "acc = [ls[1] for ls in loss_acc_list]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 繪製結果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av, B = sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv, Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (1582, 128)\n",
      "photo latent shape:  (165, 128)\n",
      "Auxilary latent shape:  (165, 128)\n",
      "Embedding shape: (200, 4876)\n",
      "Wu weight shape: (1582, 165, 128)\n",
      "Wy weight shape: (1582, 165, 128)\n",
      "Wa weight shape: (1582, 165, 128)\n",
      "Wv weight shape: (1582, 165, 200)\n",
      "Beta shape: (1582, 200)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Wu weight shape:', Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('./weight/' + SAVE_NAME + '.npz', \n",
    "         U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av, B=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload params if crash\n",
    "# SAVE_NAME = 'MRM_ALL_Embedding200_L2_resplit'\n",
    "\n",
    "# params = np.load('./weight/' + SAVE_NAME + '.npz')\n",
    "# print(params)\n",
    "# U = params['U']\n",
    "# Y = params['Y']\n",
    "# A = params['A']\n",
    "# E = params['E']\n",
    "# Au = params['Wu']\n",
    "# Ay = params['Wy']\n",
    "# Aa = params['Wa']\n",
    "# Av = params['Wv']\n",
    "# B = params['B']\n",
    "\n",
    "# print('User latent shape: ',U.shape)\n",
    "# print('photo latent shape: ', Y.shape)\n",
    "# print('Auxilary latent shape: ',A.shape)\n",
    "# print('Embedding shape:', E.shape)\n",
    "# print('Wu weight shape:', Au.shape)\n",
    "# print('Wy weight shape:', Ay.shape)\n",
    "# print('Wa weight shape:', Aa.shape)\n",
    "# print('Wv weight shape:', Av.shape)\n",
    "# print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "alpha:         [0.02927988 0.02566203 0.05361316 0.02646627 0.01021733 0.03937971\n",
      " 0.03799161]\n",
      "softmax alpha: [0.14248608 0.14197152 0.14599576 0.14208574 0.13979565 0.14393245\n",
      " 0.1437328 ]\n",
      "==================================================\n",
      "1 51\n",
      "alpha:         [0.01832538 0.02929403 0.11697476 0.02727993 0.08322401 0.1112522 ]\n",
      "softmax alpha: [0.15902966 0.1607836  0.17551773 0.16046009 0.16969273 0.17451619]\n",
      "==================================================\n",
      "2 54\n",
      "alpha:         [0.05392254 0.09252942 0.10310583 0.07062763 0.12658498 0.06274428]\n",
      "softmax alpha: [0.16152875 0.16788682 0.16967188 0.16424977 0.17370277 0.16296002]\n",
      "==================================================\n",
      "3 61\n",
      "alpha:         [0.10953117 0.0584107  0.10097866 0.07685557 0.13842113]\n",
      "softmax alpha: [0.20247784 0.1923872  0.20075353 0.19596868 0.20841274]\n",
      "==================================================\n",
      "4 65\n",
      "alpha:         [0.07901194 0.07498497 0.0328394  0.06563788 0.05616643 0.10870329]\n",
      "softmax alpha: [0.16820509 0.16752909 0.1606152  0.16597048 0.16440592 0.17327421]\n",
      "==================================================\n",
      "5 88\n",
      "alpha:         [0.00702683 0.01170304 0.01261435 0.00353422 0.01650948 0.00609636\n",
      " 0.01324405 0.00726861 0.01657891]\n",
      "softmax alpha: [0.11072385 0.11124283 0.11134425 0.11033781 0.1117788  0.11062087\n",
      " 0.11141439 0.11075063 0.11178656]\n",
      "==================================================\n",
      "6 93\n",
      "alpha:         [0.03053004 0.03279867 0.02867278 0.05799727 0.0133375  0.01964968\n",
      " 0.04714049]\n",
      "softmax alpha: [0.14250808 0.14283175 0.14224365 0.14647664 0.14007895 0.14096595\n",
      " 0.14489498]\n",
      "==================================================\n",
      "7 96\n",
      "alpha:         [0.0621214  0.18723863 0.12123374 0.13630104 0.19265664]\n",
      "softmax alpha: [0.1848219  0.20945521 0.19607653 0.19905324 0.21059312]\n",
      "==================================================\n",
      "8 114\n",
      "alpha:         [0.08045868 0.06539571 0.10742463 0.11001966 0.10345798]\n",
      "softmax alpha: [0.197408   0.19445673 0.20280372 0.20333069 0.20200086]\n",
      "==================================================\n",
      "9 130\n",
      "alpha:         [0.1041268  0.14841186 0.16921701 0.10452945 0.06479568]\n",
      "softmax alpha: [0.19706878 0.2059921  0.21032269 0.19714814 0.18946829]\n",
      "==================================================\n",
      "10 135\n",
      "alpha:         [0.12826797 0.06310984 0.08791187 0.14389842 0.06077961]\n",
      "softmax alpha: [0.20627665 0.19326457 0.19811786 0.20952617 0.19281474]\n",
      "==================================================\n",
      "11 142\n",
      "alpha:         [0.01208877 0.06796634 0.1004887  0.01740881 0.0276331  0.12049471]\n",
      "softmax alpha: [0.15909904 0.16824217 0.17380375 0.15994771 0.16159145 0.17731589]\n",
      "==================================================\n",
      "12 146\n",
      "alpha:         [0.15886208 0.1499849  0.09374064 0.07166171 0.11481806]\n",
      "softmax alpha: [0.20826734 0.20642669 0.19513685 0.19087565 0.19929348]\n",
      "==================================================\n",
      "13 161\n",
      "alpha:         [0.1406808  0.10923362 0.08377753 0.16891774 0.05117403]\n",
      "softmax alpha: [0.20589905 0.19952485 0.19450983 0.21179587 0.1882704 ]\n",
      "==================================================\n",
      "14 163\n",
      "alpha:         [0.12592032 0.09165829 0.05798553 0.05521922 0.09151333 0.1297476 ]\n",
      "softmax alpha: [0.17234269 0.16653789 0.16102347 0.16057864 0.16651375 0.17300356]\n",
      "==================================================\n",
      "15 178\n",
      "alpha:         [0.11597482 0.12698377 0.11657076 0.23323134 0.09592324]\n",
      "softmax alpha: [0.19545673 0.19762039 0.19557325 0.21977308 0.19157655]\n",
      "==================================================\n",
      "16 186\n",
      "alpha:         [0.05696043 0.06943927 0.0498229  0.09089347 0.05307525 0.02614807]\n",
      "softmax alpha: [0.16650737 0.16859821 0.16532315 0.17225443 0.16586172 0.16145512]\n",
      "==================================================\n",
      "17 189\n",
      "alpha:         [0.14001688 0.12892389 0.06089083 0.11894289 0.1561026 ]\n",
      "softmax alpha: [0.20373843 0.20149086 0.18823872 0.19948978 0.20704221]\n",
      "==================================================\n",
      "18 191\n",
      "alpha:         [0.1760259  0.09316458 0.1363891  0.29165382 0.05396217]\n",
      "softmax alpha: [0.20452879 0.18826441 0.19658048 0.22959954 0.18102678]\n",
      "==================================================\n",
      "19 198\n",
      "alpha:         [0.12210534 0.14384524 0.08685241 0.25255589 0.0937899 ]\n",
      "softmax alpha: [0.19612602 0.20043647 0.18933246 0.22345454 0.19065052]\n",
      "==================================================\n",
      "20 206\n",
      "alpha:         [0.12289592 0.07079587 0.05075737 0.19275919 0.038269  ]\n",
      "softmax alpha: [0.20530287 0.19488045 0.1910142  0.22015891 0.18864358]\n",
      "==================================================\n",
      "21 209\n",
      "alpha:         [0.0788329  0.05902302 0.05552395 0.07651217 0.12101341 0.03376588]\n",
      "softmax alpha: [0.16795308 0.16465869 0.16408354 0.16756376 0.17518896 0.16055196]\n",
      "==================================================\n",
      "22 224\n",
      "alpha:         [0.10811378 0.09836205 0.07348384 0.05925527 0.07034948 0.06155999]\n",
      "softmax alpha: [0.17164354 0.16997786 0.16580128 0.16345887 0.16528241 0.16383603]\n",
      "==================================================\n",
      "23 228\n",
      "alpha:         [0.04387066 0.05117032 0.08516312 0.12405728 0.08065553 0.07065614]\n",
      "softmax alpha: [0.16135305 0.16253518 0.16815519 0.17482429 0.16739892 0.16573337]\n",
      "==================================================\n",
      "24 255\n",
      "alpha:         [0.04330118 0.0171797  0.0166654  0.01013994 0.04387063 0.01346858\n",
      " 0.02734622 0.01633996]\n",
      "softmax alpha: [0.12748495 0.12419797 0.12413411 0.12332671 0.12755756 0.12373791\n",
      " 0.12546707 0.12409372]\n",
      "==================================================\n",
      "25 283\n",
      "alpha:         [0.05933477 0.01680351 0.02176242 0.04304483 0.04191796 0.02998671\n",
      " 0.03984584]\n",
      "softmax alpha: [0.14620226 0.14011447 0.14081102 0.14383993 0.14367793 0.14197386\n",
      " 0.14338052]\n",
      "==================================================\n",
      "26 285\n",
      "alpha:         [0.01441831 0.03748995 0.00565192 0.01942857 0.02046378 0.00797009\n",
      " 0.007479   0.0094834  0.00481255]\n",
      "softmax alpha: [0.11113738 0.1137313  0.11016736 0.1116956  0.11181129 0.11042304\n",
      " 0.11036883 0.11059027 0.11007493]\n",
      "==================================================\n",
      "27 292\n",
      "alpha:         [0.1131131  0.10381262 0.02974356 0.09296238 0.07044621 0.08461102]\n",
      "softmax alpha: [0.17179369 0.17020334 0.1580521  0.16836657 0.16461796 0.16696634]\n",
      "==================================================\n",
      "28 313\n",
      "alpha:         [0.1826455  0.21696452 0.21870721 0.18463611 0.04467546]\n",
      "softmax alpha: [0.20223461 0.20929558 0.20966063 0.20263759 0.17617159]\n",
      "==================================================\n",
      "29 318\n",
      "alpha:         [0.00182672 0.00221251 0.00306488 0.00352453 0.00358711 0.00298857\n",
      " 0.00160159 0.00111616 0.00419426 0.00202599 0.00183196 0.00130572]\n",
      "softmax alpha: [0.0832822  0.08331434 0.08338538 0.08342372 0.08342894 0.08337902\n",
      " 0.08326346 0.08322305 0.08347961 0.0832988  0.08328264 0.08323883]\n",
      "==================================================\n",
      "30 326\n",
      "alpha:         [0.05004787 0.01693682 0.04684853 0.04075328 0.03671785 0.01829673\n",
      " 0.05185064]\n",
      "softmax alpha: [0.14466973 0.139958   0.14420763 0.14333132 0.14275408 0.14014846\n",
      " 0.14493077]\n",
      "==================================================\n",
      "31 327\n",
      "alpha:         [0.00503235 0.00574611 0.00928855 0.00436127 0.00577834 0.00612401\n",
      " 0.01001992 0.00964943 0.00873813 0.00760618]\n",
      "softmax alpha: [0.09977984 0.09985109 0.10020543 0.0997129  0.0998543  0.09988883\n",
      " 0.10027874 0.1002416  0.10015029 0.10003699]\n",
      "==================================================\n",
      "32 333\n",
      "alpha:         [0.10694175 0.20755799 0.06422989 0.08428142 0.19313269]\n",
      "softmax alpha: [0.19486915 0.21549648 0.18672117 0.19050301 0.21241019]\n",
      "==================================================\n",
      "33 334\n",
      "alpha:         [0.12393866 0.05446692 0.06685666 0.14209785 0.09970566 0.12470016]\n",
      "softmax alpha: [0.17028383 0.15885548 0.16083591 0.17340429 0.16620694 0.17041355]\n",
      "==================================================\n",
      "34 350\n",
      "alpha:         [0.06897873 0.03646408 0.05337022 0.04883527 0.06983305 0.1856985 ]\n",
      "softmax alpha: [0.16509174 0.15981017 0.16253491 0.16179949 0.16523284 0.18553084]\n",
      "==================================================\n",
      "35 393\n",
      "alpha:         [0.03515149 0.00096173 0.00094996 0.00198759 0.00084973 0.00122126\n",
      " 0.00095494 0.00030797 0.0004943  0.00054213 0.00096252 0.0136073\n",
      " 0.00150908 0.00041357 0.00276343]\n",
      "softmax alpha: [0.06876112 0.06644993 0.06644914 0.06651813 0.06644249 0.06646718\n",
      " 0.06644948 0.0664065  0.06641887 0.06642205 0.06644998 0.06729556\n",
      " 0.06648631 0.06641351 0.06656976]\n",
      "==================================================\n",
      "36 407\n",
      "alpha:         [0.10623613 0.11536426 0.17633992 0.06031018 0.15344166]\n",
      "softmax alpha: [0.19664757 0.19845081 0.21092802 0.18782059 0.20615301]\n",
      "==================================================\n",
      "37 429\n",
      "alpha:         [0.05132947 0.16384621 0.18926774 0.24606525 0.0522896 ]\n",
      "softmax alpha: [0.1823837  0.20410395 0.2093591  0.22159435 0.18255889]\n",
      "==================================================\n",
      "38 432\n",
      "alpha:         [0.01280837 0.13337901 0.05400818 0.05670954 0.09412822 0.05546342]\n",
      "softmax alpha: [0.15764478 0.17784544 0.16427536 0.16471973 0.17100009 0.1645146 ]\n",
      "==================================================\n",
      "39 435\n",
      "alpha:         [0.10867959 0.09372781 0.11910602 0.13421282 0.16065526]\n",
      "softmax alpha: [0.19705006 0.19412573 0.19911534 0.20214617 0.20756271]\n",
      "==================================================\n",
      "40 440\n",
      "alpha:         [0.0014066  0.00178817 0.00133228 0.00271885 0.00193575 0.00121493\n",
      " 0.00292609 0.00182478 0.00291728 0.00457118 0.001398   0.0009771\n",
      " 0.01037599]\n",
      "softmax alpha: [0.07682173 0.07685105 0.07681602 0.07692261 0.07686239 0.07680701\n",
      " 0.07693855 0.07685386 0.07693787 0.07706522 0.07682107 0.07678874\n",
      " 0.07751387]\n",
      "==================================================\n",
      "41 447\n",
      "alpha:         [0.10644402 0.03913178 0.06560627 0.05322207 0.0474102  0.07719638]\n",
      "softmax alpha: [0.17370434 0.16239675 0.16675354 0.16470117 0.16374672 0.16869748]\n",
      "==================================================\n",
      "42 449\n",
      "alpha:         [0.00072774 0.00021383 0.00263349 0.00014171 0.00152692 0.0013521\n",
      " 0.00639619 0.00018052 0.00046519 0.00015729 0.00150609 0.00047397\n",
      " 0.0004339  0.00031625 0.00026229 0.00051799 0.0002477 ]\n",
      "softmax alpha: [0.05880554 0.05877532 0.05891771 0.05877109 0.05885255 0.05884226\n",
      " 0.05913982 0.05877337 0.0587901  0.058772   0.05885133 0.05879062\n",
      " 0.05878826 0.05878134 0.05877817 0.0587932  0.05877731]\n",
      "==================================================\n",
      "43 451\n",
      "alpha:         [7.96178661e-04 5.55408885e-03 1.60600260e-04 1.18441510e-05\n",
      " 2.40989113e-03 4.31556341e-03 1.09146194e-03 1.53214727e-03\n",
      " 1.02041489e-04 1.08230155e-02 1.61463582e-05 5.49784683e-03\n",
      " 3.45719661e-03 1.47626410e-03 8.98113829e-04 4.21452415e-04\n",
      " 4.28901904e-05 3.98417764e-04 1.70620271e-04 7.50336384e-05]\n",
      "softmax alpha: [0.04994153 0.05017972 0.0499098  0.04990238 0.05002219 0.05011761\n",
      " 0.04995628 0.0499783  0.04990688 0.05044481 0.04990259 0.0501769\n",
      " 0.05007461 0.04997551 0.04994663 0.04992282 0.04990393 0.04992167\n",
      " 0.0499103  0.04990553]\n",
      "==================================================\n",
      "44 457\n",
      "alpha:         [0.10848046 0.14605772 0.07981259 0.08583803 0.11497112]\n",
      "softmax alpha: [0.20023407 0.20790147 0.19457528 0.19575123 0.20153795]\n",
      "==================================================\n",
      "45 466\n",
      "alpha:         [0.11115193 0.07294477 0.15381978 0.07240351 0.14143099 0.12326906]\n",
      "softmax alpha: [0.16636083 0.16012455 0.1736127  0.1600379  0.17147512 0.16838891]\n",
      "==================================================\n",
      "46 469\n",
      "alpha:         [0.00688251 0.01789028 0.03738573 0.01456644 0.03378633 0.03599124\n",
      " 0.0264327  0.04565772]\n",
      "softmax alpha: [0.12246137 0.12381685 0.12625439 0.12340598 0.12580077 0.12607846\n",
      " 0.12487907 0.1273031 ]\n",
      "==================================================\n",
      "47 476\n",
      "alpha:         [0.09664657 0.174945   0.11447623 0.11364545 0.04299787]\n",
      "softmax alpha: [0.19745894 0.21354105 0.20101114 0.20084421 0.18714467]\n",
      "==================================================\n",
      "48 501\n",
      "alpha:         [0.08697177 0.07270892 0.07853315 0.06817101 0.15812574 0.04995413]\n",
      "softmax alpha: [0.16677185 0.16441009 0.16537045 0.1636657  0.1790707  0.16071122]\n",
      "==================================================\n",
      "49 505\n",
      "alpha:         [0.08819916 0.03409216 0.09617268 0.08897249 0.05021537 0.04070419]\n",
      "softmax alpha: [0.17028657 0.16131771 0.17164978 0.17041831 0.16393975 0.16238788]\n",
      "==================================================\n",
      "50 514\n",
      "alpha:         [0.12690016 0.11622105 0.31199525 0.09982567 0.05621589]\n",
      "softmax alpha: [0.19616477 0.19408105 0.23605151 0.19092496 0.18277771]\n",
      "==================================================\n",
      "51 538\n",
      "alpha:         [0.292914   0.08151866 0.11478744 0.08818308 0.13126043]\n",
      "softmax alpha: [0.23191671 0.1877259  0.19407636 0.18898117 0.19729986]\n",
      "==================================================\n",
      "52 541\n",
      "alpha:         [0.00146013 0.00031527 0.00057457 0.00041601 0.00080463 0.00075928\n",
      " 0.00076709 0.00110611 0.00064581 0.00065707 0.00016792 0.03512263\n",
      " 0.00083492 0.00111482 0.00126109 0.00023564]\n",
      "softmax alpha: [0.0624085  0.06233709 0.06235326 0.06234337 0.0623676  0.06236477\n",
      " 0.06236526 0.06238641 0.0623577  0.0623584  0.06232791 0.06454508\n",
      " 0.06236949 0.06238695 0.06239608 0.06233213]\n",
      "==================================================\n",
      "53 542\n",
      "alpha:         [0.13276293 0.12558796 0.11955801 0.08743551 0.12647948]\n",
      "softmax alpha: [0.2028746  0.20142419 0.20021326 0.19388411 0.20160384]\n",
      "==================================================\n",
      "54 546\n",
      "alpha:         [0.0179445  0.04770887 0.04222397 0.03441304 0.03424824 0.01877018\n",
      " 0.10645197]\n",
      "softmax alpha: [0.13925251 0.14345957 0.14267487 0.14156478 0.14154146 0.13936754\n",
      " 0.15213927]\n",
      "==================================================\n",
      "55 548\n",
      "alpha:         [0.06841604 0.07569206 0.04681595 0.10540011 0.06439114 0.09774163]\n",
      "softmax alpha: [0.16530692 0.16651408 0.16177456 0.1715351  0.16464291 0.17022642]\n",
      "==================================================\n",
      "56 552\n",
      "alpha:         [0.07580318 0.18051223 0.12721498 0.16688695 0.20869666]\n",
      "softmax alpha: [0.18516348 0.20560322 0.19493203 0.20282081 0.21148046]\n",
      "==================================================\n",
      "57 563\n",
      "alpha:         [0.07039411 0.01094731 0.08237364 0.09711421 0.05784382 0.06961354]\n",
      "softmax alpha: [0.16755568 0.15788531 0.16957499 0.17209313 0.16546595 0.16742494]\n",
      "==================================================\n",
      "58 569\n",
      "alpha:         [0.05659188 0.08642006 0.03581253 0.07179363 0.05108786 0.05604045]\n",
      "softmax alpha: [0.16614082 0.17117115 0.16272415 0.16868575 0.16522889 0.16604923]\n",
      "==================================================\n",
      "59 592\n",
      "alpha:         [0.02308641 0.04398724 0.01465886 0.01900463 0.02987644 0.01026119\n",
      " 0.01871169 0.0127865 ]\n",
      "softmax alpha: [0.12518601 0.12783004 0.12413544 0.12467607 0.12603892 0.12359073\n",
      " 0.12463956 0.12390323]\n",
      "==================================================\n",
      "60 600\n",
      "alpha:         [0.05035639 0.13759167 0.06342152 0.1489615  0.09902094]\n",
      "softmax alpha: [0.19019352 0.2075303  0.19269473 0.20990335 0.19967811]\n",
      "==================================================\n",
      "61 644\n",
      "alpha:         [0.08769216 0.07392854 0.12728531 0.14137721 0.0864818 ]\n",
      "softmax alpha: [0.1968248  0.19413433 0.20477404 0.20768012 0.19658671]\n",
      "==================================================\n",
      "62 646\n",
      "alpha:         [0.13850763 0.174968   0.04564018 0.07671543 0.05385854]\n",
      "softmax alpha: [0.20801436 0.2157386  0.18956646 0.19554977 0.19113081]\n",
      "==================================================\n",
      "63 664\n",
      "alpha:         [0.00170749 0.00153145 0.00227918 0.00189249 0.00241168 0.0031113\n",
      " 0.00175304 0.00164117 0.00166826 0.00190595 0.00119711 0.00247999\n",
      " 0.00184145]\n",
      "softmax alpha: [0.076904   0.07689046 0.07694798 0.07691823 0.07695817 0.07701203\n",
      " 0.0769075  0.0768989  0.07690098 0.07691926 0.07686476 0.07696343\n",
      " 0.0769143 ]\n",
      "==================================================\n",
      "64 689\n",
      "alpha:         [0.0082924  0.01080808 0.01942304 0.03386453 0.00813962 0.01232583\n",
      " 0.00779516 0.01575419 0.0131175 ]\n",
      "softmax alpha: [0.11043218 0.11071034 0.11166822 0.11329258 0.11041531 0.1108785\n",
      " 0.11037728 0.11125928 0.11096631]\n",
      "==================================================\n",
      "65 696\n",
      "alpha:         [0.02070379 0.01916208 0.01083709 0.01158572 0.01938616 0.00833624\n",
      " 0.01390076 0.01124607 0.01068374]\n",
      "softmax alpha: [0.11185941 0.11168709 0.11076115 0.1108441  0.11171212 0.1104845\n",
      " 0.11110101 0.11080646 0.11074417]\n",
      "==================================================\n",
      "66 704\n",
      "alpha:         [0.00514082 0.0077296  0.00592516 0.00618161 0.00651468 0.0340981\n",
      " 0.00485318 0.00836653 0.00796446 0.01099325]\n",
      "softmax alpha: [0.09953404 0.09979205 0.09961214 0.09963769 0.09967088 0.10245841\n",
      " 0.09950542 0.09985563 0.09981549 0.10011826]\n",
      "==================================================\n",
      "67 727\n",
      "alpha:         [0.14487581 0.1088623  0.16724441 0.13046327 0.09580231]\n",
      "softmax alpha: [0.20304363 0.19586142 0.20763661 0.20013824 0.1933201 ]\n",
      "==================================================\n",
      "68 735\n",
      "alpha:         [0.09086602 0.15018671 0.07445493 0.18194201 0.11565599]\n",
      "softmax alpha: [0.19359996 0.20543191 0.1904487  0.21206015 0.19845928]\n",
      "==================================================\n",
      "69 740\n",
      "alpha:         [0.1385392  0.1045241  0.15831878 0.11937141 0.13729847]\n",
      "softmax alpha: [0.20135681 0.19462282 0.20537922 0.19753401 0.20110714]\n",
      "==================================================\n",
      "70 741\n",
      "alpha:         [0.09169686 0.04411123 0.04011751 0.0570984  0.05392912 0.02720986\n",
      " 0.03615543]\n",
      "softmax alpha: [0.14890463 0.14198485 0.14141894 0.14384086 0.14338571 0.13960528\n",
      " 0.14085973]\n",
      "==================================================\n",
      "71 747\n",
      "alpha:         [0.00355752 0.01095451 0.0122759  0.00495097 0.01297343 0.04057418\n",
      " 0.00928867 0.02334662 0.00549115]\n",
      "softmax alpha: [0.10998178 0.11079833 0.11094483 0.11013514 0.11102225 0.11412922\n",
      " 0.11061391 0.1121799  0.11019465]\n",
      "==================================================\n",
      "72 758\n",
      "alpha:         [0.00462907 0.00489079 0.00547026 0.0075435  0.0076791  0.00225165\n",
      " 0.00591165 0.00605246 0.00166589 0.00202681]\n",
      "softmax alpha: [0.09998148 0.10000765 0.10006562 0.1002733  0.10028689 0.09974407\n",
      " 0.1001098  0.1001239  0.09968566 0.09972164]\n",
      "==================================================\n",
      "73 775\n",
      "alpha:         [0.07865237 0.08215473 0.32601416 0.34700547 0.12319317]\n",
      "softmax alpha: [0.17738336 0.17800571 0.22716465 0.23198353 0.18546275]\n",
      "==================================================\n",
      "74 777\n",
      "alpha:         [0.01706069 0.01670529 0.02227092 0.01232431 0.01503268 0.01872921\n",
      " 0.02372569 0.01795594]\n",
      "softmax alpha: [0.12488495 0.12484057 0.12553732 0.12429484 0.12463194 0.12509349\n",
      " 0.12572009 0.1249968 ]\n",
      "==================================================\n",
      "75 778\n",
      "alpha:         [0.03635998 0.09540287 0.13551131 0.05229781 0.19869233]\n",
      "softmax alpha: [0.18665873 0.19801145 0.2061148  0.1896575  0.21955753]\n",
      "==================================================\n",
      "76 781\n",
      "alpha:         [0.04073248 0.12440389 0.13135479 0.08001534 0.15367813]\n",
      "softmax alpha: [0.18720412 0.20354172 0.20496145 0.19470439 0.20958832]\n",
      "==================================================\n",
      "77 788\n",
      "alpha:         [0.08632471 0.02869668 0.0977545  0.12135902 0.06566851 0.13128979]\n",
      "softmax alpha: [0.1662041  0.15689684 0.16811468 0.17213015 0.16280617 0.17384805]\n",
      "==================================================\n",
      "78 810\n",
      "alpha:         [0.17154711 0.15038817 0.11745025 0.11174356 0.09719677]\n",
      "softmax alpha: [0.20847668 0.20411187 0.19749837 0.19637451 0.19353857]\n",
      "==================================================\n",
      "79 817\n",
      "alpha:         [0.00168307 0.00111348 0.0049219  0.0019067  0.00188391 0.00222702\n",
      " 0.00162095 0.00199304 0.02177383 0.00127913 0.00192109 0.00140824\n",
      " 0.00226724]\n",
      "softmax alpha: [0.07677939 0.07673567 0.07702847 0.07679656 0.07679481 0.07682116\n",
      " 0.07677462 0.07680319 0.07833754 0.07674838 0.07679767 0.07675829\n",
      " 0.07682425]\n",
      "==================================================\n",
      "80 821\n",
      "alpha:         [0.21285694 0.22199029 0.12358097 0.04068807 0.02375337]\n",
      "softmax alpha: [0.21770811 0.21970562 0.19911435 0.18327474 0.18019717]\n",
      "==================================================\n",
      "81 859\n",
      "alpha:         [0.10495915 0.13272159 0.08035814 0.16310557 0.17033104]\n",
      "softmax alpha: [0.19488328 0.20036951 0.19014744 0.20655097 0.2080488 ]\n",
      "==================================================\n",
      "82 864\n",
      "alpha:         [0.07102982 0.04526377 0.05610856 0.10859347 0.10421594 0.05104367]\n",
      "softmax alpha: [0.16633439 0.16210335 0.1638709  0.17270135 0.171947   0.16304301]\n",
      "==================================================\n",
      "83 865\n",
      "alpha:         [0.10853047 0.11786436 0.03164451 0.12169581 0.03290872 0.10473433]\n",
      "softmax alpha: [0.17029963 0.17189663 0.15769668 0.17255651 0.15789617 0.16965438]\n",
      "==================================================\n",
      "84 877\n",
      "alpha:         [0.04471697 0.21310874 0.03146951 0.06430305 0.15476873]\n",
      "softmax alpha: [0.18845328 0.22301564 0.18597321 0.19218072 0.21037715]\n",
      "==================================================\n",
      "85 919\n",
      "alpha:         [0.01152521 0.01848828 0.01659942 0.00693153 0.00716729 0.00964381\n",
      " 0.01230142 0.01598675 0.02610672]\n",
      "softmax alpha: [0.11085    0.11162455 0.11141391 0.11034196 0.11036798 0.11064164\n",
      " 0.11093608 0.11134567 0.1124782 ]\n",
      "==================================================\n",
      "86 928\n",
      "alpha:         [0.0865602  0.04242427 0.14668741 0.12538006 0.07361892]\n",
      "softmax alpha: [0.19819559 0.18963828 0.2104781  0.20604081 0.19564721]\n",
      "==================================================\n",
      "87 939\n",
      "alpha:         [0.04373224 0.02717753 0.12340375 0.13435094 0.12481241]\n",
      "softmax alpha: [0.19062789 0.18749808 0.2064369  0.20870922 0.20672791]\n",
      "==================================================\n",
      "88 940\n",
      "alpha:         [0.17218878 0.04354381 0.08148388 0.15155781 0.3379632 ]\n",
      "softmax alpha: [0.20191983 0.1775453  0.1844108  0.19779671 0.23832736]\n",
      "==================================================\n",
      "89 946\n",
      "alpha:         [0.00700276 0.01101379 0.00739208 0.01495876 0.01935295 0.00371439\n",
      " 0.01029135 0.00685826 0.01111995]\n",
      "softmax alpha: [0.1107565  0.11120164 0.11079963 0.1116412  0.11213285 0.11039289\n",
      " 0.11112134 0.1107405  0.11121345]\n",
      "==================================================\n",
      "90 958\n",
      "alpha:         [0.03681418 0.09082692 0.03903709 0.0393622  0.03814673 0.06956371\n",
      " 0.07133694]\n",
      "softmax alpha: [0.14025195 0.14803566 0.14056407 0.14060977 0.14043897 0.14492118\n",
      " 0.14517839]\n",
      "==================================================\n",
      "91 1010\n",
      "alpha:         [0.19216965 0.15659723 0.0760642  0.04718797 0.03243829 0.06330809]\n",
      "softmax alpha: [0.18342092 0.17701088 0.16331457 0.15866609 0.15634299 0.16124454]\n",
      "==================================================\n",
      "92 1022\n",
      "alpha:         [0.02085498 0.03076987 0.03630186 0.034829   0.01922177 0.01704707\n",
      " 0.00830343 0.03197805]\n",
      "softmax alpha: [0.12448831 0.12572874 0.1264262  0.12624013 0.12428516 0.12401517\n",
      " 0.12293556 0.12588073]\n",
      "==================================================\n",
      "93 1034\n",
      "alpha:         [0.06282196 0.03543905 0.04698559 0.02395578 0.04254714 0.04295054\n",
      " 0.05025123]\n",
      "softmax alpha: [0.14562573 0.14169218 0.14333771 0.14007439 0.14270292 0.1427605\n",
      " 0.14380657]\n",
      "==================================================\n",
      "94 1043\n",
      "alpha:         [0.0529016  0.02788947 0.04924235 0.03702334 0.02187969 0.03576529\n",
      " 0.04568717]\n",
      "softmax alpha: [0.144903   0.14132362 0.14437374 0.14262037 0.14047684 0.14244106\n",
      " 0.14386137]\n",
      "==================================================\n",
      "95 1083\n",
      "alpha:         [0.01268317 0.01331466 0.01474551 0.01421075 0.03659625 0.02952032\n",
      " 0.02601826 0.00984224]\n",
      "softmax alpha: [0.12413118 0.1242096  0.12438745 0.12432095 0.12713532 0.12623889\n",
      " 0.12579757 0.12377904]\n",
      "==================================================\n",
      "96 1093\n",
      "alpha:         [0.10286856 0.13372878 0.04457589 0.02567079 0.08753112 0.12960322]\n",
      "softmax alpha: [0.16913901 0.17444005 0.15956131 0.15657312 0.16656464 0.17372187]\n",
      "==================================================\n",
      "97 1098\n",
      "alpha:         [0.04760234 0.08775991 0.02932719 0.05807556 0.05295632 0.03607182\n",
      " 0.0297782 ]\n",
      "softmax alpha: [0.14266085 0.14850635 0.14007738 0.14416282 0.1434267  0.14102534\n",
      " 0.14014057]\n",
      "==================================================\n",
      "98 1103\n",
      "alpha:         [0.09142162 0.04270483 0.07957259 0.06584584 0.05613929 0.06708697]\n",
      "softmax alpha: [0.17074427 0.16262552 0.16873305 0.16643272 0.16482504 0.16663941]\n",
      "==================================================\n",
      "99 1116\n",
      "alpha:         [0.03182361 0.00926275 0.0175488  0.01493946 0.01378528 0.01385591\n",
      " 0.01786301 0.01039508 0.01056208]\n",
      "softmax alpha: [0.11293067 0.11041138 0.11133005 0.11103993 0.11091185 0.11091968\n",
      " 0.11136504 0.11053647 0.11055493]\n",
      "==================================================\n",
      "100 1130\n",
      "alpha:         [0.12129242 0.15719123 0.10996675 0.17130402 0.16970873]\n",
      "softmax alpha: [0.19507704 0.20220729 0.19288012 0.20508123 0.20475432]\n",
      "==================================================\n",
      "101 1133\n",
      "alpha:         [0.24112461 0.13690419 0.07286755 0.08088739 0.07620918]\n",
      "softmax alpha: [0.22491772 0.20265687 0.19008619 0.19161678 0.19072245]\n",
      "==================================================\n",
      "102 1140\n",
      "alpha:         [0.05694802 0.06747691 0.10204233 0.08540801 0.06720687 0.0698623 ]\n",
      "softmax alpha: [0.16369587 0.16542851 0.17124659 0.16842158 0.16538385 0.16582359]\n",
      "==================================================\n",
      "103 1149\n",
      "alpha:         [0.06877933 0.11328114 0.1138413  0.1275671  0.14083568]\n",
      "softmax alpha: [0.19131925 0.20002559 0.20013767 0.20290366 0.20561384]\n",
      "==================================================\n",
      "104 1161\n",
      "alpha:         [0.03378439 0.03917815 0.05425951 0.05131601 0.05415201 0.04711103\n",
      " 0.02370954]\n",
      "softmax alpha: [0.14148777 0.14225298 0.14441461 0.14399015 0.14439909 0.14338595\n",
      " 0.14006946]\n",
      "==================================================\n",
      "105 1182\n",
      "alpha:         [0.01688847 0.02083804 0.02178354 0.016559   0.01373691 0.01050526\n",
      " 0.01720689 0.00788955]\n",
      "softmax alpha: [0.12515041 0.12564568 0.12576453 0.12510918 0.12475661 0.12435409\n",
      " 0.12519026 0.12402924]\n",
      "==================================================\n",
      "106 1195\n",
      "alpha:         [0.18971961 0.16381716 0.05426394 0.10050704 0.07985323]\n",
      "softmax alpha: [0.21466815 0.20917911 0.18747353 0.19634646 0.19233275]\n",
      "==================================================\n",
      "107 1197\n",
      "alpha:         [0.09639918 0.03989974 0.11165076 0.10172557 0.16685453]\n",
      "softmax alpha: [0.19846102 0.18755897 0.20151107 0.19952093 0.21294802]\n",
      "==================================================\n",
      "108 1206\n",
      "alpha:         [0.00587346 0.01808699 0.00584009 0.01190598 0.01466102 0.01486359\n",
      " 0.00692602 0.00816887 0.01173466]\n",
      "softmax alpha: [0.11055352 0.11191205 0.11054984 0.11122246 0.1115293  0.1115519\n",
      " 0.11066995 0.11080758 0.1112034 ]\n",
      "==================================================\n",
      "109 1209\n",
      "alpha:         [0.0597958  0.08406062 0.09416078 0.12950755 0.09380622]\n",
      "softmax alpha: [0.19356144 0.19831562 0.20032879 0.20753639 0.20025777]\n",
      "==================================================\n",
      "110 1220\n",
      "alpha:         [0.0017899  0.00135027 0.00135183 0.0030664  0.00172188 0.00170778\n",
      " 0.00166861 0.00215386 0.002017   0.00104832 0.00242502 0.00156901\n",
      " 0.00175065]\n",
      "softmax alpha: [0.07692099 0.07688718 0.0768873  0.07701924 0.07691575 0.07691467\n",
      " 0.07691166 0.07694899 0.07693846 0.07686396 0.07696985 0.076904\n",
      " 0.07691797]\n",
      "==================================================\n",
      "111 1221\n",
      "alpha:         [0.07222525 0.07857474 0.08494848 0.128605   0.05474925 0.0706466 ]\n",
      "softmax alpha: [0.16506357 0.16611497 0.16717713 0.17463715 0.16220398 0.1648032 ]\n",
      "==================================================\n",
      "112 1232\n",
      "alpha:         [0.1204318  0.08062601 0.02930584 0.14629582 0.18696407]\n",
      "softmax alpha: [0.20125277 0.1933991  0.1837242  0.20652588 0.21509805]\n",
      "==================================================\n",
      "113 1236\n",
      "alpha:         [0.09810151 0.13089839 0.11676952 0.06230325 0.085881  ]\n",
      "softmax alpha: [0.19980528 0.20646691 0.20357028 0.19277911 0.19737842]\n",
      "==================================================\n",
      "114 1247\n",
      "alpha:         [0.11319139 0.20804268 0.08800158 0.09873772 0.0747926  0.06845812]\n",
      "softmax alpha: [0.16725619 0.1838974  0.16309566 0.16485611 0.1609555  0.15993915]\n",
      "==================================================\n",
      "115 1266\n",
      "alpha:         [0.07943585 0.10984731 0.05753751 0.04379036 0.04057082 0.11650454]\n",
      "softmax alpha: [0.16739636 0.17256533 0.16377051 0.16153453 0.1610153  0.17371797]\n",
      "==================================================\n",
      "116 1285\n",
      "alpha:         [0.03479539 0.11552076 0.17312009 0.16788201 0.17665572]\n",
      "softmax alpha: [0.18092389 0.19613473 0.20776365 0.20667821 0.20849953]\n",
      "==================================================\n",
      "117 1287\n",
      "alpha:         [0.14916628 0.2076978  0.19091695 0.20178241 0.20190703]\n",
      "softmax alpha: [0.19189828 0.2034656  0.20007976 0.20226557 0.20229078]\n",
      "==================================================\n",
      "118 1300\n",
      "alpha:         [0.0709402  0.04276559 0.10131467 0.02595337 0.07120665 0.10123238]\n",
      "softmax alpha: [0.16694236 0.16230447 0.17209095 0.15959858 0.16698685 0.17207679]\n",
      "==================================================\n",
      "119 1301\n",
      "alpha:         [0.00232754 0.00203542 0.00327739 0.00372185 0.00170923 0.00183855\n",
      " 0.00150758 0.00274895 0.00227594 0.00237844 0.00199411 0.0021478\n",
      " 0.00162103]\n",
      "softmax alpha: [0.07692705 0.07690458 0.07700016 0.07703439 0.0768795  0.07688944\n",
      " 0.076864   0.07695948 0.07692308 0.07693097 0.07690141 0.07691323\n",
      " 0.07687272]\n",
      "==================================================\n",
      "120 1309\n",
      "alpha:         [0.09879076 0.1057847  0.09165843 0.12617982 0.10671495 0.07507245]\n",
      "softmax alpha: [0.16632859 0.16749596 0.1651465  0.17094713 0.16765184 0.16242997]\n",
      "==================================================\n",
      "121 1310\n",
      "alpha:         [0.09696793 0.01785874 0.10263138 0.12780404 0.08728331]\n",
      "softmax alpha: [0.20196728 0.18660545 0.20311435 0.20829218 0.20002074]\n",
      "==================================================\n",
      "122 1316\n",
      "alpha:         [0.05721707 0.09193531 0.10007558 0.06567112 0.14251502 0.08487492]\n",
      "softmax alpha: [0.16116832 0.16686206 0.16822591 0.16253662 0.17551899 0.1656881 ]\n",
      "==================================================\n",
      "123 1327\n",
      "alpha:         [0.05272206 0.05522215 0.08234419 0.0907698  0.05624746 0.02305253\n",
      " 0.02249909]\n",
      "softmax alpha: [0.14253387 0.14289066 0.14681918 0.14806145 0.14303724 0.13836708\n",
      " 0.13829052]\n",
      "==================================================\n",
      "124 1330\n",
      "alpha:         [0.12339677 0.13919749 0.08407192 0.1071099  0.14872619]\n",
      "softmax alpha: [0.20052688 0.20372052 0.19279423 0.19728738 0.20567099]\n",
      "==================================================\n",
      "125 1342\n",
      "alpha:         [0.02181401 0.02713843 0.01425539 0.00879564 0.01363091 0.01642726\n",
      " 0.01641855 0.03010251]\n",
      "softmax alpha: [0.12540294 0.12607242 0.12445864 0.12378098 0.12438094 0.12472924\n",
      " 0.12472816 0.12644666]\n",
      "==================================================\n",
      "126 1354\n",
      "alpha:         [0.00449199 0.00163209 0.00074684 0.00124966 0.00043255 0.00035384\n",
      " 0.00057739 0.00046194 0.00037256 0.0006637  0.00136857 0.00106362\n",
      " 0.00168545 0.00131049 0.0005062 ]\n",
      "softmax alpha: [0.06689129 0.06670026 0.06664124 0.06667476 0.0666203  0.06661506\n",
      " 0.06662995 0.06662226 0.0666163  0.0666357  0.06668269 0.06666235\n",
      " 0.06670382 0.06667881 0.06662521]\n",
      "==================================================\n",
      "127 1372\n",
      "alpha:         [0.02530601 0.01992866 0.04406612 0.12729397 0.09928795 0.09035188]\n",
      "softmax alpha: [0.15961845 0.15876243 0.16264118 0.17675671 0.17187513 0.17034609]\n",
      "==================================================\n",
      "128 1385\n",
      "alpha:         [0.17100089 0.11738252 0.20849253 0.15814623 0.08135972]\n",
      "softmax alpha: [0.20460426 0.19392264 0.21242082 0.20199097 0.18706132]\n",
      "==================================================\n",
      "129 1393\n",
      "alpha:         [0.08879953 0.1367572  0.09186355 0.06178429 0.11787327]\n",
      "softmax alpha: [0.1978223  0.20754056 0.19842936 0.19254962 0.20365816]\n",
      "==================================================\n",
      "130 1399\n",
      "alpha:         [0.05111755 0.09579886 0.11170611 0.21077572 0.09740491]\n",
      "softmax alpha: [0.1876651  0.19624037 0.19938698 0.22015176 0.19655579]\n",
      "==================================================\n",
      "131 1402\n",
      "alpha:         [0.07604209 0.04388062 0.0426237  0.03518156 0.04732933 0.08956093]\n",
      "softmax alpha: [0.17004625 0.16466432 0.16445748 0.1632381  0.16523318 0.17236068]\n",
      "==================================================\n",
      "132 1409\n",
      "alpha:         [0.07701605 0.06344179 0.05593374 0.09365793 0.09749731 0.07826393]\n",
      "softmax alpha: [0.16654514 0.16429969 0.16307074 0.16933996 0.16999137 0.1667531 ]\n",
      "==================================================\n",
      "133 1429\n",
      "alpha:         [0.06413506 0.0713725  0.07802517 0.06195236 0.02345834 0.0568986 ]\n",
      "softmax alpha: [0.1674481  0.1686644  0.16979021 0.16708301 0.16077353 0.16624075]\n",
      "==================================================\n",
      "134 1436\n",
      "alpha:         [0.06315143 0.05955422 0.0711286  0.02209047 0.02510072 0.03970348\n",
      " 0.03030831]\n",
      "softmax alpha: [0.14553142 0.14500886 0.146697   0.13967678 0.14009788 0.14215871\n",
      " 0.14082935]\n",
      "==================================================\n",
      "135 1437\n",
      "alpha:         [0.09061727 0.11566657 0.133172   0.13213517 0.22851399]\n",
      "softmax alpha: [0.19014705 0.19497026 0.19841334 0.19820773 0.21826162]\n",
      "==================================================\n",
      "136 1442\n",
      "alpha:         [0.11588332 0.09951618 0.14161381 0.14566321 0.20221116]\n",
      "softmax alpha: [0.19492334 0.19175897 0.2000039  0.20081544 0.21249835]\n",
      "==================================================\n",
      "137 1466\n",
      "alpha:         [0.1212162  0.04976195 0.17182685 0.08003385 0.14395046]\n",
      "softmax alpha: [0.2013855  0.18749772 0.21184007 0.19326042 0.20601629]\n",
      "==================================================\n",
      "138 1470\n",
      "alpha:         [0.12684495 0.17282853 0.07224953 0.19499188 0.20617545]\n",
      "softmax alpha: [0.19428756 0.20343019 0.1839647  0.20798922 0.21032834]\n",
      "==================================================\n",
      "139 1493\n",
      "alpha:         [0.30182587 0.12495185 0.13712534 0.11400402 0.11810595]\n",
      "softmax alpha: [0.2300456  0.19275184 0.19511264 0.19065313 0.19143679]\n",
      "==================================================\n",
      "140 1494\n",
      "alpha:         [0.02871478 0.10587626 0.05783519 0.1229111  0.03290628]\n",
      "softmax alpha: [0.19183766 0.20722621 0.19750619 0.21078651 0.19264344]\n",
      "==================================================\n",
      "141 1508\n",
      "alpha:         [0.00356926 0.00947755 0.00651178 0.00422168 0.00926629 0.00231082\n",
      " 0.0030091  0.00814353 0.00149613 0.00588645]\n",
      "softmax alpha: [0.09981779 0.10040929 0.10011194 0.09988293 0.10038807 0.09969225\n",
      " 0.09976189 0.10027543 0.09961107 0.10004935]\n",
      "==================================================\n",
      "142 1516\n",
      "alpha:         [0.01045529 0.01119751 0.0200534  0.01638255 0.02987964 0.02407064\n",
      " 0.01296776 0.02962258]\n",
      "softmax alpha: [0.12389237 0.12398436 0.12508723 0.12462889 0.12632242 0.12559074\n",
      " 0.12420404 0.12628995]\n",
      "==================================================\n",
      "143 1518\n",
      "alpha:         [0.03947023 0.08456264 0.06553874 0.12135499 0.0871857  0.09790687]\n",
      "softmax alpha: [0.15956811 0.16692811 0.1637825  0.17318417 0.16736655 0.16917057]\n",
      "==================================================\n",
      "144 1525\n",
      "alpha:         [0.01084966 0.01580011 0.01420913 0.00275176 0.01552591 0.01526005\n",
      " 0.01918639 0.0207567  0.00970295]\n",
      "softmax alpha: [0.11078428 0.11133407 0.11115708 0.10989078 0.11130354 0.11127396\n",
      " 0.11171171 0.11188727 0.11065731]\n",
      "==================================================\n",
      "145 1529\n",
      "alpha:         [0.09861708 0.10383827 0.1210757  0.09204184 0.13479082]\n",
      "softmax alpha: [0.19769762 0.19873254 0.20218787 0.19640197 0.20498   ]\n",
      "==================================================\n",
      "146 1547\n",
      "alpha:         [0.00445189 0.00395247 0.00475174 0.00764808 0.00281053 0.00310073\n",
      " 0.00541942 0.00399822 0.0017727  0.00252637 0.00668033]\n",
      "softmax alpha: [0.09092432 0.09087892 0.09095159 0.0912154  0.0907752  0.09080155\n",
      " 0.09101233 0.09088308 0.09068104 0.09074941 0.09112716]\n",
      "==================================================\n",
      "147 1554\n",
      "alpha:         [0.23121491 0.16260048 0.03088977 0.20663199 0.13320837]\n",
      "softmax alpha: [0.2157725  0.20146389 0.17660215 0.21053285 0.19562862]\n",
      "==================================================\n",
      "148 1563\n",
      "alpha:         [0.01592301 0.02472635 0.01497999 0.02169938 0.02062451 0.01051312\n",
      " 0.02982667 0.01216065 0.01044   ]\n",
      "softmax alpha: [0.11089195 0.11187248 0.11078742 0.11153435 0.11141453 0.11029365\n",
      " 0.11244452 0.11047551 0.11028559]\n",
      "==================================================\n",
      "149 1573\n",
      "alpha:         [0.10655673 0.13322151 0.07528906 0.07352329 0.1102761  0.09248734]\n",
      "softmax alpha: [0.16796839 0.17250748 0.16279767 0.16251046 0.16859429 0.16562172]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "\n",
    "#with Embedding\n",
    "result = np.zeros((usr_test_amount, movie_nb))\n",
    "RS = np.zeros((usr_test_amount, movie_nb))\n",
    "\n",
    "#test_idx --> Test 的 index length = 150\n",
    "test_yes_id = []\n",
    "\n",
    "for s in range(usr_test_amount):\n",
    "    print(s, test_idx[s])\n",
    "\n",
    "    yes = []\n",
    "    sample = random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]]))\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha = np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        \n",
    "# #         ''' Observe each part in attention\n",
    "#         WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "#         WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "#         WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "#         WvVy = np.sum(np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))\n",
    "#         print('The sum of each par -->',\n",
    "#               '\\nw1:',testW1,\n",
    "#               '\\nWuU:',WuUu,\n",
    "#               '\\nwyY:',WyYy,\n",
    "#               '\\nWaA:',WaAa,\n",
    "#               '\\nWvV:',WvVy)\n",
    "# #         '''\n",
    "        \n",
    "        alpha[a] = np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T) +\n",
    "                                np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T) +\n",
    "                                np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T) +\n",
    "                                np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))))*r\n",
    "        \n",
    "    mul = np.zeros((1,latent_dim))\n",
    "    \n",
    "    print(\"{:<15}{}\".format('alpha:', alpha))\n",
    "    print(\"{:<15}{}\".format('softmax alpha:', softmax(alpha)))\n",
    "    print('==================================================')\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        mul += alpha[i] * A[sample[i]] #attention alpha * Ai part \n",
    "    new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "    for k in range(movie_nb):\n",
    "        result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 165)\n"
     ]
    }
   ],
   "source": [
    "#取出test的資料\n",
    "print(RS.shape)\n",
    "\n",
    "testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "target = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "        \n",
    "for z in range(usr_test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    # positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    # not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "#     print(user_id)\n",
    "#     print(youtube_t)\n",
    "#     print(youtube_f)\n",
    "    \n",
    "    #前面放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "        \n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "    \n",
    "#     print(testRS[z])\n",
    "#     print(target[z])\n",
    "#     print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n",
      "num of positive data in testing: 1078.0\n"
     ]
    }
   ],
   "source": [
    "print(target.shape, testRS.shape)\n",
    "sumtarget = np.sum(target)\n",
    "print('num of positive data in testing:', sumtarget) # whole matrix: 4800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "def topN(RSls, n):\n",
    "    maxn = np.argsort(RSls)[::-1][:n]\n",
    "    return maxn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32)\n"
     ]
    }
   ],
   "source": [
    "all_sort = []\n",
    "\n",
    "for i in range(usr_test_amount):\n",
    "    all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "    \n",
    "all_sort = np.asarray(all_sort)\n",
    "print(all_sort.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    return dcg\n",
    "\n",
    "def NDCG(target, testRS, num_ndcg): #target是真正的喜好\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(usr_test_amount): # the number of testing users\n",
    "        idcg = DCG(target[m][:num_ndcg])\n",
    "        \n",
    "        pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        \n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        total_ndcg += ndcg\n",
    "        \n",
    "    avg_ndcg = total_ndcg/usr_test_amount\n",
    "    return avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def MAP(target,testRS):\n",
    "    total_prec = 0\n",
    "    for u in range(usr_test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec += average_precision_score(y_true, y_scores)\n",
    "        \n",
    "    Map_value = total_prec/usr_test_amount\n",
    "    \n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1\n",
      "Num of TP: 65\n",
      "prec: 0.43333333333333335\n",
      "recall: 0.06029684601113173\n",
      "F1_score: 0.10586319218241043\n",
      "*****\n",
      "Top 3\n",
      "Num of TP: 233\n",
      "prec: 0.5177777777777778\n",
      "recall: 0.21614100185528756\n",
      "F1_score: 0.3049738219895288\n",
      "*****\n",
      "Top 5\n",
      "Num of TP: 498\n",
      "prec: 0.664\n",
      "recall: 0.4619666048237477\n",
      "F1_score: 0.5448577680525164\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "NDCG@ 1\n",
      "NDCG score: 0.43333333333333335\n",
      "*****\n",
      "NDCG@ 3\n",
      "NDCG score: 0.38577322716856316\n",
      "*****\n",
      "NDCG@ 5\n",
      "NDCG score: 0.36950073286430907\n",
      "*****\n",
      "NDCG@ 10\n",
      "NDCG score: 0.4159441133846608\n",
      "*****\n",
      "NDCG@ 15\n",
      "NDCG score: 0.4924613621549381\n",
      "*****\n",
      "NDCG@ 20\n",
      "NDCG score: 0.5483583847753011\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "MAP: 0.4033369954123207\n"
     ]
    }
   ],
   "source": [
    "# Top N\n",
    "N = [1, 3, 5]\n",
    "correct = 0\n",
    "\n",
    "for n in N:\n",
    "    print('Top', n)\n",
    "    \n",
    "    for i in range(len(testRS)):\n",
    "        topn = topN(testRS[i], n)\n",
    "        sum_target = int(np.sum(target[i]))\n",
    "        \n",
    "        TP = 0\n",
    "        for i in topn:\n",
    "            if i < sum_target:\n",
    "                TP += 1\n",
    "                \n",
    "        correct += TP\n",
    "\n",
    "    print('Num of TP:', correct)\n",
    "\n",
    "    prec = correct/(len(testRS)*n)\n",
    "    recall = correct/sumtarget\n",
    "    \n",
    "    print('prec:', prec)\n",
    "    print('recall:', recall)\n",
    "    print('F1_score:', F1_score(prec, recall))\n",
    "    \n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# NDCG\n",
    "num_ndcgs = [1, 3, 5, 10, 15, 20]\n",
    "for num_ndcg in num_ndcgs:\n",
    "    print('NDCG@', num_ndcg)\n",
    "    print('NDCG score:', NDCG(target, testRS, num_ndcg))\n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# MAP\n",
    "print('MAP:', MAP(target,testRS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
