{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_NAME = 'MRM_E200_10epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def newPath(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def writeProgress(msg, count, total):\n",
    "    sys.stdout.write(msg + \"{:.2%}\\r\".format(count/total))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 4882)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_4882.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n",
      "150 32\n",
      "128 4882 200\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "print(usr_nb, movie_nb)\n",
    "\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "print(usr_test_amount, movie_test_amount)\n",
    "\n",
    "latent_dim = 128 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "embedding_dims = 200\n",
    "print(latent_dim, ft_dim, embedding_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)\n",
    "# print('Before:', usr_genre)\n",
    "# print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "Avg of followers: 142.0969696969697\n",
      "The num of followers over 5: 163\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "print('Avg of followers:', np.mean(moive_followers))\n",
    "\n",
    "asc = np.sort(moive_followers)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)\n",
    "\n",
    "over5 = 0\n",
    "for num in moive_followers:\n",
    "    if num >= 5:\n",
    "        over5 += 1\n",
    "print('The num of followers over 5:', over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 50: 125\n",
      "Over 100: 89\n",
      "Over 150: 58\n",
      "Over 200: 42\n",
      "Over 250: 31\n",
      "Over 300: 21\n"
     ]
    }
   ],
   "source": [
    "print('Over 50:', np.sum(moive_followers >= 50))\n",
    "print('Over 100:', np.sum(moive_followers >= 100))\n",
    "print('Over 150:', np.sum(moive_followers >= 150))\n",
    "print('Over 200:', np.sum(moive_followers >= 200))\n",
    "print('Over 250:', np.sum(moive_followers >= 250))\n",
    "print('Over 300:', np.sum(moive_followers >= 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,) [  0   2   3   4   9  12  24  28  30  34  40  44  49  55  57  58  60  66\n",
      "  68  78  80  81  84  86  87  99 101 102 112 119 122 123 125 126 127 128\n",
      " 129 134 144 156 161 164]\n",
      "32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]\n"
     ]
    }
   ],
   "source": [
    "over200_idx = np.nonzero(moive_followers >= 200)[0]\n",
    "print(over200_idx.shape, over200_idx)\n",
    "\n",
    "random.seed(42)\n",
    "movie_test_idx = sorted(random.sample(list(over200_idx), movie_test_amount))\n",
    "print(len(movie_test_idx), movie_test_idx) # 32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "Avg of followers: 14.820480404551201\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "print('Avg of followers:', np.mean(each_user))\n",
    "\n",
    "asc = np.sort(each_user)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 10: 1582\n",
      "Over 12: 937\n",
      "Over 14: 613\n",
      "Over 16: 440\n",
      "Over 18: 315\n",
      "Over 20: 229\n"
     ]
    }
   ],
   "source": [
    "print('Over 10:', np.sum(each_user >= 10))\n",
    "print('Over 12:', np.sum(each_user >= 12))\n",
    "print('Over 14:', np.sum(each_user >= 14))\n",
    "print('Over 16:', np.sum(each_user >= 16))\n",
    "print('Over 18:', np.sum(each_user >= 18))\n",
    "print('Over 20:', np.sum(each_user >= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10]) # 150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init\n",
    "train_t = []\n",
    "train_f = []\n",
    "test_t = []\n",
    "test_f = []\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    # init\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    t_for_test = []\n",
    "    f_for_test = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "#         print(len(t_for_train) + len(f_for_train))\n",
    "        \n",
    "    else: #if in test id, choose half of true and other \n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #the others for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    if not (len(t_for_train) + len(f_for_train) + len(t_for_test) + len(f_for_test)) == movie_nb:\n",
    "        print('Error!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None, ft_dim))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "image_j = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [usr_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [movie_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [movie_nb, latent_dim],\n",
    "                               initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    \n",
    "#     W1 = tf.get_variable(\"W1\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [usr_nb, movie_nb, embedding_dims], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     Wve = tf.get_variable(\"Wve\", [embedding_dims, ft_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1, latent_dim], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "with tf.variable_scope('feature_level'):\n",
    "    embedding = tf.get_variable(\"embedding\", [embedding_dims,ft_dim],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Beta = tf.get_variable(\"beta\", [usr_nb, embedding_dims],\n",
    "                           initializer=tf.random_normal_initializer(0.01, 0.001, seed=10))\n",
    "    \n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user)\n",
    "vi = tf.nn.embedding_lookup(item_latent, i)\n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "# w1 = tf.nn.embedding_lookup(W1, user)\n",
    "wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user))\n",
    "wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user))\n",
    "wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user))\n",
    "wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user))\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-19-45fa636fc37a>:95: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "a_list = tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    \n",
    "    a_list = tf.concat([a_list,[(tf.nn.relu(tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                            tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(wvui, tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "    q += 1\n",
    "    return q, a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "norm_par = [wu,wy,wa,wv]\n",
    "# norm_par = [tf.reduce_sum(tf.multiply(u, u)),\n",
    "#             tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "#             tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "#             tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "#             tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "#             tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "#             tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "#             tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "#             tf.reduce_sum(tf.multiply(embedding,embedding))]\n",
    "\n",
    "wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "\n",
    "last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(latent_dim),0)\n",
    "q = tf.constant(0)\n",
    "\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_, aux_np = tf.while_loop(sum_att_cond, sum_att_body, [q,aux_np])\n",
    "\n",
    "aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "aux_np += u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new = tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "# ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "l2_norm = tf.add_n([\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.001 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(embedding,embedding))\n",
    "          ])\n",
    "\n",
    "loss = l2_norm - tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Sat Mar 14 16:53:30 2020\n",
      "Epoch: 0\n",
      "total_loss          [[0.61395583]]\n",
      "train_auc:          0.7869769313304721\n",
      "\tCurrent time: Sat Mar 14 22:24:09 2020  sec\n",
      "==================================================\n",
      "Epoch: 1\n",
      "total_loss          [[0.54928829]]\n",
      "train_auc:          0.8299400929899857\n",
      "\tCurrent time: Sun Mar 15 03:53:08 2020  sec\n",
      "==================================================\n",
      "Epoch: 2\n",
      "total_loss          [[0.52603437]]\n",
      "train_auc:          0.8485917381974248\n",
      "\tCurrent time: Sun Mar 15 09:21:55 2020  sec\n",
      "==================================================\n",
      "Epoch: 3\n",
      "total_loss          [[0.5095818]]\n",
      "train_auc:          0.8603183118741059\n",
      "\tCurrent time: Sun Mar 15 14:49:35 2020  sec\n",
      "==================================================\n",
      "Epoch: 4\n",
      "total_loss          [[0.50282592]]\n",
      "train_auc:          0.8666845493562232\n",
      "\tCurrent time: Sun Mar 15 20:15:21 2020  sec\n",
      "==================================================\n",
      "Epoch: 5\n",
      "total_loss          [[0.4953117]]\n",
      "train_auc:          0.8738286838340487\n",
      "\tCurrent time: Mon Mar 16 01:39:33 2020  sec\n",
      "==================================================\n",
      "Epoch: 6\n",
      "total_loss          [[0.49440673]]\n",
      "train_auc:          0.8774320457796853\n",
      "\tCurrent time: Mon Mar 16 06:57:24 2020  sec\n",
      "==================================================\n",
      "Epoch: 7\n",
      "total_loss          [[0.49470889]]\n",
      "train_auc:          0.8794974964234621\n",
      "\tCurrent time: Mon Mar 16 12:08:27 2020  sec\n",
      "==================================================\n",
      "Epoch: 8\n",
      "total_loss          [[0.49471549]]\n",
      "train_auc:          0.8816970672389127\n",
      "\tCurrent time: Mon Mar 16 17:20:59 2020  sec\n",
      "==================================================\n",
      "Epoch: 9\n",
      "total_loss          [[0.48999219]]\n",
      "train_auc:          0.8855776108726753\n",
      "\tCurrent time: Mon Mar 16 22:39:21 2020  sec\n",
      "==================================================\n",
      "Total cost time: 193548.28648924828  sec\n",
      "End time: Mon Mar 16 22:39:21 2020\n"
     ]
    }
   ],
   "source": [
    "print('Start time:', time.ctime())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_yes_id=[]\n",
    "\n",
    "for q in range(10):\n",
    "    print('Epoch:',q)\n",
    "    train_auc = 0\n",
    "    total_loss = 0\n",
    "    xuij_auc = 0\n",
    "    length = 0\n",
    "    \n",
    "    for z in range(usr_nb):\n",
    "        writeProgress('Progress:', z, usr_nb)\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes = []\n",
    "        yesr = []\n",
    "        \n",
    "#         #選全部的Positive\n",
    "#         sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #選全部的電影\n",
    "        sample = all_auxilary\n",
    "        \n",
    "        #change\n",
    "        r_3 = np.zeros(len(sample))\n",
    "         \n",
    "        for b in range(len(sample)):\n",
    "            yes.append(all_npy[sample[b]])\n",
    "            yesr.append(movie_genre[sample[b]] * usr_genre_norm[z])\n",
    "        \n",
    "        for b in range(len(yesr)):\n",
    "            r_3[b]=max(yesr[b])\n",
    "        #print('r_3:',r_3)\n",
    "        \n",
    "        yes = np.array(yes)\n",
    "        \n",
    "        # positive sample\n",
    "        train_t_sample = train_t[z]\n",
    "        for ta in train_t_sample:\n",
    "            #print(ta,'--> positive feedback')\n",
    "            \n",
    "            pos = sample.index(ta)\n",
    "            \n",
    "            image_1=np.expand_dims(all_npy[ta],0)\n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            \n",
    "            for b in train_f_sample:\n",
    "                image_2 = np.expand_dims(all_npy[b],0)\n",
    "                \n",
    "                _last_be_relu, _norm_par, _a_list, r3, _auc, _loss, _ = sess.run(\n",
    "                    [last_be_relu, norm_par, a_list_smooth, a_list_soft, auc, loss, train_op], \n",
    "                    feed_dict={user: [z], i: [ta], j: [b], xf: yes, \n",
    "                               l_id:sample, l_id_len:[len(sample)],\n",
    "                               positive_id: train_t[z], positive_len:[len(train_t[z])],\n",
    "                               r: r_3, image_i: image_1, image_j: image_2})\n",
    "                \n",
    "                '''Observe all params\n",
    "                print('u,vi,vj',_norm_par[:3])\n",
    "                print('w1,wu,wy,wa,wv',_norm_par[3:7])\n",
    "                print('beta',_norm_par[7])\n",
    "                print('Embedding',_norm_par[8])\n",
    "                print('after softmax:', r3)\n",
    "                print('before softmax:', _a_list)\n",
    "                print('---------------------------------------------------')\n",
    "                '''\n",
    "                train_auc += _auc\n",
    "                total_loss += _loss\n",
    "                length += 1\n",
    "    \n",
    "    print(\"{:<20}{}\".format('total_loss', total_loss/length))\n",
    "    print(\"{:<20}{}\".format('train_auc:', train_auc/length))\n",
    "    \n",
    "    loss_acc_list.append([total_loss/length, train_auc/length])\n",
    "    \n",
    "    print('\\tCurrent time:', time.ctime(), ' sec')\n",
    "    print('==================================================')\n",
    "    \n",
    "print('Total cost time:',time.time()-t0, ' sec')\n",
    "\n",
    "print('End time:', time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "loss= [[0.61395583]]\n",
      "acc= 0.7869769313304721\n",
      "==================================================\n",
      "Iteration: 1\n",
      "loss= [[0.54928829]]\n",
      "acc= 0.8299400929899857\n",
      "==================================================\n",
      "Iteration: 2\n",
      "loss= [[0.52603437]]\n",
      "acc= 0.8485917381974248\n",
      "==================================================\n",
      "Iteration: 3\n",
      "loss= [[0.5095818]]\n",
      "acc= 0.8603183118741059\n",
      "==================================================\n",
      "Iteration: 4\n",
      "loss= [[0.50282592]]\n",
      "acc= 0.8666845493562232\n",
      "==================================================\n",
      "Iteration: 5\n",
      "loss= [[0.4953117]]\n",
      "acc= 0.8738286838340487\n",
      "==================================================\n",
      "Iteration: 6\n",
      "loss= [[0.49440673]]\n",
      "acc= 0.8774320457796853\n",
      "==================================================\n",
      "Iteration: 7\n",
      "loss= [[0.49470889]]\n",
      "acc= 0.8794974964234621\n",
      "==================================================\n",
      "Iteration: 8\n",
      "loss= [[0.49471549]]\n",
      "acc= 0.8816970672389127\n",
      "==================================================\n",
      "Iteration: 9\n",
      "loss= [[0.48999219]]\n",
      "acc= 0.8855776108726753\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(loss_acc_list)):\n",
    "    print('Iteration:',i)\n",
    "    print('loss=',loss_acc_list[i][0])\n",
    "    print('acc=',loss_acc_list[i][1])\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 11)\n",
      "[0.6139558325509656, 0.5492882899119278, 0.5260343668969063, 0.5095818035251252, 0.502825919002593, 0.49531169667605507, 0.4944067347885372, 0.4947088893732117, 0.4947154906004113, 0.48999219029417024]\n",
      "[0.7869769313304721, 0.8299400929899857, 0.8485917381974248, 0.8603183118741059, 0.8666845493562232, 0.8738286838340487, 0.8774320457796853, 0.8794974964234621, 0.8816970672389127, 0.8855776108726753]\n"
     ]
    }
   ],
   "source": [
    "# training history\n",
    "epochs = range(1, len(loss_acc_list) + 1)\n",
    "print(epochs)\n",
    "loss = [ls[0].tolist()[0][0] for ls in loss_acc_list]\n",
    "print(loss)\n",
    "acc = [ls[1] for ls in loss_acc_list]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 繪製結果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av, B = sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv, Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (1582, 128)\n",
      "photo latent shape:  (165, 128)\n",
      "Auxilary latent shape:  (165, 128)\n",
      "Embedding shape: (200, 4882)\n",
      "Wu weight shape: (1582, 165, 128)\n",
      "Wy weight shape: (1582, 165, 128)\n",
      "Wa weight shape: (1582, 165, 128)\n",
      "Wv weight shape: (1582, 165, 200)\n",
      "Beta shape: (1582, 200)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Wu weight shape:', Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('./weight/' + SAVE_NAME + '.npz', \n",
    "         U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av, B=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload params if crash\n",
    "# SAVE_NAME = 'MRM_ALL_Embedding200_L2_resplit'\n",
    "\n",
    "# params = np.load('./weight/' + SAVE_NAME + '.npz')\n",
    "# print(params)\n",
    "# U = params['U']\n",
    "# Y = params['Y']\n",
    "# A = params['A']\n",
    "# E = params['E']\n",
    "# Au = params['Wu']\n",
    "# Ay = params['Wy']\n",
    "# Aa = params['Wa']\n",
    "# Av = params['Wv']\n",
    "# B = params['B']\n",
    "\n",
    "# print('User latent shape: ',U.shape)\n",
    "# print('photo latent shape: ', Y.shape)\n",
    "# print('Auxilary latent shape: ',A.shape)\n",
    "# print('Embedding shape:', E.shape)\n",
    "# print('Wu weight shape:', Au.shape)\n",
    "# print('Wy weight shape:', Ay.shape)\n",
    "# print('Wa weight shape:', Aa.shape)\n",
    "# print('Wv weight shape:', Av.shape)\n",
    "# print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "alpha:         [0.04927135 0.07475033 0.01672607 0.01227989 0.04322003 0.04641869\n",
      " 0.03230102]\n",
      "softmax alpha: [0.1442635  0.14798641 0.13964398 0.13902447 0.14339315 0.14385255\n",
      " 0.14183595]\n",
      "==================================================\n",
      "1 51\n",
      "alpha:         [0.02679391 0.03763398 0.06202491 0.09681213 0.010803   0.07486699]\n",
      "softmax alpha: [0.16253108 0.16430251 0.16835928 0.17431909 0.15995273 0.1705353 ]\n",
      "==================================================\n",
      "2 54\n",
      "alpha:         [0.04993242 0.09975638 0.08566583 0.05839287 0.11232916 0.07029778]\n",
      "softmax alpha: [0.1617882  0.17005332 0.16767397 0.16316281 0.17220486 0.16511685]\n",
      "==================================================\n",
      "3 61\n",
      "alpha:         [0.11486366 0.12289823 0.06547429 0.07236239 0.11108172]\n",
      "softmax alpha: [0.20347984 0.2051213  0.19367424 0.19501289 0.20271174]\n",
      "==================================================\n",
      "4 65\n",
      "alpha:         [0.05796829 0.05777051 0.12126263 0.09915473 0.08103265 0.03938788]\n",
      "softmax alpha: [0.16360955 0.1635772  0.17429986 0.17048874 0.16742696 0.16059769]\n",
      "==================================================\n",
      "5 88\n",
      "alpha:         [0.01356331 0.00547962 0.00842972 0.01737771 0.014364   0.00421428\n",
      " 0.01525035 0.01055254 0.01677631]\n",
      "softmax alpha: [0.11130842 0.11041227 0.11073848 0.11173381 0.11139758 0.11027265\n",
      " 0.11149636 0.1109738  0.11166663]\n",
      "==================================================\n",
      "6 93\n",
      "alpha:         [0.03118185 0.04635307 0.00910519 0.01888816 0.04340174 0.03365373\n",
      " 0.04153873]\n",
      "softmax alpha: [0.14272636 0.1449082  0.13960997 0.14098247 0.14448116 0.1430796\n",
      " 0.14421224]\n",
      "==================================================\n",
      "7 96\n",
      "alpha:         [0.15666737 0.13834529 0.16040817 0.09859623 0.15893477]\n",
      "softmax alpha: [0.20278031 0.19909878 0.20354029 0.19134001 0.20324061]\n",
      "==================================================\n",
      "8 114\n",
      "alpha:         [0.07222105 0.11000262 0.09098629 0.10202994 0.09832821]\n",
      "softmax alpha: [0.19553571 0.20306469 0.19923963 0.20145216 0.20070781]\n",
      "==================================================\n",
      "9 130\n",
      "alpha:         [0.24879711 0.07304164 0.12151849 0.15475354 0.14119134]\n",
      "softmax alpha: [0.22087069 0.18527143 0.19447406 0.20104602 0.1983378 ]\n",
      "==================================================\n",
      "10 135\n",
      "alpha:         [0.09442508 0.12965269 0.08732734 0.152581   0.07794736]\n",
      "softmax alpha: [0.19714843 0.20421728 0.19575408 0.20895373 0.19392649]\n",
      "==================================================\n",
      "11 142\n",
      "alpha:         [0.00844333 0.07573668 0.01578421 0.02524126 0.08528203 0.05390795]\n",
      "softmax alpha: [0.16076398 0.17195463 0.16194847 0.16348729 0.17360386 0.16824175]\n",
      "==================================================\n",
      "12 146\n",
      "alpha:         [0.10242184 0.12084244 0.10966046 0.18662017 0.16612708]\n",
      "softmax alpha: [0.19306961 0.19665903 0.19447224 0.21002974 0.20576938]\n",
      "==================================================\n",
      "13 161\n",
      "alpha:         [0.11696467 0.09344759 0.09645305 0.04733373 0.17390807]\n",
      "softmax alpha: [0.2021105  0.1974129  0.19800711 0.18851614 0.21395334]\n",
      "==================================================\n",
      "14 163\n",
      "alpha:         [0.07203686 0.08916628 0.0303942  0.05551375 0.03381463 0.09715506]\n",
      "softmax alpha: [0.16812268 0.17102733 0.16126537 0.16536759 0.16181792 0.1723991 ]\n",
      "==================================================\n",
      "15 178\n",
      "alpha:         [0.15140098 0.18211174 0.28150642 0.1560792  0.15496089]\n",
      "softmax alpha: [0.19311017 0.19913274 0.21994254 0.1940157  0.19379885]\n",
      "==================================================\n",
      "16 186\n",
      "alpha:         [0.05467235 0.06852011 0.08180444 0.07029979 0.03843843 0.08158507]\n",
      "softmax alpha: [0.16478881 0.16708664 0.16932109 0.16738427 0.16213524 0.16928395]\n",
      "==================================================\n",
      "17 189\n",
      "alpha:         [0.14590027 0.18166976 0.12658938 0.2246564  0.08120014]\n",
      "softmax alpha: [0.19854742 0.2057779  0.19475007 0.21481648 0.18610812]\n",
      "==================================================\n",
      "18 191\n",
      "alpha:         [0.08864809 0.46061903 0.23897171 0.1271574  0.06507243]\n",
      "softmax alpha: [0.17765897 0.25771048 0.20647709 0.18463393 0.17351953]\n",
      "==================================================\n",
      "19 198\n",
      "alpha:         [0.23987112 0.15180084 0.17597547 0.20300461 0.11069655]\n",
      "softmax alpha: [0.21292793 0.19497736 0.19974831 0.20522096 0.18712544]\n",
      "==================================================\n",
      "20 206\n",
      "alpha:         [0.17405695 0.09392502 0.03200465 0.11242442 0.04077671]\n",
      "softmax alpha: [0.21710725 0.20038881 0.18835701 0.20413038 0.19001656]\n",
      "==================================================\n",
      "21 209\n",
      "alpha:         [0.05376749 0.06152734 0.06618908 0.03529216 0.12042089 0.07737021]\n",
      "softmax alpha: [0.16407451 0.16535266 0.16612529 0.16107101 0.17538334 0.16799318]\n",
      "==================================================\n",
      "22 224\n",
      "alpha:         [0.09563253 0.05416349 0.07546675 0.06687427 0.06567133 0.08748325]\n",
      "softmax alpha: [0.17025816 0.16334211 0.16685916 0.16543157 0.16523268 0.16887632]\n",
      "==================================================\n",
      "23 228\n",
      "alpha:         [0.04625515 0.08573816 0.06285792 0.07862332 0.07147556 0.07719317]\n",
      "softmax alpha: [0.1626843  0.16923606 0.16540786 0.16803624 0.16683944 0.1677961 ]\n",
      "==================================================\n",
      "24 255\n",
      "alpha:         [0.03269444 0.01779716 0.01157069 0.05677201 0.04571594 0.00771474\n",
      " 0.01812705 0.01995383]\n",
      "softmax alpha: [0.12578624 0.12392625 0.12315702 0.12885162 0.12743487 0.12268305\n",
      " 0.12396714 0.12419381]\n",
      "==================================================\n",
      "25 283\n",
      "alpha:         [0.07572713 0.03606406 0.05871096 0.06145896 0.05018807 0.02857154\n",
      " 0.02839822]\n",
      "softmax alpha: [0.14678746 0.14107937 0.14431083 0.14470794 0.14308611 0.14002628\n",
      " 0.14000201]\n",
      "==================================================\n",
      "26 285\n",
      "alpha:         [0.00674227 0.01938317 0.03930661 0.01198386 0.00569948 0.0099381\n",
      " 0.01424376 0.01746351 0.00860932]\n",
      "softmax alpha: [0.11021211 0.11161413 0.11386017 0.11079131 0.11009724 0.11056489\n",
      " 0.11104198 0.11140008 0.11041808]\n",
      "==================================================\n",
      "27 292\n",
      "alpha:         [0.08290055 0.08325606 0.07880549 0.06719027 0.0368024  0.0665764 ]\n",
      "softmax alpha: [0.16893496 0.16899503 0.16824457 0.16630168 0.16132414 0.16619962]\n",
      "==================================================\n",
      "28 313\n",
      "alpha:         [0.16434279 0.04942453 0.10943878 0.15411566 0.18644827]\n",
      "softmax alpha: [0.20617742 0.18379459 0.1951626  0.20407956 0.21078582]\n",
      "==================================================\n",
      "29 318\n",
      "alpha:         [0.0036686  0.00236364 0.00531161 0.00129992 0.00260501 0.00397872\n",
      " 0.00123458 0.00188975 0.00468272 0.00204827 0.00372492 0.00202865]\n",
      "softmax alpha: [0.08339709 0.08328833 0.08353422 0.08319978 0.08330843 0.08342295\n",
      " 0.08319434 0.08324887 0.0834817  0.08326207 0.08340178 0.08326043]\n",
      "==================================================\n",
      "30 326\n",
      "alpha:         [0.079822   0.09024951 0.02037344 0.04068551 0.05356907 0.03877323\n",
      " 0.01548132]\n",
      "softmax alpha: [0.14736348 0.14890815 0.13885825 0.14170758 0.14354509 0.14143686\n",
      " 0.13818059]\n",
      "==================================================\n",
      "31 327\n",
      "alpha:         [0.00743363 0.00469203 0.00455615 0.00893224 0.01038514 0.00496128\n",
      " 0.00616979 0.00814861 0.00422495 0.0076421 ]\n",
      "softmax alpha: [0.10007173 0.09979775 0.09978419 0.10022181 0.10036753 0.09982462\n",
      " 0.09994533 0.1001433  0.09975115 0.10009259]\n",
      "==================================================\n",
      "32 333\n",
      "alpha:         [0.20991108 0.08285383 0.09261346 0.08778181 0.21941184]\n",
      "softmax alpha: [0.2143814  0.18880211 0.19065377 0.18973482 0.21642789]\n",
      "==================================================\n",
      "33 334\n",
      "alpha:         [0.09677965 0.09481106 0.04577741 0.07550112 0.06830601 0.1381861 ]\n",
      "softmax alpha: [0.16830881 0.1679778  0.15993991 0.16476528 0.16358403 0.17542417]\n",
      "==================================================\n",
      "34 350\n",
      "alpha:         [0.21469159 0.04726112 0.0491569  0.04154892 0.06366052 0.03944106]\n",
      "softmax alpha: [0.19108178 0.16162374 0.16193044 0.16070315 0.16429613 0.16036476]\n",
      "==================================================\n",
      "35 393\n",
      "alpha:         [0.0235621  0.00095492 0.05054191 0.00239924 0.00032071 0.00064946\n",
      " 0.00034424 0.0011508  0.00083291 0.00246504 0.00041434 0.00195192\n",
      " 0.00128966 0.00095496 0.0012701 ]\n",
      "softmax alpha: [0.06784592 0.06632932 0.0697013  0.06642519 0.06628727 0.06630906\n",
      " 0.06628882 0.06634231 0.06632123 0.06642956 0.06629347 0.06639548\n",
      " 0.06635153 0.06632932 0.06635023]\n",
      "==================================================\n",
      "36 407\n",
      "alpha:         [0.25116303 0.16041869 0.19787175 0.07565502 0.13818322]\n",
      "softmax alpha: [0.21769635 0.19881144 0.20639874 0.18265391 0.19443956]\n",
      "==================================================\n",
      "37 429\n",
      "alpha:         [0.05308319 0.16125172 0.05567975 0.20765023 0.122615  ]\n",
      "softmax alpha: [0.1867067  0.20803524 0.18719213 0.2179152  0.20015073]\n",
      "==================================================\n",
      "38 432\n",
      "alpha:         [0.05760761 0.09569834 0.0560442  0.0736801  0.01415044 0.03922597]\n",
      "softmax alpha: [0.16686898 0.17334775 0.1666083  0.16957265 0.15977264 0.16382968]\n",
      "==================================================\n",
      "39 435\n",
      "alpha:         [0.099501   0.11109523 0.09624286 0.14322237 0.1323522 ]\n",
      "softmax alpha: [0.19659896 0.19889164 0.19595946 0.20538521 0.20316473]\n",
      "==================================================\n",
      "40 440\n",
      "alpha:         [0.00316796 0.00130658 0.00150023 0.01327798 0.0021713  0.003126\n",
      " 0.00413944 0.00279449 0.0023183  0.00229989 0.00270353 0.0019108\n",
      " 0.00222495]\n",
      "softmax alpha: [0.07691234 0.07676931 0.07678417 0.07769387 0.07683572 0.07690911\n",
      " 0.07698709 0.07688362 0.07684701 0.0768456  0.07687662 0.07681571\n",
      " 0.07683984]\n",
      "==================================================\n",
      "41 447\n",
      "alpha:         [0.04477523 0.08895482 0.07349155 0.09050081 0.07343071 0.06403415]\n",
      "softmax alpha: [0.16208492 0.1694063  0.16680688 0.16966841 0.16679673 0.16523676]\n",
      "==================================================\n",
      "42 449\n",
      "alpha:         [0.00029351 0.00357941 0.00080077 0.002776   0.00025436 0.00159107\n",
      " 0.00052526 0.00029227 0.00018919 0.00018158 0.00019856 0.00047986\n",
      " 0.00157374 0.00037082 0.00230629 0.00880192 0.00050123]\n",
      "softmax alpha: [0.05875518 0.05894856 0.058785   0.05890122 0.05875288 0.05883147\n",
      " 0.0587688  0.05875511 0.05874905 0.05874861 0.05874961 0.05876613\n",
      " 0.05883045 0.05875973 0.05887356 0.05925723 0.05876739]\n",
      "==================================================\n",
      "43 451\n",
      "alpha:         [4.67191967e-03 1.67521841e-05 1.28804153e-05 1.91445231e-04\n",
      " 4.95474912e-03 1.40961810e-04 7.70080456e-04 4.36314136e-05\n",
      " 2.07215074e-03 1.34021501e-02 1.55257222e-03 6.60461734e-04\n",
      " 5.36578473e-03 2.36735361e-04 1.53683881e-03 3.45401118e-03\n",
      " 1.02692876e-03 1.09379666e-03 1.23567228e-04 9.69269728e-03]\n",
      "softmax alpha: [0.05010585 0.04987314 0.04987295 0.04988186 0.05012003 0.04987934\n",
      " 0.04991073 0.04987448 0.04997576 0.0505452  0.0499498  0.04990526\n",
      " 0.05014063 0.04988412 0.04994901 0.05004487 0.04992355 0.04992689\n",
      " 0.04987847 0.05035806]\n",
      "==================================================\n",
      "44 457\n",
      "alpha:         [0.10310974 0.16935854 0.12855834 0.18966476 0.11838681]\n",
      "softmax alpha: [0.19230498 0.20547643 0.19726167 0.20969153 0.19526539]\n",
      "==================================================\n",
      "45 466\n",
      "alpha:         [0.05130137 0.11390034 0.11535457 0.11287983 0.10423278 0.1318471 ]\n",
      "softmax alpha: [0.15791555 0.16811686 0.16836152 0.16794539 0.16649942 0.17116126]\n",
      "==================================================\n",
      "46 469\n",
      "alpha:         [0.01574122 0.03067    0.01848252 0.02836521 0.03408745 0.00574118\n",
      " 0.03376761 0.01410863]\n",
      "softmax alpha: [0.12413703 0.12600415 0.1244778  0.12571407 0.1264355  0.12290184\n",
      " 0.12639507 0.12393453]\n",
      "==================================================\n",
      "47 476\n",
      "alpha:         [0.10815376 0.090755   0.05172848 0.08097333 0.14609818]\n",
      "softmax alpha: [0.20243948 0.19894775 0.19133306 0.19701119 0.21026852]\n",
      "==================================================\n",
      "48 501\n",
      "alpha:         [0.07232654 0.11670812 0.07361095 0.06623395 0.07308611 0.08491647]\n",
      "softmax alpha: [0.16517945 0.17267549 0.16539175 0.16417614 0.16530497 0.1672722 ]\n",
      "==================================================\n",
      "49 505\n",
      "alpha:         [0.04626113 0.06033973 0.12977507 0.09217633 0.12327411 0.04107878]\n",
      "softmax alpha: [0.16069029 0.16296859 0.17468648 0.16824042 0.17355453 0.15985969]\n",
      "==================================================\n",
      "50 514\n",
      "alpha:         [0.1437353  0.12468377 0.08261654 0.37968353 0.13823526]\n",
      "softmax alpha: [0.1929584  0.18931705 0.18151819 0.24430632 0.19190004]\n",
      "==================================================\n",
      "51 538\n",
      "alpha:         [0.16027469 0.15827836 0.1120555  0.34462434 0.11819366]\n",
      "softmax alpha: [0.19561208 0.19522196 0.18640362 0.23521104 0.18755131]\n",
      "==================================================\n",
      "52 541\n",
      "alpha:         [0.00075357 0.00190911 0.0012556  0.00028273 0.00126642 0.00070353\n",
      " 0.00129953 0.00038678 0.00051076 0.00020253 0.05052772 0.00121004\n",
      " 0.00073404 0.00084051 0.00017856 0.00141802]\n",
      "softmax alpha: [0.06229488 0.0623669  0.06232616 0.06226556 0.06232684 0.06229176\n",
      " 0.0623289  0.06227203 0.06227976 0.06226056 0.06547402 0.06232332\n",
      " 0.06229366 0.0623003  0.06225907 0.06233628]\n",
      "==================================================\n",
      "53 542\n",
      "alpha:         [0.12104653 0.07676238 0.12357227 0.09710114 0.11012184]\n",
      "softmax alpha: [0.20305868 0.1942626  0.2035722  0.19825411 0.2008524 ]\n",
      "==================================================\n",
      "54 546\n",
      "alpha:         [0.06531734 0.04057821 0.01982435 0.03834378 0.09797037 0.04244934\n",
      " 0.0243095 ]\n",
      "softmax alpha: [0.14545703 0.14190269 0.13898801 0.14158598 0.15028503 0.14216846\n",
      " 0.1396128 ]\n",
      "==================================================\n",
      "55 548\n",
      "alpha:         [0.07730867 0.10245369 0.0771079  0.0609025  0.03684019 0.12491273]\n",
      "softmax alpha: [0.1661658  0.17039702 0.16613244 0.1634619  0.15957557 0.17426727]\n",
      "==================================================\n",
      "56 552\n",
      "alpha:         [0.1937741  0.09230719 0.13986881 0.18163184 0.26860407]\n",
      "softmax alpha: [0.2033899  0.18376503 0.19271637 0.20093522 0.21919348]\n",
      "==================================================\n",
      "57 563\n",
      "alpha:         [0.01121643 0.07788144 0.08137267 0.06304986 0.05912608 0.06284937]\n",
      "softmax alpha: [0.15880885 0.16975671 0.1703504  0.16725752 0.16660253 0.16722399]\n",
      "==================================================\n",
      "58 569\n",
      "alpha:         [0.06301141 0.02617068 0.08485633 0.06444887 0.04901024 0.0704714 ]\n",
      "softmax alpha: [0.16719786 0.16115026 0.17089047 0.16743838 0.16487321 0.16844982]\n",
      "==================================================\n",
      "59 592\n",
      "alpha:         [0.02131994 0.04580405 0.02873246 0.01011089 0.01046603 0.01629367\n",
      " 0.02370439 0.01299071]\n",
      "softmax alpha: [0.12500994 0.12810848 0.12594002 0.12361652 0.12366043 0.12438319\n",
      " 0.12530838 0.12397303]\n",
      "==================================================\n",
      "60 600\n",
      "alpha:         [0.07334779 0.06447284 0.13083142 0.14319062 0.13082287]\n",
      "softmax alpha: [0.19298199 0.19127686 0.20440033 0.20694223 0.20439859]\n",
      "==================================================\n",
      "61 644\n",
      "alpha:         [0.18707352 0.11321246 0.12895199 0.2499968  0.12314426]\n",
      "softmax alpha: [0.20511337 0.19050945 0.1935317  0.21843449 0.19241098]\n",
      "==================================================\n",
      "62 646\n",
      "alpha:         [0.04880015 0.12238599 0.07852225 0.11679719 0.03381786]\n",
      "softmax alpha: [0.19372259 0.20851542 0.19956685 0.20735332 0.19084181]\n",
      "==================================================\n",
      "63 664\n",
      "alpha:         [0.00132638 0.0021236  0.00199653 0.00371701 0.00182958 0.00271823\n",
      " 0.00224503 0.00103882 0.00246211 0.00336101 0.00235799 0.00170129\n",
      " 0.00109674]\n",
      "softmax alpha: [0.07685958 0.07692088 0.07691111 0.07704354 0.07689827 0.07696663\n",
      " 0.07693022 0.07683748 0.07694692 0.07701612 0.07693891 0.0768884\n",
      " 0.07684193]\n",
      "==================================================\n",
      "64 689\n",
      "alpha:         [0.01952081 0.01824138 0.00790648 0.01444479 0.00923037 0.01706026\n",
      " 0.01188732 0.00897485 0.03759135]\n",
      "softmax alpha: [0.11148824 0.11134569 0.11020087 0.11092376 0.11034686 0.11121426\n",
      " 0.11064044 0.11031867 0.11352121]\n",
      "==================================================\n",
      "65 696\n",
      "alpha:         [0.00884582 0.01587587 0.01873533 0.01410618 0.02605485 0.01335861\n",
      " 0.03349457 0.02171858 0.01527649]\n",
      "softmax alpha: [0.11002903 0.11080526 0.11112256 0.11060934 0.1119389  0.11052669\n",
      " 0.1127748  0.11145456 0.11073887]\n",
      "==================================================\n",
      "66 704\n",
      "alpha:         [0.00856499 0.01166288 0.00542938 0.00669164 0.00652443 0.00472584\n",
      " 0.05234195 0.00617867 0.00367864 0.0093583 ]\n",
      "softmax alpha: [0.09969578 0.1000051  0.09938366 0.09950919 0.09949255 0.09931376\n",
      " 0.10415709 0.09945815 0.09920982 0.0997749 ]\n",
      "==================================================\n",
      "67 727\n",
      "alpha:         [0.13060143 0.12440616 0.15057278 0.15171193 0.11715593]\n",
      "softmax alpha: [0.19912483 0.19789501 0.2031416  0.20337314 0.19646542]\n",
      "==================================================\n",
      "68 735\n",
      "alpha:         [0.1806235  0.0950828  0.08241787 0.22797945 0.15369856]\n",
      "softmax alpha: [0.2063384  0.1894219  0.18703802 0.21634481 0.20085687]\n",
      "==================================================\n",
      "69 740\n",
      "alpha:         [0.12034993 0.13948548 0.09045172 0.11410378 0.15154485]\n",
      "softmax alpha: [0.19938891 0.20324107 0.19351578 0.19814738 0.20570686]\n",
      "==================================================\n",
      "70 741\n",
      "alpha:         [0.04143608 0.03769385 0.04892296 0.05043544 0.07171389 0.02720384\n",
      " 0.05285559]\n",
      "softmax alpha: [0.14202701 0.14149651 0.14309434 0.14331093 0.14639304 0.14001997\n",
      " 0.14365819]\n",
      "==================================================\n",
      "71 747\n",
      "alpha:         [0.05276789 0.00653801 0.01818578 0.01295976 0.00518491 0.00408154\n",
      " 0.0127327  0.0210383  0.01383465]\n",
      "softmax alpha: [0.11521858 0.11001329 0.11130219 0.11072204 0.10986453 0.10974338\n",
      " 0.1106969  0.11162014 0.11081895]\n",
      "==================================================\n",
      "72 758\n",
      "alpha:         [0.00526145 0.00520138 0.00882279 0.00664137 0.00525535 0.00616938\n",
      " 0.00294452 0.00585503 0.0017861  0.00216979]\n",
      "softmax alpha: [0.10002487 0.10001886 0.10038172 0.10016299 0.10002426 0.10011572\n",
      " 0.09979338 0.10008426 0.09967785 0.0997161 ]\n",
      "==================================================\n",
      "73 775\n",
      "alpha:         [0.22326105 0.10487402 0.37220513 0.14864605 0.10878677]\n",
      "softmax alpha: [0.20538537 0.18245454 0.23837193 0.19061832 0.18316984]\n",
      "==================================================\n",
      "74 777\n",
      "alpha:         [0.02275868 0.01613653 0.01411077 0.0185044  0.02686786 0.03029277\n",
      " 0.01722424 0.01547325]\n",
      "softmax alpha: [0.12532198 0.12449483 0.12424288 0.12478996 0.12583802 0.12626974\n",
      " 0.12463031 0.12441228]\n",
      "==================================================\n",
      "75 778\n",
      "alpha:         [0.03908214 0.16242502 0.12809933 0.04463773 0.14205983]\n",
      "softmax alpha: [0.18732171 0.21191185 0.20476126 0.18836529 0.20763988]\n",
      "==================================================\n",
      "76 781\n",
      "alpha:         [0.08795677 0.14556954 0.12404867 0.15204595 0.0534312 ]\n",
      "softmax alpha: [0.19499613 0.20656032 0.20216245 0.20790243 0.18837867]\n",
      "==================================================\n",
      "77 788\n",
      "alpha:         [0.1002268  0.12170804 0.03445599 0.05229887 0.09304109 0.08417896]\n",
      "softmax alpha: [0.16983171 0.17351937 0.15902115 0.16188401 0.16861573 0.16712803]\n",
      "==================================================\n",
      "78 810\n",
      "alpha:         [0.14206025 0.17438118 0.11120703 0.19007823 0.1618298 ]\n",
      "softmax alpha: [0.19717556 0.20365256 0.19118494 0.20687453 0.20111241]\n",
      "==================================================\n",
      "79 817\n",
      "alpha:         [0.03114979 0.00150152 0.00197845 0.00214317 0.00199835 0.00215019\n",
      " 0.00432055 0.00142945 0.00162083 0.00221344 0.00239802 0.0014838\n",
      " 0.00182993]\n",
      "softmax alpha: [0.07901208 0.0767039  0.07674049 0.07675313 0.07674202 0.07675367\n",
      " 0.07692043 0.07669837 0.07671305 0.07675853 0.07677269 0.07670254\n",
      " 0.07672909]\n",
      "==================================================\n",
      "80 821\n",
      "alpha:         [0.02726388 0.05767231 0.17128198 0.13570232 0.2285872 ]\n",
      "softmax alpha: [0.18104997 0.18663998 0.20909552 0.20178676 0.22142776]\n",
      "==================================================\n",
      "81 859\n",
      "alpha:         [0.20668835 0.07241769 0.13941452 0.23171401 0.18437822]\n",
      "softmax alpha: [0.20778915 0.18168114 0.19427021 0.21305482 0.20320467]\n",
      "==================================================\n",
      "82 864\n",
      "alpha:         [0.04586875 0.0754879  0.10109503 0.11574486 0.06142815 0.06254665]\n",
      "softmax alpha: [0.16150611 0.16636134 0.17067639 0.17319517 0.1640387  0.16422229]\n",
      "==================================================\n",
      "83 865\n",
      "alpha:         [0.09115766 0.02689501 0.08387053 0.03034823 0.0809759  0.08437004]\n",
      "softmax alpha: [0.17080569 0.16017452 0.16956553 0.16072859 0.16907541 0.16965025]\n",
      "==================================================\n",
      "84 877\n",
      "alpha:         [0.05241617 0.03317188 0.06592191 0.18530063 0.16871074]\n",
      "softmax alpha: [0.19011473 0.18649108 0.19269978 0.21713346 0.21356095]\n",
      "==================================================\n",
      "85 919\n",
      "alpha:         [0.00813521 0.03506946 0.01533406 0.01463485 0.00787626 0.01520907\n",
      " 0.00919377 0.01182258 0.01296019]\n",
      "softmax alpha: [0.11040604 0.11342015 0.1112037  0.11112597 0.11037745 0.1111898\n",
      " 0.11052297 0.11081389 0.11094003]\n",
      "==================================================\n",
      "86 928\n",
      "alpha:         [0.05153863 0.24449653 0.10244683 0.10910879 0.17625801]\n",
      "softmax alpha: [0.1832459  0.22224644 0.19281615 0.19410497 0.20758654]\n",
      "==================================================\n",
      "87 939\n",
      "alpha:         [0.1327457  0.03421068 0.04232095 0.16061629 0.1738925 ]\n",
      "softmax alpha: [0.20450004 0.18531056 0.18681959 0.21027974 0.21309007]\n",
      "==================================================\n",
      "88 940\n",
      "alpha:         [0.35842014 0.16341239 0.08748844 0.21825391 0.044853  ]\n",
      "softmax alpha: [0.23891746 0.19658804 0.18221484 0.20767034 0.17460932]\n",
      "==================================================\n",
      "89 946\n",
      "alpha:         [0.00546407 0.021658   0.00872192 0.01523358 0.01445371 0.0195077\n",
      " 0.0118373  0.00856083 0.01054158]\n",
      "softmax alpha: [0.11028806 0.1120886  0.11064795 0.1113708  0.11128398 0.11184783\n",
      " 0.11099319 0.11063012 0.11084947]\n",
      "==================================================\n",
      "90 958\n",
      "alpha:         [0.06020914 0.05953624 0.04385307 0.06053371 0.04108608 0.03057051\n",
      " 0.01819185]\n",
      "softmax alpha: [0.14505079 0.14495322 0.14269762 0.14509787 0.14230333 0.14081477\n",
      " 0.13908241]\n",
      "==================================================\n",
      "91 1010\n",
      "alpha:         [0.06555735 0.1163448  0.06740362 0.06634087 0.18835718 0.0330259 ]\n",
      "softmax alpha: [0.16251228 0.17097905 0.1628126  0.16263966 0.18374582 0.15731059]\n",
      "==================================================\n",
      "92 1022\n",
      "alpha:         [0.01806642 0.01517041 0.02186016 0.0545027  0.01017667 0.02961217\n",
      " 0.02776673 0.03306568]\n",
      "softmax alpha: [0.12396749 0.123609   0.12443868 0.1285677  0.12299326 0.12540708\n",
      " 0.12517586 0.12584092]\n",
      "==================================================\n",
      "93 1034\n",
      "alpha:         [0.03440605 0.05450348 0.04203636 0.0203572  0.05032284 0.03775521\n",
      " 0.03896598]\n",
      "softmax alpha: [0.14208625 0.1449707  0.14317456 0.14010405 0.1443659  0.14256291\n",
      " 0.14273563]\n",
      "==================================================\n",
      "94 1043\n",
      "alpha:         [0.03963057 0.04473919 0.01572269 0.03157192 0.05019124 0.04288451\n",
      " 0.04319811]\n",
      "softmax alpha: [0.14304267 0.14377529 0.13966338 0.14189457 0.1445613  0.14350888\n",
      " 0.14355389]\n",
      "==================================================\n",
      "95 1083\n",
      "alpha:         [0.01821974 0.02747626 0.00727796 0.00984535 0.0167466  0.02369385\n",
      " 0.02453411 0.0164297 ]\n",
      "softmax alpha: [0.12502124 0.12618387 0.12366074 0.12397863 0.1248372  0.1257075\n",
      " 0.12581317 0.12479765]\n",
      "==================================================\n",
      "96 1093\n",
      "alpha:         [0.03989682 0.05832274 0.08871865 0.02746232 0.10502409 0.09879121]\n",
      "softmax alpha: [0.1617016  0.16470872 0.16979206 0.15970337 0.17258329 0.17151094]\n",
      "==================================================\n",
      "97 1098\n",
      "alpha:         [0.08897267 0.02549389 0.05113238 0.03160523 0.03470841 0.07448405\n",
      " 0.0248175 ]\n",
      "softmax alpha: [0.14889228 0.13973452 0.14336342 0.1405911  0.14102805 0.14675059\n",
      " 0.13964003]\n",
      "==================================================\n",
      "98 1103\n",
      "alpha:         [0.05427767 0.13237893 0.06078076 0.07506045 0.04619822 0.04203562]\n",
      "softmax alpha: [0.16424293 0.17758473 0.16531449 0.16769207 0.16292128 0.16224451]\n",
      "==================================================\n",
      "99 1116\n",
      "alpha:         [0.01957069 0.0210461  0.01167038 0.02957525 0.01229888 0.01511986\n",
      " 0.04326396 0.01393344 0.01233575]\n",
      "softmax alpha: [0.11107259 0.11123658 0.11019853 0.11218939 0.11026782 0.11057932\n",
      " 0.11373568 0.1104482  0.11027188]\n",
      "==================================================\n",
      "100 1130\n",
      "alpha:         [0.13624149 0.15535024 0.17915088 0.14348279 0.13251293]\n",
      "softmax alpha: [0.19736783 0.20117555 0.20602109 0.19880222 0.19663331]\n",
      "==================================================\n",
      "101 1133\n",
      "alpha:         [0.09786624 0.24366501 0.1073137  0.15275742 0.1123595 ]\n",
      "softmax alpha: [0.19093152 0.22090077 0.19274388 0.20170495 0.19371889]\n",
      "==================================================\n",
      "102 1140\n",
      "alpha:         [0.05378822 0.06703519 0.06487753 0.05319485 0.05748764 0.07406999]\n",
      "softmax alpha: [0.16534151 0.16754635 0.16718523 0.16524343 0.16595431 0.16872916]\n",
      "==================================================\n",
      "103 1149\n",
      "alpha:         [0.11584447 0.09159227 0.07424411 0.08518384 0.09930123]\n",
      "softmax alpha: [0.20455377 0.19965257 0.19621883 0.1983772  0.20119763]\n",
      "==================================================\n",
      "104 1161\n",
      "alpha:         [0.04419682 0.03258017 0.06619909 0.05750651 0.04081995 0.03957831\n",
      " 0.0659954 ]\n",
      "softmax alpha: [0.14208271 0.14044173 0.14524349 0.14398642 0.14160372 0.14142801\n",
      " 0.14521391]\n",
      "==================================================\n",
      "105 1182\n",
      "alpha:         [0.01652362 0.01906555 0.01654814 0.00836542 0.01587339 0.02147691\n",
      " 0.00698929 0.01344086]\n",
      "softmax alpha: [0.1252161  0.1255348  0.12521917 0.12419872 0.12513471 0.12583787\n",
      " 0.12402793 0.12483069]\n",
      "==================================================\n",
      "106 1195\n",
      "alpha:         [0.07633212 0.12728528 0.17400278 0.16854205 0.04612817]\n",
      "softmax alpha: [0.19150793 0.20151874 0.21115657 0.21000664 0.18581012]\n",
      "==================================================\n",
      "107 1197\n",
      "alpha:         [0.14642143 0.04153678 0.28584293 0.17286991 0.20422944]\n",
      "softmax alpha: [0.19469079 0.17530511 0.2238182  0.19990876 0.20627714]\n",
      "==================================================\n",
      "108 1206\n",
      "alpha:         [0.01026264 0.00609206 0.01361039 0.01902882 0.02229509 0.01438393\n",
      " 0.00794285 0.00545808 0.01418233]\n",
      "softmax alpha: [0.11085186 0.11039051 0.11122359 0.11182788 0.11219374 0.11130966\n",
      " 0.11059501 0.11032054 0.11128722]\n",
      "==================================================\n",
      "109 1209\n",
      "alpha:         [0.09403984 0.064602   0.1744347  0.09914131 0.09120502]\n",
      "softmax alpha: [0.1977461  0.19200973 0.21430039 0.19875747 0.19718632]\n",
      "==================================================\n",
      "110 1220\n",
      "alpha:         [0.00230688 0.00231918 0.00153674 0.00152132 0.00165154 0.00165385\n",
      " 0.00184447 0.00237482 0.00210174 0.0035272  0.00148498 0.00182426\n",
      " 0.0028738 ]\n",
      "softmax alpha: [0.07694063 0.07694158 0.0768814  0.07688021 0.07689023 0.0768904\n",
      " 0.07690506 0.07694586 0.07692485 0.07703458 0.07687742 0.07690351\n",
      " 0.07698426]\n",
      "==================================================\n",
      "111 1221\n",
      "alpha:         [0.07268707 0.11788379 0.07147646 0.04424019 0.06559375 0.06781068]\n",
      "softmax alpha: [0.16652665 0.17422578 0.16632517 0.16185623 0.1653496  0.16571657]\n",
      "==================================================\n",
      "112 1232\n",
      "alpha:         [0.17717789 0.12467227 0.11354499 0.03352822 0.21520753]\n",
      "softmax alpha: [0.20867454 0.19800062 0.19580963 0.18075203 0.21676318]\n",
      "==================================================\n",
      "113 1236\n",
      "alpha:         [0.10836838 0.11483999 0.11504278 0.09363617 0.13266988]\n",
      "softmax alpha: [0.1990777  0.20037023 0.20041087 0.19616635 0.20397485]\n",
      "==================================================\n",
      "114 1247\n",
      "alpha:         [0.23859175 0.08365225 0.07604928 0.06366596 0.11796557 0.08348581]\n",
      "softmax alpha: [0.18908396 0.16194417 0.16071758 0.15873963 0.16759745 0.16191722]\n",
      "==================================================\n",
      "115 1266\n",
      "alpha:         [0.0353047  0.05830312 0.0392143  0.11121695 0.07043826 0.11763167]\n",
      "softmax alpha: [0.16057502 0.16431078 0.16120403 0.17323923 0.16631686 0.17435408]\n",
      "==================================================\n",
      "116 1285\n",
      "alpha:         [0.0510688  0.14878553 0.21110916 0.16865894 0.23180975]\n",
      "softmax alpha: [0.17860025 0.19693363 0.20959779 0.20088652 0.21398181]\n",
      "==================================================\n",
      "117 1287\n",
      "alpha:         [0.20882219 0.19554623 0.23089436 0.24274512 0.19480186]\n",
      "softmax alpha: [0.19881855 0.19619649 0.2032557  0.20567876 0.1960505 ]\n",
      "==================================================\n",
      "118 1300\n",
      "alpha:         [0.0400288  0.08221049 0.11033287 0.03561516 0.11726178 0.05391138]\n",
      "softmax alpha: [0.1611399  0.16808245 0.17287642 0.16043026 0.17407843 0.16339254]\n",
      "==================================================\n",
      "119 1301\n",
      "alpha:         [0.00238894 0.00255768 0.00448007 0.00170161 0.0030349  0.00516632\n",
      " 0.00187056 0.00277594 0.00237318 0.00215637 0.00245224 0.00256157\n",
      " 0.00212254]\n",
      "softmax alpha: [0.07689591 0.07690889 0.07705688 0.07684308 0.0769456  0.07710978\n",
      " 0.07685606 0.07692568 0.0768947  0.07687803 0.07690078 0.07690919\n",
      " 0.07687543]\n",
      "==================================================\n",
      "120 1309\n",
      "alpha:         [0.09039028 0.06706386 0.06907083 0.09124048 0.07692714 0.08206523]\n",
      "softmax alpha: [0.16849095 0.16460614 0.16493684 0.16863426 0.16623773 0.16709408]\n",
      "==================================================\n",
      "121 1310\n",
      "alpha:         [0.02040054 0.15930618 0.08170439 0.1591627  0.19087977]\n",
      "softmax alpha: [0.18027888 0.20714326 0.19167646 0.20711354 0.21378786]\n",
      "==================================================\n",
      "122 1316\n",
      "alpha:         [0.11721801 0.08238943 0.10020704 0.10846707 0.05973294 0.07166709]\n",
      "softmax alpha: [0.1712388  0.16537746 0.1683505  0.16974684 0.16167271 0.1636137 ]\n",
      "==================================================\n",
      "123 1327\n",
      "alpha:         [0.02935627 0.05648841 0.02472185 0.0717297  0.08119407 0.06318876\n",
      " 0.08475341]\n",
      "softmax alpha: [0.13868201 0.14249626 0.13804079 0.14468473 0.14606058 0.14345424\n",
      " 0.14658138]\n",
      "==================================================\n",
      "124 1330\n",
      "alpha:         [0.1670472  0.13990905 0.08519148 0.16332724 0.11413267]\n",
      "softmax alpha: [0.20663805 0.20110569 0.19039731 0.2058708  0.19598815]\n",
      "==================================================\n",
      "125 1342\n",
      "alpha:         [0.03529677 0.0125665  0.01549088 0.00816957 0.02145294 0.01908636\n",
      " 0.01468582 0.02843213]\n",
      "softmax alpha: [0.12699891 0.12414475 0.12450833 0.1236001  0.12525287 0.1249568\n",
      " 0.12440813 0.1261301 ]\n",
      "==================================================\n",
      "126 1354\n",
      "alpha:         [0.00063166 0.0004005  0.00126541 0.00398247 0.0022074  0.00043438\n",
      " 0.00055265 0.00226838 0.00064805 0.00186552 0.00139667 0.00097998\n",
      " 0.00045263 0.002118   0.00046286]\n",
      "softmax alpha: [0.06662135 0.06660595 0.06666359 0.06684496 0.06672641 0.06660821\n",
      " 0.06661609 0.06673048 0.06662245 0.06670361 0.06667234 0.06664456\n",
      " 0.06660943 0.06672045 0.06661011]\n",
      "==================================================\n",
      "127 1372\n",
      "alpha:         [0.0239854  0.02140538 0.09172962 0.04122006 0.0862857  0.08220173]\n",
      "softmax alpha: [0.16105329 0.1606383  0.17234176 0.16385304 0.1714061  0.17070751]\n",
      "==================================================\n",
      "128 1385\n",
      "alpha:         [0.19060198 0.17944202 0.1438445  0.19201612 0.09322047]\n",
      "softmax alpha: [0.20610713 0.20381977 0.19669191 0.2063988  0.18698241]\n",
      "==================================================\n",
      "129 1393\n",
      "alpha:         [0.08065726 0.16832073 0.10378043 0.1435064  0.09350747]\n",
      "softmax alpha: [0.19257387 0.21021762 0.19707867 0.2050654  0.19506445]\n",
      "==================================================\n",
      "130 1399\n",
      "alpha:         [0.06952799 0.13334983 0.11850601 0.17520845 0.18007343]\n",
      "softmax alpha: [0.18710987 0.19944087 0.19650227 0.20796638 0.2089806 ]\n",
      "==================================================\n",
      "131 1402\n",
      "alpha:         [0.10839745 0.03888753 0.0451511  0.03360722 0.0502522  0.07026625]\n",
      "softmax alpha: [0.17526638 0.1634974  0.16452469 0.16263636 0.16536609 0.16870908]\n",
      "==================================================\n",
      "132 1409\n",
      "alpha:         [0.05835547 0.12148615 0.07262911 0.05479792 0.06488405 0.10316138]\n",
      "softmax alpha: [0.16317564 0.17380915 0.16552146 0.16259617 0.16424443 0.17065314]\n",
      "==================================================\n",
      "133 1429\n",
      "alpha:         [0.05320153 0.09227081 0.06551378 0.05525236 0.09841309 0.02401993]\n",
      "softmax alpha: [0.16469654 0.17125846 0.16673686 0.16503465 0.17231362 0.15995988]\n",
      "==================================================\n",
      "134 1436\n",
      "alpha:         [0.04374026 0.03557678 0.04712001 0.0179272  0.01629624 0.07060851\n",
      " 0.05809375]\n",
      "softmax alpha: [0.1431764  0.14201234 0.14366112 0.13952788 0.1393005  0.14707545\n",
      " 0.14524631]\n",
      "==================================================\n",
      "135 1437\n",
      "alpha:         [0.14883894 0.14980145 0.25653658 0.11827968 0.12588015]\n",
      "softmax alpha: [0.19755507 0.19774531 0.22001925 0.19160925 0.19307112]\n",
      "==================================================\n",
      "136 1442\n",
      "alpha:         [0.17711002 0.2071246  0.13682975 0.10013206 0.18601575]\n",
      "softmax alpha: [0.20301083 0.20919648 0.19499601 0.18796981 0.20482687]\n",
      "==================================================\n",
      "137 1466\n",
      "alpha:         [0.18442008 0.21203506 0.07357486 0.09618059 0.16731878]\n",
      "softmax alpha: [0.20739733 0.21320442 0.18563664 0.18988088 0.20388072]\n",
      "==================================================\n",
      "138 1470\n",
      "alpha:         [0.2409768  0.16431618 0.23466372 0.21504344 0.13349424]\n",
      "softmax alpha: [0.20866358 0.19326507 0.20735042 0.2033218  0.18739913]\n",
      "==================================================\n",
      "139 1493\n",
      "alpha:         [0.12210842 0.18401638 0.31304644 0.13652717 0.13271973]\n",
      "softmax alpha: [0.18869919 0.20075035 0.22839856 0.1914397  0.19071219]\n",
      "==================================================\n",
      "140 1494\n",
      "alpha:         [0.11581995 0.03258349 0.12628457 0.03922035 0.05612207]\n",
      "softmax alpha: [0.20837836 0.19173592 0.21057041 0.19301267 0.19630264]\n",
      "==================================================\n",
      "141 1508\n",
      "alpha:         [0.00147778 0.00616189 0.00202562 0.00356063 0.01106353 0.00461657\n",
      " 0.00461733 0.00374113 0.0065483  0.00621939]\n",
      "softmax alpha: [0.09964774 0.1001156  0.09970235 0.09985551 0.10060753 0.09996101\n",
      " 0.09996108 0.09987354 0.10015429 0.10012135]\n",
      "==================================================\n",
      "142 1516\n",
      "alpha:         [0.02031492 0.02747815 0.01499289 0.02581278 0.01308842 0.01964013\n",
      " 0.00922742 0.01179495]\n",
      "softmax alpha: [0.12531314 0.12621401 0.12464799 0.12600399 0.12441083 0.12522861\n",
      " 0.12393141 0.12425001]\n",
      "==================================================\n",
      "143 1518\n",
      "alpha:         [0.09873845 0.086567   0.0291173  0.05490099 0.12322598 0.0520383 ]\n",
      "softmax alpha: [0.17073814 0.1686726  0.15925551 0.1634151  0.1749707  0.16294796]\n",
      "==================================================\n",
      "144 1525\n",
      "alpha:         [0.01323779 0.0139152  0.01537052 0.02134718 0.01767798 0.0135415\n",
      " 0.01198047 0.02245396 0.0025644 ]\n",
      "softmax alpha: [0.11094969 0.11102487 0.11118656 0.11185308 0.11144342 0.11098339\n",
      " 0.11081028 0.11197694 0.10977177]\n",
      "==================================================\n",
      "145 1529\n",
      "alpha:         [0.11419265 0.08057641 0.09683671 0.16660975 0.11427695]\n",
      "softmax alpha: [0.19985467 0.19324798 0.19641594 0.21060989 0.19987152]\n",
      "==================================================\n",
      "146 1547\n",
      "alpha:         [0.00227894 0.00603074 0.00508902 0.00470071 0.00375699 0.00451413\n",
      " 0.00600852 0.00217636 0.00587449 0.00266058 0.00410064]\n",
      "softmax alpha: [0.09072636 0.09106738 0.09098166 0.09094634 0.09086055 0.09092937\n",
      " 0.09106536 0.09071705 0.09105315 0.09076099 0.09089178]\n",
      "==================================================\n",
      "147 1554\n",
      "alpha:         [0.10474577 0.19221825 0.02783262 0.11105552 0.1329625 ]\n",
      "softmax alpha: [0.19792845 0.21602153 0.18327586 0.19918128 0.20359288]\n",
      "==================================================\n",
      "148 1563\n",
      "alpha:         [0.03456296 0.01019822 0.02883494 0.01354931 0.01328225 0.02055258\n",
      " 0.01448999 0.01525911 0.02160993]\n",
      "softmax alpha: [0.11283378 0.11011784 0.11218932 0.11048747 0.11045797 0.11126396\n",
      " 0.11059145 0.11067654 0.11138167]\n",
      "==================================================\n",
      "149 1573\n",
      "alpha:         [0.08177319 0.04783752 0.06962472 0.06162144 0.07374798 0.07286491]\n",
      "softmax alpha: [0.16898323 0.16334489 0.16694277 0.16561201 0.16763254 0.16748457]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "\n",
    "#with Embedding\n",
    "result = np.zeros((usr_test_amount, movie_nb))\n",
    "RS = np.zeros((usr_test_amount, movie_nb))\n",
    "\n",
    "#test_idx --> Test 的 index length = 150\n",
    "test_yes_id = []\n",
    "\n",
    "for s in range(usr_test_amount):\n",
    "    print(s, test_idx[s])\n",
    "\n",
    "    yes = []\n",
    "    sample = random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]]))\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha = np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        \n",
    "# #         ''' Observe each part in attention\n",
    "#         WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "#         WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "#         WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "#         WvVy = np.sum(np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))\n",
    "#         print('The sum of each par -->',\n",
    "#               '\\nw1:',testW1,\n",
    "#               '\\nWuU:',WuUu,\n",
    "#               '\\nwyY:',WyYy,\n",
    "#               '\\nWaA:',WaAa,\n",
    "#               '\\nWvV:',WvVy)\n",
    "# #         '''\n",
    "        \n",
    "        alpha[a] = np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T) +\n",
    "                                np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T) +\n",
    "                                np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T) +\n",
    "                                np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))))*r\n",
    "        \n",
    "    mul = np.zeros((1,latent_dim))\n",
    "    \n",
    "    print(\"{:<15}{}\".format('alpha:', alpha))\n",
    "    print(\"{:<15}{}\".format('softmax alpha:', softmax(alpha)))\n",
    "    print('==================================================')\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        mul += alpha[i] * A[sample[i]] #attention alpha * Ai part \n",
    "    new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "    for k in range(movie_nb):\n",
    "        result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 165)\n"
     ]
    }
   ],
   "source": [
    "#取出test的資料\n",
    "print(RS.shape)\n",
    "\n",
    "testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "target = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "        \n",
    "for z in range(usr_test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    # positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    # not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "#     print(user_id)\n",
    "#     print(youtube_t)\n",
    "#     print(youtube_f)\n",
    "    \n",
    "    #前面放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "        \n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "    \n",
    "#     print(testRS[z])\n",
    "#     print(target[z])\n",
    "#     print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n",
      "num of positive data in testing: 1078.0\n"
     ]
    }
   ],
   "source": [
    "print(target.shape, testRS.shape)\n",
    "sumtarget = np.sum(target)\n",
    "print('num of positive data in testing:', sumtarget) # whole matrix: 4800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "def topN(RSls, n):\n",
    "    maxn = np.argsort(RSls)[::-1][:n]\n",
    "    return maxn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32)\n"
     ]
    }
   ],
   "source": [
    "all_sort = []\n",
    "\n",
    "for i in range(usr_test_amount):\n",
    "    all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "    \n",
    "all_sort = np.asarray(all_sort)\n",
    "print(all_sort.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    return dcg\n",
    "\n",
    "def NDCG(target, testRS, num_ndcg): #target是真正的喜好\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(usr_test_amount): # the number of testing users\n",
    "        idcg = DCG(target[m][:num_ndcg])\n",
    "        \n",
    "        pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        \n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        total_ndcg += ndcg\n",
    "        \n",
    "    avg_ndcg = total_ndcg/usr_test_amount\n",
    "    return avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def MAP(target,testRS):\n",
    "    total_prec = 0\n",
    "    for u in range(usr_test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec += average_precision_score(y_true, y_scores)\n",
    "        \n",
    "    Map_value = total_prec/usr_test_amount\n",
    "    \n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1\n",
      "Num of TP: 61\n",
      "prec: 0.4066666666666667\n",
      "recall: 0.05658627087198516\n",
      "F1_score: 0.09934853420195441\n",
      "*****\n",
      "Top 3\n",
      "Num of TP: 245\n",
      "prec: 0.5444444444444444\n",
      "recall: 0.22727272727272727\n",
      "F1_score: 0.3206806282722513\n",
      "*****\n",
      "Top 5\n",
      "Num of TP: 531\n",
      "prec: 0.708\n",
      "recall: 0.49257884972170685\n",
      "F1_score: 0.5809628008752734\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "NDCG@ 1\n",
      "NDCG score: 0.4066666666666667\n",
      "*****\n",
      "NDCG@ 3\n",
      "NDCG score: 0.4098693970321956\n",
      "*****\n",
      "NDCG@ 5\n",
      "NDCG score: 0.3907937352964554\n",
      "*****\n",
      "NDCG@ 10\n",
      "NDCG score: 0.4456162738924699\n",
      "*****\n",
      "NDCG@ 15\n",
      "NDCG score: 0.5193816724087476\n",
      "*****\n",
      "NDCG@ 20\n",
      "NDCG score: 0.5741527706846403\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "MAP: 0.42206077640418116\n"
     ]
    }
   ],
   "source": [
    "# Top N\n",
    "N = [1, 3, 5]\n",
    "correct = 0\n",
    "\n",
    "for n in N:\n",
    "    print('Top', n)\n",
    "    \n",
    "    for i in range(len(testRS)):\n",
    "        topn = topN(testRS[i], n)\n",
    "        sum_target = int(np.sum(target[i]))\n",
    "        \n",
    "        TP = 0\n",
    "        for i in topn:\n",
    "            if i < sum_target:\n",
    "                TP += 1\n",
    "                \n",
    "        correct += TP\n",
    "\n",
    "    print('Num of TP:', correct)\n",
    "\n",
    "    prec = correct/(len(testRS)*n)\n",
    "    recall = correct/sumtarget\n",
    "    \n",
    "    print('prec:', prec)\n",
    "    print('recall:', recall)\n",
    "    print('F1_score:', F1_score(prec, recall))\n",
    "    \n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# NDCG\n",
    "num_ndcgs = [1, 3, 5, 10, 15, 20]\n",
    "for num_ndcg in num_ndcgs:\n",
    "    print('NDCG@', num_ndcg)\n",
    "    print('NDCG score:', NDCG(target, testRS, num_ndcg))\n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# MAP\n",
    "print('MAP:', MAP(target,testRS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
