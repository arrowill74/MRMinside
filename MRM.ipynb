{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_NAME = 'MRM_all2834_E200_10epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def newPath(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def writeProgress(msg, count, total):\n",
    "    sys.stdout.write(msg + \"{:.2%}\\r\".format(count/total))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 2834)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_2834.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n",
      "150 32\n",
      "64 2834 200\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "print(usr_nb, movie_nb)\n",
    "\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "print(usr_test_amount, movie_test_amount)\n",
    "\n",
    "latent_dim = 64 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "embedding_dims = 200\n",
    "print(latent_dim, ft_dim, embedding_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)\n",
    "# print('Before:', usr_genre)\n",
    "# print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "Avg of followers: 142.0969696969697\n",
      "The num of followers over 5: 163\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "print('Avg of followers:', np.mean(moive_followers))\n",
    "\n",
    "asc = np.sort(moive_followers)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)\n",
    "\n",
    "over5 = 0\n",
    "for num in moive_followers:\n",
    "    if num >= 5:\n",
    "        over5 += 1\n",
    "print('The num of followers over 5:', over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 50: 125\n",
      "Over 100: 89\n",
      "Over 150: 58\n",
      "Over 200: 42\n",
      "Over 250: 31\n",
      "Over 300: 21\n"
     ]
    }
   ],
   "source": [
    "print('Over 50:', np.sum(moive_followers >= 50))\n",
    "print('Over 100:', np.sum(moive_followers >= 100))\n",
    "print('Over 150:', np.sum(moive_followers >= 150))\n",
    "print('Over 200:', np.sum(moive_followers >= 200))\n",
    "print('Over 250:', np.sum(moive_followers >= 250))\n",
    "print('Over 300:', np.sum(moive_followers >= 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,) [  0   2   3   4   9  12  24  28  30  34  40  44  49  55  57  58  60  66\n",
      "  68  78  80  81  84  86  87  99 101 102 112 119 122 123 125 126 127 128\n",
      " 129 134 144 156 161 164]\n",
      "32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]\n"
     ]
    }
   ],
   "source": [
    "over200_idx = np.nonzero(moive_followers >= 200)[0]\n",
    "print(over200_idx.shape, over200_idx)\n",
    "\n",
    "random.seed(42)\n",
    "movie_test_idx = sorted(random.sample(list(over200_idx), movie_test_amount))\n",
    "print(len(movie_test_idx), movie_test_idx) # 32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "Avg of followers: 14.820480404551201\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "print('Avg of followers:', np.mean(each_user))\n",
    "\n",
    "asc = np.sort(each_user)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 10: 1582\n",
      "Over 12: 937\n",
      "Over 14: 613\n",
      "Over 16: 440\n",
      "Over 18: 315\n",
      "Over 20: 229\n"
     ]
    }
   ],
   "source": [
    "print('Over 10:', np.sum(each_user >= 10))\n",
    "print('Over 12:', np.sum(each_user >= 12))\n",
    "print('Over 14:', np.sum(each_user >= 14))\n",
    "print('Over 16:', np.sum(each_user >= 16))\n",
    "print('Over 18:', np.sum(each_user >= 18))\n",
    "print('Over 20:', np.sum(each_user >= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10]) # 150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init\n",
    "train_t = []\n",
    "train_f = []\n",
    "test_t = []\n",
    "test_f = []\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    # init\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    t_for_test = []\n",
    "    f_for_test = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "#         print(len(t_for_train) + len(f_for_train))\n",
    "        \n",
    "    else: #if in test id, choose half of true and other \n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #the others for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    if not (len(t_for_train) + len(f_for_train) + len(t_for_test) + len(f_for_test)) == movie_nb:\n",
    "        print('Error!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None, ft_dim))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "image_j = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [usr_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [movie_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [movie_nb, latent_dim],\n",
    "                               initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    \n",
    "#     W1 = tf.get_variable(\"W1\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [usr_nb, movie_nb, embedding_dims], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     Wve = tf.get_variable(\"Wve\", [embedding_dims, ft_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1, latent_dim], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "with tf.variable_scope('feature_level'):\n",
    "    embedding = tf.get_variable(\"embedding\", [embedding_dims,ft_dim],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Beta = tf.get_variable(\"beta\", [usr_nb, embedding_dims],\n",
    "                           initializer=tf.random_normal_initializer(0.01, 0.001, seed=10))\n",
    "    \n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user)\n",
    "vi = tf.nn.embedding_lookup(item_latent, i)\n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "# w1 = tf.nn.embedding_lookup(W1, user)\n",
    "wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user))\n",
    "wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user))\n",
    "wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user))\n",
    "wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user))\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-19-45fa636fc37a>:95: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "a_list = tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    \n",
    "    a_list = tf.concat([a_list,[(tf.nn.relu(tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                            tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(wvui, tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "    q += 1\n",
    "    return q, a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "norm_par = [wu,wy,wa,wv]\n",
    "# norm_par = [tf.reduce_sum(tf.multiply(u, u)),\n",
    "#             tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "#             tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "#             tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "#             tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "#             tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "#             tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "#             tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "#             tf.reduce_sum(tf.multiply(embedding,embedding))]\n",
    "\n",
    "wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "\n",
    "last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(latent_dim),0)\n",
    "q = tf.constant(0)\n",
    "\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_, aux_np = tf.while_loop(sum_att_cond, sum_att_body, [q,aux_np])\n",
    "\n",
    "aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "aux_np += u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new = tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "# ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "l2_norm = tf.add_n([\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.001 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(embedding,embedding))\n",
    "          ])\n",
    "\n",
    "loss = l2_norm - tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Tue Mar 17 19:46:12 2020\n",
      "Epoch: 0\n",
      "total_loss          [[0.60236533]]\n",
      "train_auc:          0.7603898426323319\n",
      "\tCurrent time: Tue Mar 17 22:57:05 2020  sec\n",
      "==================================================\n",
      "Epoch: 1\n",
      "total_loss          [[0.54917541]]\n",
      "train_auc:          0.7992668097281831\n",
      "\tCurrent time: Wed Mar 18 02:07:53 2020  sec\n",
      "==================================================\n",
      "Epoch: 2\n",
      "total_loss          [[0.52863731]]\n",
      "train_auc:          0.8187723533619456\n",
      "\tCurrent time: Wed Mar 18 05:18:48 2020  sec\n",
      "==================================================\n",
      "Epoch: 3\n",
      "total_loss          [[0.51245798]]\n",
      "train_auc:          0.8308163447782546\n",
      "\tCurrent time: Wed Mar 18 08:29:47 2020  sec\n",
      "==================================================\n",
      "Epoch: 4\n",
      "total_loss          [[0.50310589]]\n",
      "train_auc:          0.8388903791130186\n",
      "\tCurrent time: Wed Mar 18 11:41:01 2020  sec\n",
      "==================================================\n",
      "Epoch: 5\n",
      "total_loss          [[0.49472293]]\n",
      "train_auc:          0.8469867668097282\n",
      "\tCurrent time: Wed Mar 18 14:52:05 2020  sec\n",
      "==================================================\n",
      "Epoch: 6\n",
      "total_loss          [[0.49271032]]\n",
      "train_auc:          0.8509433118741059\n",
      "\tCurrent time: Wed Mar 18 18:03:21 2020  sec\n",
      "==================================================\n",
      "Epoch: 7\n",
      "total_loss          [[0.49066974]]\n",
      "train_auc:          0.85435443490701\n",
      "\tCurrent time: Wed Mar 18 21:14:51 2020  sec\n",
      "==================================================\n",
      "Epoch: 8\n",
      "total_loss          [[0.48991406]]\n",
      "train_auc:          0.8567507153075823\n",
      "\tCurrent time: Thu Mar 19 00:25:22 2020  sec\n",
      "==================================================\n",
      "Epoch: 9\n",
      "total_loss          [[0.48549012]]\n",
      "train_auc:          0.860287017167382\n",
      "\tCurrent time: Thu Mar 19 03:36:22 2020  sec\n",
      "==================================================\n",
      "Total cost time: 114608.5463039875  sec\n",
      "End time: Thu Mar 19 03:36:22 2020\n"
     ]
    }
   ],
   "source": [
    "print('Start time:', time.ctime())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_yes_id=[]\n",
    "\n",
    "for q in range(10):\n",
    "    print('Epoch:',q)\n",
    "    train_auc = 0\n",
    "    total_loss = 0\n",
    "    xuij_auc = 0\n",
    "    length = 0\n",
    "    \n",
    "    for z in range(usr_nb):\n",
    "        writeProgress('Progress:', z, usr_nb)\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes = []\n",
    "        yesr = []\n",
    "        \n",
    "#         #選全部的Positive\n",
    "#         sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #選全部的電影\n",
    "        sample = all_auxilary\n",
    "        \n",
    "        #change\n",
    "        r_3 = np.zeros(len(sample))\n",
    "         \n",
    "        for b in range(len(sample)):\n",
    "            yes.append(all_npy[sample[b]])\n",
    "            yesr.append(movie_genre[sample[b]] * usr_genre_norm[z])\n",
    "        \n",
    "        for b in range(len(yesr)):\n",
    "            r_3[b]=max(yesr[b])\n",
    "        #print('r_3:',r_3)\n",
    "        \n",
    "        yes = np.array(yes)\n",
    "        \n",
    "        # positive sample\n",
    "        train_t_sample = train_t[z]\n",
    "        for ta in train_t_sample:\n",
    "            #print(ta,'--> positive feedback')\n",
    "            \n",
    "            pos = sample.index(ta)\n",
    "            \n",
    "            image_1=np.expand_dims(all_npy[ta],0)\n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            \n",
    "            for b in train_f_sample:\n",
    "                image_2 = np.expand_dims(all_npy[b],0)\n",
    "                \n",
    "                _last_be_relu, _norm_par, _a_list, r3, _auc, _loss, _ = sess.run(\n",
    "                    [last_be_relu, norm_par, a_list_smooth, a_list_soft, auc, loss, train_op], \n",
    "                    feed_dict={user: [z], i: [ta], j: [b], xf: yes, \n",
    "                               l_id:sample, l_id_len:[len(sample)],\n",
    "                               positive_id: train_t[z], positive_len:[len(train_t[z])],\n",
    "                               r: r_3, image_i: image_1, image_j: image_2})\n",
    "                \n",
    "                '''Observe all params\n",
    "                print('u,vi,vj',_norm_par[:3])\n",
    "                print('w1,wu,wy,wa,wv',_norm_par[3:7])\n",
    "                print('beta',_norm_par[7])\n",
    "                print('Embedding',_norm_par[8])\n",
    "                print('after softmax:', r3)\n",
    "                print('before softmax:', _a_list)\n",
    "                print('---------------------------------------------------')\n",
    "                '''\n",
    "                train_auc += _auc\n",
    "                total_loss += _loss\n",
    "                length += 1\n",
    "    \n",
    "    print(\"{:<20}{}\".format('total_loss', total_loss/length))\n",
    "    print(\"{:<20}{}\".format('train_auc:', train_auc/length))\n",
    "    \n",
    "    loss_acc_list.append([total_loss/length, train_auc/length])\n",
    "    \n",
    "    print('\\tCurrent time:', time.ctime(), ' sec')\n",
    "    print('==================================================')\n",
    "    \n",
    "print('Total cost time:',time.time()-t0, ' sec')\n",
    "\n",
    "print('End time:', time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "loss= [[0.60236533]]\n",
      "acc= 0.7603898426323319\n",
      "==================================================\n",
      "Iteration: 1\n",
      "loss= [[0.54917541]]\n",
      "acc= 0.7992668097281831\n",
      "==================================================\n",
      "Iteration: 2\n",
      "loss= [[0.52863731]]\n",
      "acc= 0.8187723533619456\n",
      "==================================================\n",
      "Iteration: 3\n",
      "loss= [[0.51245798]]\n",
      "acc= 0.8308163447782546\n",
      "==================================================\n",
      "Iteration: 4\n",
      "loss= [[0.50310589]]\n",
      "acc= 0.8388903791130186\n",
      "==================================================\n",
      "Iteration: 5\n",
      "loss= [[0.49472293]]\n",
      "acc= 0.8469867668097282\n",
      "==================================================\n",
      "Iteration: 6\n",
      "loss= [[0.49271032]]\n",
      "acc= 0.8509433118741059\n",
      "==================================================\n",
      "Iteration: 7\n",
      "loss= [[0.49066974]]\n",
      "acc= 0.85435443490701\n",
      "==================================================\n",
      "Iteration: 8\n",
      "loss= [[0.48991406]]\n",
      "acc= 0.8567507153075823\n",
      "==================================================\n",
      "Iteration: 9\n",
      "loss= [[0.48549012]]\n",
      "acc= 0.860287017167382\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(loss_acc_list)):\n",
    "    print('Iteration:',i)\n",
    "    print('loss=',loss_acc_list[i][0])\n",
    "    print('acc=',loss_acc_list[i][1])\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 11)\n",
      "[0.6023653349651288, 0.5491754054341023, 0.5286373111140915, 0.5124579826649678, 0.5031058948609621, 0.49472293007868384, 0.4927103241796316, 0.4906697416510193, 0.4899140583087446, 0.4854901184169349]\n",
      "[0.7603898426323319, 0.7992668097281831, 0.8187723533619456, 0.8308163447782546, 0.8388903791130186, 0.8469867668097282, 0.8509433118741059, 0.85435443490701, 0.8567507153075823, 0.860287017167382]\n"
     ]
    }
   ],
   "source": [
    "# training history\n",
    "epochs = range(1, len(loss_acc_list) + 1)\n",
    "print(epochs)\n",
    "loss = [ls[0].tolist()[0][0] for ls in loss_acc_list]\n",
    "print(loss)\n",
    "acc = [ls[1] for ls in loss_acc_list]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 繪製結果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av, B = sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv, Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (1582, 64)\n",
      "photo latent shape:  (165, 64)\n",
      "Auxilary latent shape:  (165, 64)\n",
      "Embedding shape: (200, 2834)\n",
      "Wu weight shape: (1582, 165, 64)\n",
      "Wy weight shape: (1582, 165, 64)\n",
      "Wa weight shape: (1582, 165, 64)\n",
      "Wv weight shape: (1582, 165, 200)\n",
      "Beta shape: (1582, 200)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Wu weight shape:', Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('./weight/' + SAVE_NAME + '.npz', \n",
    "         U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av, B=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload params if crash\n",
    "# SAVE_NAME = 'MRM_ALL_Embedding200_L2_resplit'\n",
    "\n",
    "# params = np.load('./weight/' + SAVE_NAME + '.npz')\n",
    "# print(params)\n",
    "# U = params['U']\n",
    "# Y = params['Y']\n",
    "# A = params['A']\n",
    "# E = params['E']\n",
    "# Au = params['Wu']\n",
    "# Ay = params['Wy']\n",
    "# Aa = params['Wa']\n",
    "# Av = params['Wv']\n",
    "# B = params['B']\n",
    "\n",
    "# print('User latent shape: ',U.shape)\n",
    "# print('photo latent shape: ', Y.shape)\n",
    "# print('Auxilary latent shape: ',A.shape)\n",
    "# print('Embedding shape:', E.shape)\n",
    "# print('Wu weight shape:', Au.shape)\n",
    "# print('Wy weight shape:', Ay.shape)\n",
    "# print('Wa weight shape:', Aa.shape)\n",
    "# print('Wv weight shape:', Av.shape)\n",
    "# print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "sum_alpha:     0.0007437109564706757\n",
      "==================================================\n",
      "1 51\n",
      "sum_alpha:     0.0027958863071132125\n",
      "==================================================\n",
      "2 54\n",
      "sum_alpha:     0.004186824539631062\n",
      "==================================================\n",
      "3 61\n",
      "sum_alpha:     0.008483421295960827\n",
      "==================================================\n",
      "4 65\n",
      "sum_alpha:     0.00958306071546065\n",
      "==================================================\n",
      "5 88\n",
      "sum_alpha:     0.01073275377027689\n",
      "==================================================\n",
      "6 93\n",
      "sum_alpha:     0.011823760574288661\n",
      "==================================================\n",
      "7 96\n",
      "sum_alpha:     0.014046808133805181\n",
      "==================================================\n",
      "8 114\n",
      "sum_alpha:     0.014395342131598505\n",
      "==================================================\n",
      "9 130\n",
      "sum_alpha:     0.01689727832530338\n",
      "==================================================\n",
      "10 135\n",
      "sum_alpha:     0.0173552482445936\n",
      "==================================================\n",
      "11 142\n",
      "sum_alpha:     0.01796228895690277\n",
      "==================================================\n",
      "12 146\n",
      "sum_alpha:     0.018748174797853914\n",
      "==================================================\n",
      "13 161\n",
      "sum_alpha:     0.019708477667995914\n",
      "==================================================\n",
      "14 163\n",
      "sum_alpha:     0.02173930580457728\n",
      "==================================================\n",
      "15 178\n",
      "sum_alpha:     0.024188816858739005\n",
      "==================================================\n",
      "16 186\n",
      "sum_alpha:     0.02682470066895603\n",
      "==================================================\n",
      "17 189\n",
      "sum_alpha:     0.027798317966540947\n",
      "==================================================\n",
      "18 191\n",
      "sum_alpha:     0.030689966807006988\n",
      "==================================================\n",
      "19 198\n",
      "sum_alpha:     0.033046189509678726\n",
      "==================================================\n",
      "20 206\n",
      "sum_alpha:     0.034998506877270984\n",
      "==================================================\n",
      "21 209\n",
      "sum_alpha:     0.03719295730669145\n",
      "==================================================\n",
      "22 224\n",
      "sum_alpha:     0.03974264979343314\n",
      "==================================================\n",
      "23 228\n",
      "sum_alpha:     0.04359079123621687\n",
      "==================================================\n",
      "24 255\n",
      "sum_alpha:     0.04404822964872911\n",
      "==================================================\n",
      "25 283\n",
      "sum_alpha:     0.04565310825837117\n",
      "==================================================\n",
      "26 285\n",
      "sum_alpha:     0.04622169578172141\n",
      "==================================================\n",
      "27 292\n",
      "sum_alpha:     0.04912955470387904\n",
      "==================================================\n",
      "28 313\n",
      "sum_alpha:     0.05002209730544973\n",
      "==================================================\n",
      "29 318\n",
      "sum_alpha:     0.05018162376553447\n",
      "==================================================\n",
      "30 326\n",
      "sum_alpha:     0.05146649404421488\n",
      "==================================================\n",
      "31 327\n",
      "sum_alpha:     0.051515826701691754\n",
      "==================================================\n",
      "32 333\n",
      "sum_alpha:     0.0541698620725016\n",
      "==================================================\n",
      "33 334\n",
      "sum_alpha:     0.05634691653147275\n",
      "==================================================\n",
      "34 350\n",
      "sum_alpha:     0.058477679880642695\n",
      "==================================================\n",
      "35 393\n",
      "sum_alpha:     0.0585410444822679\n",
      "==================================================\n",
      "36 407\n",
      "sum_alpha:     0.06828325320873677\n",
      "==================================================\n",
      "37 429\n",
      "sum_alpha:     0.07085679932155865\n",
      "==================================================\n",
      "38 432\n",
      "sum_alpha:     0.07537895845340883\n",
      "==================================================\n",
      "39 435\n",
      "sum_alpha:     0.07670405139702544\n",
      "==================================================\n",
      "40 440\n",
      "sum_alpha:     0.07692667729894076\n",
      "==================================================\n",
      "41 447\n",
      "sum_alpha:     0.07738069106840262\n",
      "==================================================\n",
      "42 449\n",
      "sum_alpha:     0.07739185687811188\n",
      "==================================================\n",
      "43 451\n",
      "sum_alpha:     0.07739789662983136\n",
      "==================================================\n",
      "44 457\n",
      "sum_alpha:     0.08153310964721418\n",
      "==================================================\n",
      "45 466\n",
      "sum_alpha:     0.08288887557581037\n",
      "==================================================\n",
      "46 469\n",
      "sum_alpha:     0.08432540495074323\n",
      "==================================================\n",
      "47 476\n",
      "sum_alpha:     0.09006649123113747\n",
      "==================================================\n",
      "48 501\n",
      "sum_alpha:     0.09206516240003221\n",
      "==================================================\n",
      "49 505\n",
      "sum_alpha:     0.0936067604423008\n",
      "==================================================\n",
      "50 514\n",
      "sum_alpha:     0.09879740313704426\n",
      "==================================================\n",
      "51 538\n",
      "sum_alpha:     0.1008734838015895\n",
      "==================================================\n",
      "52 541\n",
      "sum_alpha:     0.10090270005085837\n",
      "==================================================\n",
      "53 542\n",
      "sum_alpha:     0.10306070918192177\n",
      "==================================================\n",
      "54 546\n",
      "sum_alpha:     0.10622563032721638\n",
      "==================================================\n",
      "55 548\n",
      "sum_alpha:     0.10965418493572487\n",
      "==================================================\n",
      "56 552\n",
      "sum_alpha:     0.11779275679748516\n",
      "==================================================\n",
      "57 563\n",
      "sum_alpha:     0.11860115995249637\n",
      "==================================================\n",
      "58 569\n",
      "sum_alpha:     0.120538582589287\n",
      "==================================================\n",
      "59 592\n",
      "sum_alpha:     0.12150006657213959\n",
      "==================================================\n",
      "60 600\n",
      "sum_alpha:     0.12476331387732728\n",
      "==================================================\n",
      "61 644\n",
      "sum_alpha:     0.12825265140106729\n",
      "==================================================\n",
      "62 646\n",
      "sum_alpha:     0.13326153551434075\n",
      "==================================================\n",
      "63 664\n",
      "sum_alpha:     0.13339843174332458\n",
      "==================================================\n",
      "64 689\n",
      "sum_alpha:     0.133962688772434\n",
      "==================================================\n",
      "65 696\n",
      "sum_alpha:     0.13491132988799584\n",
      "==================================================\n",
      "66 704\n",
      "sum_alpha:     0.13520157847598646\n",
      "==================================================\n",
      "67 727\n",
      "sum_alpha:     0.14071120962781486\n",
      "==================================================\n",
      "68 735\n",
      "sum_alpha:     0.14714630622677127\n",
      "==================================================\n",
      "69 740\n",
      "sum_alpha:     0.15028653535607855\n",
      "==================================================\n",
      "70 741\n",
      "sum_alpha:     0.15289890611267556\n",
      "==================================================\n",
      "71 747\n",
      "sum_alpha:     0.15378180458671153\n",
      "==================================================\n",
      "72 758\n",
      "sum_alpha:     0.15409017903798594\n",
      "==================================================\n",
      "73 775\n",
      "sum_alpha:     0.16452388500705742\n",
      "==================================================\n",
      "74 777\n",
      "sum_alpha:     0.16486144485144857\n",
      "==================================================\n",
      "75 778\n",
      "sum_alpha:     0.16972821692404277\n",
      "==================================================\n",
      "76 781\n",
      "sum_alpha:     0.17074008484744072\n",
      "==================================================\n",
      "77 788\n",
      "sum_alpha:     0.17113892525369406\n",
      "==================================================\n",
      "78 810\n",
      "sum_alpha:     0.17656990836202105\n",
      "==================================================\n",
      "79 817\n",
      "sum_alpha:     0.1767160011508599\n",
      "==================================================\n",
      "80 821\n",
      "sum_alpha:     0.17841374382871694\n",
      "==================================================\n",
      "81 859\n",
      "sum_alpha:     0.18363185017188366\n",
      "==================================================\n",
      "82 864\n",
      "sum_alpha:     0.1869990339642453\n",
      "==================================================\n",
      "83 865\n",
      "sum_alpha:     0.1869990339642453\n",
      "==================================================\n",
      "84 877\n",
      "sum_alpha:     0.18756884334050158\n",
      "==================================================\n",
      "85 919\n",
      "sum_alpha:     0.18818799124317542\n",
      "==================================================\n",
      "86 928\n",
      "sum_alpha:     0.19336056326331238\n",
      "==================================================\n",
      "87 939\n",
      "sum_alpha:     0.19336517697684816\n",
      "==================================================\n",
      "88 940\n",
      "sum_alpha:     0.19597268425607745\n",
      "==================================================\n",
      "89 946\n",
      "sum_alpha:     0.19654306507747193\n",
      "==================================================\n",
      "90 958\n",
      "sum_alpha:     0.1980072708977533\n",
      "==================================================\n",
      "91 1010\n",
      "sum_alpha:     0.19995803722830466\n",
      "==================================================\n",
      "92 1022\n",
      "sum_alpha:     0.2008705189590994\n",
      "==================================================\n",
      "93 1034\n",
      "sum_alpha:     0.20256944541481736\n",
      "==================================================\n",
      "94 1043\n",
      "sum_alpha:     0.20291473629730825\n",
      "==================================================\n",
      "95 1083\n",
      "sum_alpha:     0.2034501041515331\n",
      "==================================================\n",
      "96 1093\n",
      "sum_alpha:     0.20665759256132893\n",
      "==================================================\n",
      "97 1098\n",
      "sum_alpha:     0.20776924866342225\n",
      "==================================================\n",
      "98 1103\n",
      "sum_alpha:     0.20808671856815164\n",
      "==================================================\n",
      "99 1116\n",
      "sum_alpha:     0.20996509555778134\n",
      "==================================================\n",
      "100 1130\n",
      "sum_alpha:     0.21270672024935672\n",
      "==================================================\n",
      "101 1133\n",
      "sum_alpha:     0.21456550875242233\n",
      "==================================================\n",
      "102 1140\n",
      "sum_alpha:     0.21632405777060512\n",
      "==================================================\n",
      "103 1149\n",
      "sum_alpha:     0.21766657158280026\n",
      "==================================================\n",
      "104 1161\n",
      "sum_alpha:     0.21844256187175964\n",
      "==================================================\n",
      "105 1182\n",
      "sum_alpha:     0.2192464525137018\n",
      "==================================================\n",
      "106 1195\n",
      "sum_alpha:     0.23216987204126302\n",
      "==================================================\n",
      "107 1197\n",
      "sum_alpha:     0.23631182802749046\n",
      "==================================================\n",
      "108 1206\n",
      "sum_alpha:     0.2371822099592208\n",
      "==================================================\n",
      "109 1209\n",
      "sum_alpha:     0.23944304316783926\n",
      "==================================================\n",
      "110 1220\n",
      "sum_alpha:     0.23952918715311888\n",
      "==================================================\n",
      "111 1221\n",
      "sum_alpha:     0.2400481774501716\n",
      "==================================================\n",
      "112 1232\n",
      "sum_alpha:     0.24549976133506685\n",
      "==================================================\n",
      "113 1236\n",
      "sum_alpha:     0.2476562245356194\n",
      "==================================================\n",
      "114 1247\n",
      "sum_alpha:     0.24798847412607433\n",
      "==================================================\n",
      "115 1266\n",
      "sum_alpha:     0.25043533804844764\n",
      "==================================================\n",
      "116 1285\n",
      "sum_alpha:     0.25753254488070015\n",
      "==================================================\n",
      "117 1287\n",
      "sum_alpha:     0.26211357539073643\n",
      "==================================================\n",
      "118 1300\n",
      "sum_alpha:     0.262686525904109\n",
      "==================================================\n",
      "119 1301\n",
      "sum_alpha:     0.2628225462998867\n",
      "==================================================\n",
      "120 1309\n",
      "sum_alpha:     0.26440516190290114\n",
      "==================================================\n",
      "121 1310\n",
      "sum_alpha:     0.2662069083730834\n",
      "==================================================\n",
      "122 1316\n",
      "sum_alpha:     0.26997946390893557\n",
      "==================================================\n",
      "123 1327\n",
      "sum_alpha:     0.27046690159799475\n",
      "==================================================\n",
      "124 1330\n",
      "sum_alpha:     0.2741645671568963\n",
      "==================================================\n",
      "125 1342\n",
      "sum_alpha:     0.2744122480857902\n",
      "==================================================\n",
      "126 1354\n",
      "sum_alpha:     0.2744670478848631\n",
      "==================================================\n",
      "127 1372\n",
      "sum_alpha:     0.275223038040724\n",
      "==================================================\n",
      "128 1385\n",
      "sum_alpha:     0.2782966527584243\n",
      "==================================================\n",
      "129 1393\n",
      "sum_alpha:     0.2797915433895137\n",
      "==================================================\n",
      "130 1399\n",
      "sum_alpha:     0.2800040030404021\n",
      "==================================================\n",
      "131 1402\n",
      "sum_alpha:     0.28080701104921035\n",
      "==================================================\n",
      "132 1409\n",
      "sum_alpha:     0.28339726258310904\n",
      "==================================================\n",
      "133 1429\n",
      "sum_alpha:     0.2876549917326952\n",
      "==================================================\n",
      "134 1436\n",
      "sum_alpha:     0.29066695816920085\n",
      "==================================================\n",
      "135 1437\n",
      "sum_alpha:     0.2956776754379827\n",
      "==================================================\n",
      "136 1442\n",
      "sum_alpha:     0.30448994020497766\n",
      "==================================================\n",
      "137 1466\n",
      "sum_alpha:     0.30985602848210947\n",
      "==================================================\n",
      "138 1470\n",
      "sum_alpha:     0.31496498384751154\n",
      "==================================================\n",
      "139 1493\n",
      "sum_alpha:     0.31798239495175695\n",
      "==================================================\n",
      "140 1494\n",
      "sum_alpha:     0.31868345637704626\n",
      "==================================================\n",
      "141 1508\n",
      "sum_alpha:     0.3190211007196956\n",
      "==================================================\n",
      "142 1516\n",
      "sum_alpha:     0.3196130513568478\n",
      "==================================================\n",
      "143 1518\n",
      "sum_alpha:     0.31981468329816154\n",
      "==================================================\n",
      "144 1525\n",
      "sum_alpha:     0.32073768815372056\n",
      "==================================================\n",
      "145 1529\n",
      "sum_alpha:     0.32170068627214643\n",
      "==================================================\n",
      "146 1547\n",
      "sum_alpha:     0.32188226531355635\n",
      "==================================================\n",
      "147 1554\n",
      "sum_alpha:     0.322987158544886\n",
      "==================================================\n",
      "148 1563\n",
      "sum_alpha:     0.3244567781333448\n",
      "==================================================\n",
      "149 1573\n",
      "sum_alpha:     0.325713724915625\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "\n",
    "#with Embedding\n",
    "result = np.zeros((usr_test_amount, movie_nb))\n",
    "RS = np.zeros((usr_test_amount, movie_nb))\n",
    "\n",
    "#test_idx --> Test 的 index length = 150\n",
    "sum_alpha = 0\n",
    "test_yes_id = []\n",
    "\n",
    "for s in range(usr_test_amount):\n",
    "    print(s, test_idx[s])\n",
    "\n",
    "    yes = []\n",
    "    sample = train_t[test_idx[s]]\n",
    "    alpha = np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        \n",
    "# #         ''' Observe each part in attention\n",
    "#         WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "#         WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "#         WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "#         WvVy = np.sum(np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))\n",
    "#         print('The sum of each par -->',\n",
    "#               '\\nw1:',testW1,\n",
    "#               '\\nWuU:',WuUu,\n",
    "#               '\\nwyY:',WyYy,\n",
    "#               '\\nWaA:',WaAa,\n",
    "#               '\\nWvV:',WvVy)\n",
    "# #         '''\n",
    "\n",
    "        alpha_a = (np.dot(Au[test_idx[s]][sample[a]],np.expand_dims(U[test_idx[s]],0).T) + \n",
    "                   np.dot(Ay[test_idx[s]][sample[a]],np.expand_dims(Y[sample[a]],0).T) + \n",
    "                   np.dot(Aa[test_idx[s]][sample[a]],np.expand_dims(A[sample[a]],0).T) +\n",
    "                   np.dot(Av[test_idx[s]][sample[a]],np.dot(E,np.expand_dims(all_npy[sample[a]],0).T)))\n",
    "        \n",
    "        \n",
    "        # relu part\n",
    "        alpha[a]=np.sum((relu(alpha_a)))*r\n",
    "        # tanh part\n",
    "#         alpha[a]=np.sum((np.tanh(alpha_a)))*r\n",
    "        \n",
    "    mul = np.zeros((1,latent_dim))\n",
    "    added_alpha = np.add(alpha,0.0000000001)\n",
    "    norm_alpha = added_alpha/np.sum(added_alpha)\n",
    "    sum_alpha += np.sum(alpha)\n",
    "    \n",
    "    print(\"{:<15}{}\".format('sum_alpha:', sum_alpha))\n",
    "    print('==================================================')\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        mul += norm_alpha[i] * A[sample[i]] # attention alpha*Ai part\n",
    "    new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "    for k in range(movie_nb):\n",
    "        result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 165)\n"
     ]
    }
   ],
   "source": [
    "#取出test的資料\n",
    "print(RS.shape)\n",
    "\n",
    "testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "target = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "        \n",
    "for z in range(usr_test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    # positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    # not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "#     print(user_id)\n",
    "#     print(youtube_t)\n",
    "#     print(youtube_f)\n",
    "    \n",
    "    #前面放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "        \n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "    \n",
    "#     print(testRS[z])\n",
    "#     print(target[z])\n",
    "#     print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n",
      "num of positive data in testing: 1078.0\n"
     ]
    }
   ],
   "source": [
    "print(target.shape, testRS.shape)\n",
    "sumtarget = np.sum(target)\n",
    "print('num of positive data in testing:', sumtarget) # whole matrix: 4800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = 2*((prec*rec)/(prec+rec))\n",
    "    return f1\n",
    "\n",
    "def topN(RSls, n):\n",
    "    maxn = np.argsort(RSls)[::-1][:n]\n",
    "    return maxn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32)\n"
     ]
    }
   ],
   "source": [
    "all_sort = []\n",
    "\n",
    "for i in range(usr_test_amount):\n",
    "    all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "    \n",
    "all_sort = np.asarray(all_sort)\n",
    "print(all_sort.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    return dcg\n",
    "\n",
    "def NDCG(target, testRS, num_ndcg): #target是真正的喜好\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    for m in range(usr_test_amount): # the number of testing users\n",
    "        idcg = DCG(target[m][:num_ndcg])\n",
    "        \n",
    "        pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "        \n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        total_ndcg += ndcg\n",
    "        \n",
    "    avg_ndcg = total_ndcg/usr_test_amount\n",
    "    return avg_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def MAP(target,testRS):\n",
    "    total_prec = 0\n",
    "    for u in range(usr_test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec += average_precision_score(y_true, y_scores)\n",
    "        \n",
    "    Map_value = total_prec/usr_test_amount\n",
    "    \n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1\n",
      "Num of TP: 73\n",
      "prec: 0.4866666666666667\n",
      "recall: 0.06771799628942486\n",
      "F1_score: 0.11889250814332249\n",
      "*****\n",
      "Top 2\n",
      "Num of TP: 139\n",
      "prec: 0.4633333333333333\n",
      "recall: 0.12894248608534323\n",
      "F1_score: 0.20174165457184326\n",
      "*****\n",
      "Top 3\n",
      "Num of TP: 202\n",
      "prec: 0.4488888888888889\n",
      "recall: 0.18738404452690166\n",
      "F1_score: 0.2643979057591623\n",
      "*****\n",
      "Top 4\n",
      "Num of TP: 258\n",
      "prec: 0.43\n",
      "recall: 0.23933209647495363\n",
      "F1_score: 0.3075089392133492\n",
      "*****\n",
      "Top 5\n",
      "Num of TP: 313\n",
      "prec: 0.41733333333333333\n",
      "recall: 0.29035250463821893\n",
      "F1_score: 0.34245076586433265\n",
      "*****\n",
      "Top 6\n",
      "Num of TP: 363\n",
      "prec: 0.4033333333333333\n",
      "recall: 0.336734693877551\n",
      "F1_score: 0.3670374115267947\n",
      "*****\n",
      "Top 7\n",
      "Num of TP: 409\n",
      "prec: 0.38952380952380955\n",
      "recall: 0.37940630797773656\n",
      "F1_score: 0.3843984962406015\n",
      "*****\n",
      "Top 8\n",
      "Num of TP: 451\n",
      "prec: 0.37583333333333335\n",
      "recall: 0.41836734693877553\n",
      "F1_score: 0.3959613696224759\n",
      "*****\n",
      "Top 9\n",
      "Num of TP: 498\n",
      "prec: 0.3688888888888889\n",
      "recall: 0.4619666048237477\n",
      "F1_score: 0.4102141680395387\n",
      "*****\n",
      "Top 10\n",
      "Num of TP: 545\n",
      "prec: 0.36333333333333334\n",
      "recall: 0.5055658627087198\n",
      "F1_score: 0.42280837858805276\n",
      "*****\n",
      "Top 11\n",
      "Num of TP: 580\n",
      "prec: 0.3515151515151515\n",
      "recall: 0.5380333951762524\n",
      "F1_score: 0.4252199413489736\n",
      "*****\n",
      "Top 12\n",
      "Num of TP: 615\n",
      "prec: 0.3416666666666667\n",
      "recall: 0.5705009276437848\n",
      "F1_score: 0.4273801250868659\n",
      "*****\n",
      "Top 13\n",
      "Num of TP: 658\n",
      "prec: 0.3374358974358974\n",
      "recall: 0.6103896103896104\n",
      "F1_score: 0.4346103038309115\n",
      "*****\n",
      "Top 14\n",
      "Num of TP: 690\n",
      "prec: 0.32857142857142857\n",
      "recall: 0.640074211502783\n",
      "F1_score: 0.434235368156073\n",
      "*****\n",
      "Top 15\n",
      "Num of TP: 724\n",
      "prec: 0.3217777777777778\n",
      "recall: 0.6716141001855288\n",
      "F1_score: 0.4350961538461538\n",
      "*****\n",
      "Top 16\n",
      "Num of TP: 763\n",
      "prec: 0.3179166666666667\n",
      "recall: 0.7077922077922078\n",
      "F1_score: 0.4387579068430132\n",
      "*****\n",
      "Top 17\n",
      "Num of TP: 786\n",
      "prec: 0.30823529411764705\n",
      "recall: 0.7291280148423006\n",
      "F1_score: 0.43329658213891953\n",
      "*****\n",
      "Top 18\n",
      "Num of TP: 811\n",
      "prec: 0.30037037037037034\n",
      "recall: 0.7523191094619666\n",
      "F1_score: 0.42932768660667014\n",
      "*****\n",
      "Top 19\n",
      "Num of TP: 839\n",
      "prec: 0.2943859649122807\n",
      "recall: 0.7782931354359925\n",
      "F1_score: 0.4271894093686354\n",
      "*****\n",
      "Top 20\n",
      "Num of TP: 867\n",
      "prec: 0.289\n",
      "recall: 0.8042671614100185\n",
      "F1_score: 0.42520843550760173\n",
      "*****\n",
      "Top 21\n",
      "Num of TP: 887\n",
      "prec: 0.2815873015873016\n",
      "recall: 0.8228200371057514\n",
      "F1_score: 0.41958372753074746\n",
      "*****\n",
      "Top 22\n",
      "Num of TP: 906\n",
      "prec: 0.27454545454545454\n",
      "recall: 0.8404452690166976\n",
      "F1_score: 0.4138876199177707\n",
      "*****\n",
      "Top 23\n",
      "Num of TP: 929\n",
      "prec: 0.2692753623188406\n",
      "recall: 0.8617810760667903\n",
      "F1_score: 0.41033568904593637\n",
      "*****\n",
      "Top 24\n",
      "Num of TP: 947\n",
      "prec: 0.26305555555555554\n",
      "recall: 0.87847866419295\n",
      "F1_score: 0.40487387772552375\n",
      "*****\n",
      "Top 25\n",
      "Num of TP: 969\n",
      "prec: 0.2584\n",
      "recall: 0.898886827458256\n",
      "F1_score: 0.4014084507042254\n",
      "*****\n",
      "Top 26\n",
      "Num of TP: 982\n",
      "prec: 0.2517948717948718\n",
      "recall: 0.9109461966604824\n",
      "F1_score: 0.39453595821615106\n",
      "*****\n",
      "Top 27\n",
      "Num of TP: 999\n",
      "prec: 0.24666666666666667\n",
      "recall: 0.9267161410018553\n",
      "F1_score: 0.38962558502340094\n",
      "*****\n",
      "Top 28\n",
      "Num of TP: 1014\n",
      "prec: 0.24142857142857144\n",
      "recall: 0.9406307977736549\n",
      "F1_score: 0.3842364532019704\n",
      "*****\n",
      "Top 29\n",
      "Num of TP: 1028\n",
      "prec: 0.23632183908045978\n",
      "recall: 0.9536178107606679\n",
      "F1_score: 0.37877671333824614\n",
      "*****\n",
      "Top 30\n",
      "Num of TP: 1046\n",
      "prec: 0.23244444444444445\n",
      "recall: 0.9703153988868275\n",
      "F1_score: 0.37504481893151675\n",
      "*****\n",
      "Top 31\n",
      "Num of TP: 1062\n",
      "prec: 0.22838709677419355\n",
      "recall: 0.9851576994434137\n",
      "F1_score: 0.3708100558659218\n",
      "*****\n",
      "Top 32\n",
      "Num of TP: 1078\n",
      "prec: 0.22458333333333333\n",
      "recall: 1.0\n",
      "F1_score: 0.3667914256549847\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "NDCG@ 1\n",
      "NDCG score: 0.4866666666666667\n",
      "*****\n",
      "NDCG@ 2\n",
      "NDCG score: 0.4686135356623881\n",
      "*****\n",
      "NDCG@ 3\n",
      "NDCG score: 0.45720688662083453\n",
      "*****\n",
      "NDCG@ 4\n",
      "NDCG score: 0.4431054327483026\n",
      "*****\n",
      "NDCG@ 5\n",
      "NDCG score: 0.4330762785196138\n",
      "*****\n",
      "NDCG@ 6\n",
      "NDCG score: 0.4310435696411828\n",
      "*****\n",
      "NDCG@ 7\n",
      "NDCG score: 0.4410576974640607\n",
      "*****\n",
      "NDCG@ 8\n",
      "NDCG score: 0.4534543205736265\n",
      "*****\n",
      "NDCG@ 9\n",
      "NDCG score: 0.4700817109338564\n",
      "*****\n",
      "NDCG@ 10\n",
      "NDCG score: 0.489905350583386\n",
      "*****\n",
      "NDCG@ 11\n",
      "NDCG score: 0.5051180661823905\n",
      "*****\n",
      "NDCG@ 12\n",
      "NDCG score: 0.5195729557853063\n",
      "*****\n",
      "NDCG@ 13\n",
      "NDCG score: 0.5373960176325048\n",
      "*****\n",
      "NDCG@ 14\n",
      "NDCG score: 0.5508735235639377\n",
      "*****\n",
      "NDCG@ 15\n",
      "NDCG score: 0.565251485880565\n",
      "*****\n",
      "NDCG@ 16\n",
      "NDCG score: 0.5812450651836036\n",
      "*****\n",
      "NDCG@ 17\n",
      "NDCG score: 0.5901100982180689\n",
      "*****\n",
      "NDCG@ 18\n",
      "NDCG score: 0.6002832835706804\n",
      "*****\n",
      "NDCG@ 19\n",
      "NDCG score: 0.6117328408559612\n",
      "*****\n",
      "NDCG@ 20\n",
      "NDCG score: 0.6225521753435814\n",
      "*****\n",
      "NDCG@ 21\n",
      "NDCG score: 0.6301012569739665\n",
      "*****\n",
      "NDCG@ 22\n",
      "NDCG score: 0.6367083462802854\n",
      "*****\n",
      "NDCG@ 23\n",
      "NDCG score: 0.6455495333412797\n",
      "*****\n",
      "NDCG@ 24\n",
      "NDCG score: 0.6522342247455468\n",
      "*****\n",
      "NDCG@ 25\n",
      "NDCG score: 0.6604736381480625\n",
      "*****\n",
      "NDCG@ 26\n",
      "NDCG score: 0.6653064582245135\n",
      "*****\n",
      "NDCG@ 27\n",
      "NDCG score: 0.6715713313662688\n",
      "*****\n",
      "NDCG@ 28\n",
      "NDCG score: 0.6767259646569993\n",
      "*****\n",
      "NDCG@ 29\n",
      "NDCG score: 0.6822573566869481\n",
      "*****\n",
      "NDCG@ 30\n",
      "NDCG score: 0.6891336751106467\n",
      "*****\n",
      "NDCG@ 31\n",
      "NDCG score: 0.6948209026962683\n",
      "*****\n",
      "NDCG@ 32\n",
      "NDCG score: 0.7006026713750553\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "MAP: 0.4605934485025527\n"
     ]
    }
   ],
   "source": [
    "# Top N\n",
    "# print([i for i in range(1, 33)])\n",
    "N = [i for i in range(1, 33)]\n",
    "correct = 0\n",
    "\n",
    "for n in N:\n",
    "    print('Top', n)\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(len(testRS)):\n",
    "        topn = topN(testRS[i], n)\n",
    "        sum_target = int(np.sum(target[i]))\n",
    "        \n",
    "        TP = 0\n",
    "        for i in topn:\n",
    "            if i < sum_target:\n",
    "                TP += 1\n",
    "                \n",
    "        correct += TP\n",
    "\n",
    "    print('Num of TP:', correct)\n",
    "\n",
    "    prec = correct/(len(testRS)*n)\n",
    "    recall = correct/sumtarget\n",
    "    \n",
    "    print('prec:', prec)\n",
    "    print('recall:', recall)\n",
    "    print('F1_score:', F1_score(prec, recall))\n",
    "    \n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# NDCG\n",
    "num_ndcgs = [i for i in range(1, 33)]\n",
    "# [1, 3, 5, 10, 15, 20]\n",
    "for num_ndcg in num_ndcgs:\n",
    "    print('NDCG@', num_ndcg)\n",
    "    print('NDCG score:', NDCG(target, testRS, num_ndcg))\n",
    "    print('*****')\n",
    "\n",
    "print('\\n==============================\\n')\n",
    "\n",
    "# MAP\n",
    "print('MAP:', MAP(target,testRS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
